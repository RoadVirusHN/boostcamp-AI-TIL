{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 1021,
     "status": "ok",
     "timestamp": 1615391513463,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "YoRpR1fcm0V-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1615387834746,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "zInhnEaanEbV",
    "outputId": "1c246506-d741-49de-9449-b1a760036df7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([300.0000,   0.3000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, requires_grad=True) # requires_grad=True를 해야 gradient를 저장하여 사용 가능\n",
    "y = x*3\n",
    "gradients = torch.tensor([100, 0.1], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)# (dy/dx)*gradient 출력\n",
    "# 굳이 주어진 gradients값이 곱해 출력하는 이유는, gradient가 또다른 gradient일 때, chain rule을 만족시키기 위해\n",
    "# 필요없다면 1을 넣어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 866,
     "status": "error",
     "timestamp": 1615388282066,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "-mrMUz86pJf_",
    "outputId": "63950391-9dd5-4e67-a29e-6d9c3acfdd0d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2c97ebdb1611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# requires_grad=False의 예제\n",
    "x = torch.randn(2, requires_grad=False) # requires_grad=True를 해야 gradient를 저장하여 사용 가능\n",
    "y = x*3\n",
    "gradients = torch.tensor([100, 0.1], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 844,
     "status": "error",
     "timestamp": 1615388722033,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "nONB_YNzq2e3",
    "outputId": "d038395f-a43c-444c-d461-56371f897bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([300.0000,   0.3000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e9a840e81a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# False 일 경우 ErrorThrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2번 미분해야하는 경우 retain_graph=True, 번 호출 시, 미분 값이 더해져 중첩됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, requires_grad=True)\n",
    "y = x*3\n",
    "gradients = torch.tensor([100, 0.1], dtype=torch.float)\n",
    "y.backward(gradients, retain_graph= False) # False 일 경우 ErrorThrow\n",
    "print(x.grad) \n",
    "y.backward(gradients) # 2번 미분해야하는 경우 retain_graph=True, 번 호출 시, 미분 값이 더해져 중첩됨 \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1615388690205,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "WeBHutYapVbW",
    "outputId": "567721f0-c5a1-46bf-862c-1b6141cbf001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([300.0000,   0.3000])\n",
      "tensor([600.0000,   0.6000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, requires_grad=True)\n",
    "y = x*3\n",
    "gradients = torch.tensor([100, 0.1], dtype=torch.float)\n",
    "y.backward(gradients, retain_graph= True) # retain_graph=True가 아니면, backward하고 난 뒤, 최적화를 위해 graph를 지워버림(종말단의 gradient만 필요하므로)\n",
    "print(x.grad) \n",
    "y.backward(gradients) # 2번 미분해야하는 경우 retain_graph=True,여러번 호출 시, 미분 값이 더해져 중첩됨 \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1615388957807,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "6FqqU8PJrDY-",
    "outputId": "8cf73d8a-1b35-417d-f63b-40b9627c2f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4437, -1.3125], grad_fn=<AddBackward0>)\n",
      "tensor([-0.3328, -0.9844], grad_fn=<MulBackward0>)\n",
      "tensor([-0.0555, -0.1641], grad_fn=<DivBackward0>)\n",
      "<AddBackward0 object at 0x7f1db82fbc50>\n",
      "<MulBackward0 object at 0x7f1dbd2ee150>\n",
      "<DivBackward0 object at 0x7f1db8bb4390>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, requires_grad=True)\n",
    "y = x * 3\n",
    "z = x / 2\n",
    "w = x + y \n",
    "# 각기 다른 computation graph 가짐\n",
    "print(w,y,z,sep='\\n') \n",
    "print(w.grad_fn,y.grad_fn,z.grad_fn,sep='\\n') \n",
    "# grad_fn attr을 통하여 호출할 backpropagation class를 알 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 853,
     "status": "ok",
     "timestamp": 1615391373801,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "Hj0RQXOQtjKf"
   },
   "outputs": [],
   "source": [
    "# CAM을 이용한 CNN visualization 등에서 중간층 Layer의 Gradient값이 필요할 때가 있다.\n",
    "# hook을 사용하여 중간의 gradient 값을 저장해놓을 수 있다.\n",
    "# hooking : 양 객체 사이의 통신내용(메세지, 값, 이벤트, gradient 등)을 가져오는 것\n",
    "\n",
    "# register_hook(hook) : registers a backward hook\n",
    "# register_forward_hook(hook) : registers a forward hook\n",
    "class SimpleNet(nn.Module):# hook을 사용해보기위한 평범한 NN\n",
    "  def __init__(self):\n",
    "    super(SimpleNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "    self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "    self.pool2 = nn.MaxPool2d(2,2)\n",
    "    self.fc = nn.Linear(320, 50)\n",
    "    self.out = nn.Linear(50, 10)\n",
    "\n",
    "  def forward(self, input):\n",
    "    x = self.pool1(F.relu(self.conv1(input)))\n",
    "    x = self.pool2(F.relu(self.conv2(x)))\n",
    "    x = x.view(x.size(0),-1)\n",
    "    x = F.relu(self.fc(x))\n",
    "    x = F.relu(self.out(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1615391375518,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "cWlQRtL70sPY"
   },
   "outputs": [],
   "source": [
    "# hook을 사용하기 위해...\n",
    "# 1. Hook의 signature 정의\n",
    "# hook이 실행됬을 때 해야할 활동들을 정의\n",
    "# prototype에 맞게 해줘야함, self, input, output이 argument\n",
    "def hook_func(self, input, output):\n",
    "  # type(self) should be tensor\n",
    "  print('Inside ' + self.__class__.__name__ + ' forward')\n",
    "  print('')\n",
    "  print('input: ', type(input))\n",
    "  print('input[0]: ', type(input[0]))\n",
    "  print('output: ', type(output))\n",
    "  print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1135,
     "status": "ok",
     "timestamp": 1615393378653,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "GKWfcTMox-8_",
    "outputId": "311a9f7b-9127-4f8a-abfc-160145dd35d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.hooks.RemovableHandle object at 0x7f1db8206810>\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7f1db806f890>\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "h = net.conv1.register_forward_hook(hook_func) # 1번째 layer에 forward 시 hook 실행 등록\n",
    "print(h)\n",
    "print(net.conv2.register_forward_hook(hook_func)) # 2번째 layer에forward 시 hook 실행 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 802,
     "status": "ok",
     "timestamp": 1615393378654,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "HDMHJOXb1OWB",
    "outputId": "edf09867-282c-44ea-ee57-e6f37581d82f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d forward\n",
      "\n",
      "input:  <class 'tuple'>\n",
      "input[0]:  <class 'torch.Tensor'>\n",
      "output:  <class 'torch.Tensor'>\n",
      "\n",
      "Inside Conv2d forward\n",
      "\n",
      "input:  <class 'tuple'>\n",
      "input[0]:  <class 'torch.Tensor'>\n",
      "output:  <class 'torch.Tensor'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = net(input) # forward 시 hook function 실행되어 print 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1615393379828,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "U8P4Yc9b8j05",
    "outputId": "9e61fa7a-49b7-4d08-ae0d-ea07403d4217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d forward\n",
      "\n",
      "input:  <class 'tuple'>\n",
      "input[0]:  <class 'torch.Tensor'>\n",
      "output:  <class 'torch.Tensor'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h.remove() # register 지우기, 1번째 layer의 register return(=handler)을 받아 remove() 함수로 지움\n",
    "out = net(input) # 2번째 layer hook만 print됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1615392527357,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "DuPXzcvD2wKA",
    "outputId": "fe2ca6a1-3c2a-4632-9bab-cae3360652ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d forward\n",
      "\n",
      "input:  <class 'tuple'>\n",
      "input[0]:  <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# register_forward_hook(hook_pre) # layer의 forwar_pass 이전에 호줄되는 함수\n",
    "def hook_pre(self, input, output):\n",
    "  print('Inside ' + self.__class__.__name__ + ' forward')\n",
    "  print('')\n",
    "  print('input: ', type(input))\n",
    "  print('input[0]: ', type(input[0]))\n",
    "\n",
    "net = SimpleNet()\n",
    "net.conv1.register_forward_hook(hook_pre) # layer의 forwar_pass 이전에 호줄되는 함수\n",
    "\n",
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1615393211806,
     "user": {
      "displayName": "윤준석",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64",
      "userId": "08731269143009806715"
     },
     "user_tz": -540
    },
    "id": "qz8bHl6E5ocw",
    "outputId": "4505ad90-94c5-4d32-cacf-9e805f893040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside Conv2d backward\n",
      "Inside classs:Conv2d\n",
      "grad_input:  <class 'tuple'>\n",
      "grad_input[0]:  <class 'NoneType'>\n",
      "grad_output:  <class 'tuple'>\n",
      "grad_output[0]:  <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "# register_backward_hook(hook_grad) # layer의 backward_pass 이후에 호줄되는 함수\n",
    "# register_full_backward_hook(hook_grad)# forward시 autograd nodes가 많이 포함될 경우 full 버전을 써야함 \n",
    "\n",
    "def hook_grad(self, grad_input, grad_output): \n",
    "  # grad_input, grad_output을 내부 로직에서 바꾸면 안된다\n",
    "  # 굳이 바꾸고 싶으면 return new_grad_input, new_grad_output\n",
    "  # 새로운 input, output을 정해주고 return 해줘야함\n",
    "  print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "  print('Inside classs:' + self.__class__.__name__)\n",
    "\n",
    "  print('grad_input: ', type(grad_input))\n",
    "  print('grad_input[0]: ', type(grad_input[0]))\n",
    "  print('grad_output: ', type(grad_output))\n",
    "  print('grad_output[0]: ', type(grad_output[0]))\n",
    "\n",
    "net = SimpleNet()\n",
    "net.conv1.register_backward_hook(hook_grad) # backward_pass 이전에 호줄되는 함수\n",
    "# net.conv1.register_full_backward_hook(hook_grad) # forward시 autograd nodes가 많이 포함될 경우 full 버전을 써야함 \n",
    "\n",
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = net(input)\n",
    "\n",
    "target = torch.tensor([3], dtype=torch.long)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "err = loss_fn(out, target)\n",
    "err.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QF8s-3IP9CnZ"
   },
   "outputs": [],
   "source": [
    "save_feat = [] # featuremap을 저장할 전역 변수 실행\n",
    "def hook_feat(module, input, output): # hook function 정의\n",
    "  save_feat.append(output)\n",
    "  return output\n",
    "\n",
    "# model layer를 돌면서 원하는 layer에 hook 등록\n",
    "for name, module in model.get_model_shortcuts(): # model layer들을 가져오는 함수\n",
    "  if(name == 'target_layer_name'):\n",
    "    module.register_forward_hook(hook_feat)\n",
    "\n",
    "\n",
    "img = img.unsqueeze(0)\n",
    "s = model(img)[0]\n",
    "print(save_feat)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNAc5i9Uvm1VEdx2btsl7L6",
   "collapsed_sections": [],
   "name": "torch.autograd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
