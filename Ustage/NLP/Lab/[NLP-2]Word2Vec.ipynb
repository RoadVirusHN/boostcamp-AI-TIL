{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"2_word2vec.ipynb의 사본","provenance":[{"file_id":"1VTvW_xhCVskuAJokXST6RImd8CAZ7Pdr","timestamp":1613413438967}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"h3FAK0fz1kOr"},"source":["##**2. Word2Vec**\r\n","1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만듭니다.\r\n","2. CBOW, Skip-gram 모델을 각각 구현합니다.\r\n","3. 모델을 실제로 학습해보고 결과를 확인합니다."]},{"cell_type":"markdown","metadata":{"id":"u9FrxTPWIsct"},"source":["### **필요 패키지 import**"]},{"cell_type":"code","metadata":{"id":"QjroCdtwI9Rz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414852803,"user_tz":-540,"elapsed":6166,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"b64ae254-16f0-4387-de49-9850eb52d543"},"source":["!pip install konlpy"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting konlpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n","\u001b[K     |████████████████████████████████| 19.4MB 172kB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n","Collecting JPype1>=0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n","\u001b[K     |████████████████████████████████| 460kB 46.0MB/s \n","\u001b[?25hCollecting tweepy>=3.7.0\n","  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n","Collecting beautifulsoup4==4.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n","\u001b[K     |████████████████████████████████| 92kB 10.8MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: colorama, JPype1, tweepy, beautifulsoup4, konlpy\n","  Found existing installation: tweepy 3.6.0\n","    Uninstalling tweepy-3.6.0:\n","      Successfully uninstalled tweepy-3.6.0\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nSP7aXfJIr3i","executionInfo":{"status":"ok","timestamp":1613414856636,"user_tz":-540,"elapsed":9993,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["from tqdm import tqdm\r\n","from konlpy.tag import Okt\r\n","from torch import nn\r\n","from torch.nn import functional as F\r\n","from torch.utils.data import Dataset, DataLoader\r\n","from collections import defaultdict\r\n","\r\n","import torch\r\n","import copy\r\n","import numpy as np"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qugro74yJASr"},"source":["### **데이터 전처리**"]},{"cell_type":"markdown","metadata":{"id":"Q36dfSRRJDtX"},"source":["\r\n","\r\n","데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다.  \r\n","학습 데이터는 1번 실습과 동일하고, 테스트를 위한 단어를 아래와 같이 가정해봅시다."]},{"cell_type":"code","metadata":{"id":"CLZ2f-lRJSus","executionInfo":{"status":"ok","timestamp":1613414856637,"user_tz":-540,"elapsed":9989,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["train_data = [\r\n","  \"정말 맛있습니다. 추천합니다.\",\r\n","  \"기대했던 것보단 별로였네요.\",\r\n","  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\r\n","  \"완전 최고입니다! 재방문 의사 있습니다.\",\r\n","  \"음식도 서비스도 다 만족스러웠습니다.\",\r\n","  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\r\n","  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\r\n","  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\r\n","  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\r\n","  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \r\n","]\r\n","\r\n","test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vReElaFSLBYL"},"source":["Tokenization과 vocab을 만드는 과정은 이전 실습과 유사합니다."]},{"cell_type":"code","metadata":{"id":"dTjlRzmWMDK_","executionInfo":{"status":"ok","timestamp":1613414858323,"user_tz":-540,"elapsed":11669,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["tokenizer = Okt()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DTUsX672icp","executionInfo":{"status":"ok","timestamp":1613414858325,"user_tz":-540,"elapsed":11667,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["def make_tokenized(data):\r\n","  tokenized = []\r\n","  for sent in tqdm(data):\r\n","    tokens = tokenizer.morphs(sent, stem=True)\r\n","    tokenized.append(tokens)\r\n","\r\n","  return tokenized"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"C-z0z6HD2rrX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414863879,"user_tz":-540,"elapsed":17174,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"ce616402-e063-47eb-97d8-81a19a27a046"},"source":["train_tokenized = make_tokenized(train_data)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:05<00:00,  1.74it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"51exEpI0Mc3l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414864264,"user_tz":-540,"elapsed":17551,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"6e10b8ec-0751-4117-f141-fe9b08793e62"},"source":["word_count = defaultdict(int)\r\n","\r\n","for tokens in tqdm(train_tokenized):\r\n","  for token in tokens:\r\n","    word_count[token] += 1"],"execution_count":7,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 43509.38it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gyvHAMAnMh1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414864887,"user_tz":-540,"elapsed":18162,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"78e4f7d1-e711-4ca4-bf7e-25dea8bd6c76"},"source":["word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\r\n","print(list(word_count))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DaK_i3zL2vO3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414864888,"user_tz":-540,"elapsed":18135,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"80b1afac-8d14-4810-a9e9-4904bbd0b750"},"source":["w2i = {}\r\n","for pair in tqdm(word_count):\r\n","  if pair[0] not in w2i:\r\n","    w2i[pair[0]] = len(w2i)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["100%|██████████| 60/60 [00:00<00:00, 258907.65it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LiGqiEGDL5B_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414864888,"user_tz":-540,"elapsed":18127,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"ecd94e32-a7e8-4abf-ea57-f421505ebe45"},"source":["print(train_tokenized)\r\n","print(w2i)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n","{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vXA5zaPPM3Wd"},"source":["실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."]},{"cell_type":"code","metadata":{"id":"s47ssyVt89t1","executionInfo":{"status":"ok","timestamp":1613414864889,"user_tz":-540,"elapsed":18126,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["class CBOWDataset(Dataset):# 주변단어가 input, 중심단어가 output\r\n","  def __init__(self, train_tokenized, window_size=2):\r\n","    self.x = []\r\n","    self.y = []\r\n","\r\n","    for tokens in tqdm(train_tokenized):\r\n","      token_ids = [w2i[token] for token in tokens]\r\n","      for i, id in enumerate(token_ids):\r\n","        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n","          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n","          self.y.append(id)\r\n","\r\n","    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\r\n","    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n","\r\n","  def __len__(self):\r\n","    return self.x.shape[0]\r\n","\r\n","  def __getitem__(self, idx):\r\n","    return self.x[idx], self.y[idx]"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvInhQ33AMJv","executionInfo":{"status":"ok","timestamp":1613414864890,"user_tz":-540,"elapsed":18124,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["class SkipGramDataset(Dataset):# 중심단어가 input, 주변단어가 output\r\n","  def __init__(self, train_tokenized, window_size=2):\r\n","    self.x = []\r\n","    self.y = []\r\n","\r\n","    for tokens in tqdm(train_tokenized):\r\n","      token_ids = [w2i[token] for token in tokens]\r\n","      for i, id in enumerate(token_ids):\r\n","        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n","          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n","          self.x += [id] * 2 * window_size\r\n","\r\n","    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\r\n","    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n","\r\n","  def __len__(self):\r\n","    return self.x.shape[0]\r\n","\r\n","  def __getitem__(self, idx):\r\n","    return self.x[idx], self.y[idx]"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyAGV5IUUba0"},"source":["각 모델에 맞는 `Dataset` 객체를 생성합니다."]},{"cell_type":"code","metadata":{"id":"5ep7Hm6oBWyy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414864890,"user_tz":-540,"elapsed":18116,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"332d9e5f-1168-4340-bb8b-733accef57b9"},"source":["cbow_set = CBOWDataset(train_tokenized)\r\n","skipgram_set = SkipGramDataset(train_tokenized)\r\n","print(list(skipgram_set))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:00<00:00, 20301.57it/s]\n","100%|██████████| 10/10 [00:00<00:00, 3433.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1QSo73PoRyd9"},"source":["### **모델 Class 구현**"]},{"cell_type":"markdown","metadata":{"id":"jnnk44R6R28x"},"source":["차례대로 두 가지 Word2Vec 모델을 구현합니다.  \r\n","\r\n","\r\n","*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\r\n","*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\r\n"]},{"cell_type":"code","metadata":{"id":"b_HP1ISq5CWv","executionInfo":{"status":"ok","timestamp":1613414864891,"user_tz":-540,"elapsed":18114,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["class CBOW(nn.Module):\r\n","  def __init__(self, vocab_size, dim):\r\n","    super(CBOW, self).__init__()\r\n","    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n","    self.linear = nn.Linear(dim, vocab_size)\r\n","\r\n","  # B: batch size, W: window size, d_w: word embedding size, V: vocab size, 이런식으로 차원수를 추적하면서 실행하면 좋다.\r\n","  def forward(self, x):  # x: (B, 2W)\r\n","    embeddings = self.embedding(x)  # (B, 2W, d_w)\r\n","    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\r\n","    output = self.linear(embeddings)  # (B, V)\r\n","    return output"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQAUApww68MJ","executionInfo":{"status":"ok","timestamp":1613414865123,"user_tz":-540,"elapsed":18343,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["class SkipGram(nn.Module):\r\n","  def __init__(self, vocab_size, dim):\r\n","    super(SkipGram, self).__init__()\r\n","    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n","    self.linear = nn.Linear(dim, vocab_size)\r\n","\r\n","  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n","  def forward(self, x): # x: (B)\r\n","    embeddings = self.embedding(x)  # (B, d_w)\r\n","    output = self.linear(embeddings)  # (B, V)\r\n","    return output"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"58cJalkDWYMT"},"source":["두 가지 모델을 생성합니다."]},{"cell_type":"code","metadata":{"id":"8vWUXEi8WeM-","executionInfo":{"status":"ok","timestamp":1613414865124,"user_tz":-540,"elapsed":18341,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["cbow = CBOW(vocab_size=len(w2i), dim=256)\r\n","skipgram = SkipGram(vocab_size=len(w2i), dim=256)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xxP7qdtNWil1"},"source":["### **모델 학습**"]},{"cell_type":"markdown","metadata":{"id":"QVggZrQ4WpBS"},"source":["다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."]},{"cell_type":"code","metadata":{"id":"ygVdz5rSBeNu","executionInfo":{"status":"ok","timestamp":1613414865124,"user_tz":-540,"elapsed":18339,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["batch_size=4\r\n","learning_rate = 5e-4\r\n","num_epochs = 5\r\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n","\r\n","cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\r\n","skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ekixqKB3X5C1"},"source":["첫번째로 CBOW 모델 학습입니다."]},{"cell_type":"markdown","metadata":{"id":"8mBvPHiWCIeP"},"source":["**CBOW(Continuous Bag-of-Words)**\r\n","\r\n","- 주변 단어들을 가지고 중심 단어를 예측하는 방식으로 학습합니다.\r\n","- 주변 단어들의 one-hot encoding 벡터를 각각 embedding layer에 projection하여 각각의 embedding 벡터를 얻고 이 embedding들을 element-wise한 덧셈으로 합친 뒤, 다시 linear transformation하여 예측하고자 하는 중심 단어의 one-hot encoding 벡터와 같은 사이즈의 벡터로 만든 뒤, 중심 단어의 one-hot encoding 벡터와의 loss를 계산합니다.\r\n","- 예) \"A cute puppy is walking in the park.\" & window size: 2\r\n","  - Input(주변 단어): \"A\", \"cute\", \"is\", \"walking\"\r\n","  - Output(중심 단어): \"puppy\""]},{"cell_type":"code","metadata":{"id":"-d95qR7oC822","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414875453,"user_tz":-540,"elapsed":28661,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"9025ad72-aee7-4746-c5c8-e4bc59db8b1c"},"source":["cbow.train()\r\n","cbow = cbow.to(device)\r\n","optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\r\n","loss_function = nn.CrossEntropyLoss()\r\n","\r\n","for e in range(1, num_epochs+1):\r\n","  print(\"#\" * 50)\r\n","  print(f\"Epoch: {e}\")\r\n","  for batch in tqdm(cbow_loader):\r\n","    x, y = batch\r\n","    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n","    output = cbow(x)  # (B, V)\r\n"," \r\n","    optim.zero_grad()\r\n","    loss = loss_function(output, y)\r\n","    loss.backward()\r\n","    optim.step()\r\n","\r\n","    print(f\"Train loss: {loss.item()}\")\r\n","\r\n","print(\"Finished.\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["100%|██████████| 16/16 [00:00<00:00, 83.44it/s]\n","  0%|          | 0/16 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["##################################################\n","Epoch: 1\n","Train loss: 5.400465965270996\n","Train loss: 5.497161388397217\n","Train loss: 4.7446818351745605\n","Train loss: 5.193964958190918\n","Train loss: 4.940471649169922\n","Train loss: 5.513838291168213\n","Train loss: 5.023492336273193\n","Train loss: 5.240934371948242\n","Train loss: 5.6840972900390625\n","Train loss: 5.390736103057861\n","Train loss: 4.35294771194458\n","Train loss: 5.906587600708008\n","Train loss: 3.830533981323242\n","Train loss: 4.302803039550781\n","Train loss: 4.920823097229004\n","Train loss: 4.524956703186035\n","##################################################\n","Epoch: 2\n","Train loss: 5.193465232849121\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 16/16 [00:00<00:00, 603.00it/s]\n","100%|██████████| 16/16 [00:00<00:00, 591.18it/s]\n","100%|██████████| 16/16 [00:00<00:00, 647.11it/s]\n","100%|██████████| 16/16 [00:00<00:00, 649.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 5.369214057922363\n","Train loss: 4.618618488311768\n","Train loss: 5.058065891265869\n","Train loss: 4.813621520996094\n","Train loss: 5.214047431945801\n","Train loss: 4.822259426116943\n","Train loss: 5.1097002029418945\n","Train loss: 5.546432971954346\n","Train loss: 5.211984157562256\n","Train loss: 4.168797492980957\n","Train loss: 5.491884231567383\n","Train loss: 3.6983656883239746\n","Train loss: 4.194274425506592\n","Train loss: 4.74608850479126\n","Train loss: 4.399409294128418\n","##################################################\n","Epoch: 3\n","Train loss: 4.991948127746582\n","Train loss: 5.242608547210693\n","Train loss: 4.494771957397461\n","Train loss: 4.923862457275391\n","Train loss: 4.688762664794922\n","Train loss: 4.9228363037109375\n","Train loss: 4.624922275543213\n","Train loss: 4.980766296386719\n","Train loss: 5.412385940551758\n","Train loss: 5.038084983825684\n","Train loss: 3.991915464401245\n","Train loss: 5.091802597045898\n","Train loss: 3.568585157394409\n","Train loss: 4.088517189025879\n","Train loss: 4.574139595031738\n","Train loss: 4.2777934074401855\n","##################################################\n","Epoch: 4\n","Train loss: 4.795230865478516\n","Train loss: 5.117334365844727\n","Train loss: 4.3731207847595215\n","Train loss: 4.791327953338623\n","Train loss: 4.565791130065918\n","Train loss: 4.641474723815918\n","Train loss: 4.431588172912598\n","Train loss: 4.854059219360352\n","Train loss: 5.281740188598633\n","Train loss: 4.868793487548828\n","Train loss: 3.82267427444458\n","Train loss: 4.707857608795166\n","Train loss: 3.4412331581115723\n","Train loss: 3.9853689670562744\n","Train loss: 4.404925346374512\n","Train loss: 4.159715175628662\n","##################################################\n","Epoch: 5\n","Train loss: 4.602813720703125\n","Train loss: 4.993386268615723\n","Train loss: 4.253647804260254\n","Train loss: 4.660447120666504\n","Train loss: 4.444636821746826\n","Train loss: 4.371624946594238\n","Train loss: 4.242376804351807\n","Train loss: 4.729523658752441\n","Train loss: 5.154354095458984\n","Train loss: 4.704014301300049\n","Train loss: 3.661736011505127\n","Train loss: 4.342507362365723\n","Train loss: 3.3163604736328125\n","Train loss: 3.8846945762634277\n","Train loss: 4.238417625427246\n","Train loss: 4.044854164123535\n","Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FDahBf6IX4py"},"source":["다음으로 Skip-gram 모델 학습입니다."]},{"cell_type":"markdown","metadata":{"id":"HpcmPPBDBlvl"},"source":["**Skip-gram**\r\n","\r\n","- 중심 단어를 가지고 주변 단어들을 예측하는 방식으로 학습합니다.\r\n","- 중심 단어의 one-hot encoding 벡터를 embedding layer에 projection하여 해당 단어의 embedding 벡터를 얻고 이 벡터를 다시 linear transformation하여 예측하고자 하는 각각의 주변 단어들과의 one-hot encoding 벡터와 같은 사이즈의 벡터로 만든 뒤, 그 주변 단어들의 one-hot encoding 벡터와의 loss를 각각 계산합니다.\r\n","- 예) \"A cute puppy is walking in the park.\" & window size: 2\r\n","  - Input(중심 단어): \"puppy\"\r\n","  - Output(주변 단어): \"A\", \"cute\", \"is\", \"walking\""]},{"cell_type":"code","metadata":{"id":"jJxGEusqFV5r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414875939,"user_tz":-540,"elapsed":29138,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"90f25616-e5aa-47be-d6dc-2297d4cedf06"},"source":["skipgram.train()\r\n","skipgram = skipgram.to(device)\r\n","optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\r\n","loss_function = nn.CrossEntropyLoss()\r\n","\r\n","for e in range(1, num_epochs+1):\r\n","  print(\"#\" * 50)\r\n","  print(f\"Epoch: {e}\")\r\n","  for batch in tqdm(skipgram_loader):\r\n","    x, y = batch\r\n","    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n","    output = skipgram(x)  # (B, V)\r\n","\r\n","    optim.zero_grad()\r\n","    loss = loss_function(output, y)\r\n","    loss.backward()\r\n","    optim.step()\r\n","\r\n","    print(f\"Train loss: {loss.item()}\")\r\n","\r\n","print(\"Finished.\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/64 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["##################################################\n","Epoch: 1\n","Train loss: 3.957571268081665\n","Train loss: 3.998509645462036\n","Train loss: 4.128431797027588\n","Train loss: 4.149399280548096\n","Train loss: 4.745832443237305\n","Train loss: 4.339951515197754\n","Train loss: 4.190417766571045\n","Train loss: 4.260259628295898\n","Train loss: 4.578275680541992\n","Train loss: 4.0970330238342285\n","Train loss: 4.349617004394531\n","Train loss: 4.4171342849731445\n","Train loss: 4.195819854736328\n","Train loss: 4.793526649475098\n","Train loss: 4.422263145446777\n","Train loss: 4.181345462799072\n","Train loss: 4.199154853820801\n","Train loss: 4.504922866821289\n","Train loss: 4.373044013977051\n","Train loss: 4.181743621826172\n","Train loss: 4.497072696685791\n","Train loss: 4.043231964111328\n","Train loss: 3.951425313949585\n","Train loss: 4.185670852661133\n","Train loss: 4.201718330383301\n","Train loss: 4.35379695892334\n","Train loss: 4.145749568939209\n","Train loss: 4.365073204040527\n","Train loss: 4.257458686828613\n","Train loss: 4.244597434997559\n","Train loss: 4.429289817810059\n","Train loss: 3.8909928798675537\n","Train loss: 4.1299543380737305\n","Train loss: 4.3099894523620605\n","Train loss: 4.570613861083984\n","Train loss: 4.0926337242126465\n","Train loss: 4.159653186798096\n","Train loss: 4.369415760040283\n","Train loss: 4.422065734863281\n","Train loss: 3.7901439666748047\n","Train loss: 4.489071846008301\n","Train loss: 4.205597400665283\n","Train loss: 4.28570556640625\n","Train loss: 5.190889358520508\n","Train loss: 4.121967792510986\n","Train loss: 4.662091255187988\n","Train loss: 3.8430581092834473\n","Train loss: 4.276140213012695\n","Train loss: 3.7874460220336914\n","Train loss: 4.552602767944336\n","Train loss: 3.829028367996216\n","Train loss: 4.409868240356445\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 64/64 [00:00<00:00, 687.56it/s]\n","100%|██████████| 64/64 [00:00<00:00, 727.87it/s]\n","  0%|          | 0/64 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 4.503276824951172\n","Train loss: 4.5812177658081055\n","Train loss: 4.650382041931152\n","Train loss: 4.342751502990723\n","Train loss: 4.27174711227417\n","Train loss: 3.9923622608184814\n","Train loss: 4.660881996154785\n","Train loss: 3.926513671875\n","Train loss: 4.542387962341309\n","Train loss: 4.584965705871582\n","Train loss: 4.633040428161621\n","Train loss: 3.7982826232910156\n","##################################################\n","Epoch: 2\n","Train loss: 3.9361531734466553\n","Train loss: 3.9479012489318848\n","Train loss: 4.103466987609863\n","Train loss: 4.08169412612915\n","Train loss: 4.706428527832031\n","Train loss: 4.302283763885498\n","Train loss: 4.157321929931641\n","Train loss: 4.227934837341309\n","Train loss: 4.545372486114502\n","Train loss: 4.062566757202148\n","Train loss: 4.320302486419678\n","Train loss: 4.3782148361206055\n","Train loss: 4.165563583374023\n","Train loss: 4.762080192565918\n","Train loss: 4.3911590576171875\n","Train loss: 4.1500020027160645\n","Train loss: 4.1692214012146\n","Train loss: 4.480230331420898\n","Train loss: 4.338691234588623\n","Train loss: 4.151110649108887\n","Train loss: 4.377666473388672\n","Train loss: 3.9430394172668457\n","Train loss: 3.8922674655914307\n","Train loss: 4.164055347442627\n","Train loss: 4.161519527435303\n","Train loss: 4.283418655395508\n","Train loss: 4.100379943847656\n","Train loss: 4.330104827880859\n","Train loss: 4.2110443115234375\n","Train loss: 4.204730987548828\n","Train loss: 4.39990758895874\n","Train loss: 3.8612685203552246\n","Train loss: 4.096556663513184\n","Train loss: 4.2808380126953125\n","Train loss: 4.538697719573975\n","Train loss: 4.039169788360596\n","Train loss: 4.092617034912109\n","Train loss: 4.313900470733643\n","Train loss: 4.392245292663574\n","Train loss: 3.762667655944824\n","Train loss: 4.458685874938965\n","Train loss: 4.166388511657715\n","Train loss: 4.211626052856445\n","Train loss: 5.126995086669922\n","Train loss: 3.9832255840301514\n","Train loss: 4.538804054260254\n","Train loss: 3.7469193935394287\n","Train loss: 4.212550163269043\n","Train loss: 3.7602758407592773\n","Train loss: 4.521031379699707\n","Train loss: 3.7985854148864746\n","Train loss: 4.371273040771484\n","Train loss: 4.480218887329102\n","Train loss: 4.548525810241699\n","Train loss: 4.615420341491699\n","Train loss: 4.315861701965332\n","Train loss: 4.200676918029785\n","Train loss: 3.9684126377105713\n","Train loss: 4.627220630645752\n","Train loss: 3.891446828842163\n","Train loss: 4.509828090667725\n","Train loss: 4.549585342407227\n","Train loss: 4.604127407073975\n","Train loss: 3.7615065574645996\n","##################################################\n","Epoch: 3\n","Train loss: 3.9152543544769287\n","Train loss: 3.8976070880889893\n","Train loss: 4.078601837158203\n","Train loss: 4.0149993896484375\n"],"name":"stdout"},{"output_type":"stream","text":["\r100%|██████████| 64/64 [00:00<00:00, 743.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 4.667239189147949\n","Train loss: 4.265108108520508\n","Train loss: 4.124675750732422\n","Train loss: 4.195780277252197\n","Train loss: 4.512603759765625\n","Train loss: 4.028378963470459\n","Train loss: 4.291165828704834\n","Train loss: 4.339540481567383\n","Train loss: 4.135803699493408\n","Train loss: 4.73074197769165\n","Train loss: 4.360186576843262\n","Train loss: 4.119035720825195\n","Train loss: 4.139466762542725\n","Train loss: 4.455650329589844\n","Train loss: 4.304670333862305\n","Train loss: 4.120651721954346\n","Train loss: 4.2594804763793945\n","Train loss: 3.845550298690796\n","Train loss: 3.833815097808838\n","Train loss: 4.142523288726807\n","Train loss: 4.121757984161377\n","Train loss: 4.214048385620117\n","Train loss: 4.055435657501221\n","Train loss: 4.2957234382629395\n","Train loss: 4.165124893188477\n","Train loss: 4.165287494659424\n","Train loss: 4.370656490325928\n","Train loss: 3.831766128540039\n","Train loss: 4.063387870788574\n","Train loss: 4.252038478851318\n","Train loss: 4.5069899559021\n","Train loss: 3.9863619804382324\n","Train loss: 4.026553153991699\n","Train loss: 4.260464668273926\n","Train loss: 4.362805366516113\n","Train loss: 3.7355871200561523\n","Train loss: 4.428398132324219\n","Train loss: 4.127814769744873\n","Train loss: 4.139928817749023\n","Train loss: 5.0632853507995605\n","Train loss: 3.847896099090576\n","Train loss: 4.416831970214844\n","Train loss: 3.6536498069763184\n","Train loss: 4.149656295776367\n","Train loss: 3.733311891555786\n","Train loss: 4.48986291885376\n","Train loss: 3.768646001815796\n","Train loss: 4.333168029785156\n","Train loss: 4.457243919372559\n","Train loss: 4.516385078430176\n","Train loss: 4.580596923828125\n","Train loss: 4.289071083068848\n","Train loss: 4.130673885345459\n","Train loss: 3.9447498321533203\n","Train loss: 4.5938873291015625\n","Train loss: 3.856719970703125\n","Train loss: 4.477403163909912\n","Train loss: 4.514604568481445\n","Train loss: 4.575691223144531\n","Train loss: 3.7251508235931396\n"],"name":"stdout"},{"output_type":"stream","text":["\n","100%|██████████| 64/64 [00:00<00:00, 664.46it/s]\n","  0%|          | 0/64 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["##################################################\n","Epoch: 4\n","Train loss: 3.894866704940796\n","Train loss: 3.8476405143737793\n","Train loss: 4.053837776184082\n","Train loss: 3.9493579864501953\n","Train loss: 4.628266334533691\n","Train loss: 4.228429794311523\n","Train loss: 4.092483043670654\n","Train loss: 4.163799285888672\n","Train loss: 4.479972839355469\n","Train loss: 3.9944710731506348\n","Train loss: 4.262206077575684\n","Train loss: 4.301114559173584\n","Train loss: 4.106539726257324\n","Train loss: 4.699512481689453\n","Train loss: 4.329346656799316\n","Train loss: 4.088451385498047\n","Train loss: 4.109890937805176\n","Train loss: 4.431180477142334\n","Train loss: 4.270984649658203\n","Train loss: 4.090366363525391\n","Train loss: 4.142650604248047\n","Train loss: 3.751007080078125\n","Train loss: 3.776102304458618\n","Train loss: 4.1210737228393555\n","Train loss: 4.082438945770264\n","Train loss: 4.1457319259643555\n","Train loss: 4.01092529296875\n","Train loss: 4.261929512023926\n","Train loss: 4.119708061218262\n","Train loss: 4.126271724700928\n","Train loss: 4.341537952423096\n","Train loss: 3.802490234375\n","Train loss: 4.030453205108643\n","Train loss: 4.223586082458496\n","Train loss: 4.475498199462891\n","Train loss: 3.9342522621154785\n","Train loss: 3.9615559577941895\n","Train loss: 4.209245681762695\n","Train loss: 4.3337483406066895\n","Train loss: 3.7089033126831055\n","Train loss: 4.398207664489746\n","Train loss: 4.089890480041504\n","Train loss: 4.070816516876221\n","Train loss: 4.999763488769531\n","Train loss: 3.7163350582122803\n","Train loss: 4.296327590942383\n","Train loss: 3.563493251800537\n","Train loss: 4.087494850158691\n","Train loss: 3.7065553665161133\n","Train loss: 4.45909309387207\n","Train loss: 3.739213466644287\n","Train loss: 4.295562744140625\n","Train loss: 4.43435001373291\n","Train loss: 4.4847917556762695\n","Train loss: 4.545914649963379\n","Train loss: 4.262384414672852\n","Train loss: 4.061783790588379\n","Train loss: 3.9213762283325195\n","Train loss: 4.560885429382324\n","Train loss: 3.8223354816436768\n","Train loss: 4.445117950439453\n","Train loss: 4.480021953582764\n","Train loss: 4.547727584838867\n","Train loss: 3.6892266273498535\n","##################################################\n","Epoch: 5\n","Train loss: 3.8749823570251465\n","Train loss: 3.7980165481567383\n","Train loss: 4.0291748046875\n","Train loss: 3.884813070297241\n","Train loss: 4.589511871337891\n","Train loss: 4.192251205444336\n","Train loss: 4.060749053955078\n","Train loss: 4.131994247436523\n","Train loss: 4.447481632232666\n","Train loss: 3.9608452320098877\n"],"name":"stdout"},{"output_type":"stream","text":["\r100%|██████████| 64/64 [00:00<00:00, 754.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 4.233422756195068\n","Train loss: 4.262939929962158\n","Train loss: 4.0777716636657715\n","Train loss: 4.668393135070801\n","Train loss: 4.298642635345459\n","Train loss: 4.058252811431885\n","Train loss: 4.080495834350586\n","Train loss: 4.406820297241211\n","Train loss: 4.237636566162109\n","Train loss: 4.060253620147705\n","Train loss: 4.027334213256836\n","Train loss: 3.659658670425415\n","Train loss: 3.719160318374634\n","Train loss: 4.099705696105957\n","Train loss: 4.043569564819336\n","Train loss: 4.07851505279541\n","Train loss: 3.9668571949005127\n","Train loss: 4.228720664978027\n","Train loss: 4.07480525970459\n","Train loss: 4.087691783905029\n","Train loss: 4.312553405761719\n","Train loss: 3.7734408378601074\n","Train loss: 3.9977571964263916\n","Train loss: 4.195476055145264\n","Train loss: 4.444230556488037\n","Train loss: 3.8828835487365723\n","Train loss: 3.8977320194244385\n","Train loss: 4.160377502441406\n","Train loss: 4.3050737380981445\n","Train loss: 3.6826171875\n","Train loss: 4.368112087249756\n","Train loss: 4.052628517150879\n","Train loss: 4.0044941902160645\n","Train loss: 4.936428546905518\n","Train loss: 3.588914632797241\n","Train loss: 4.177463531494141\n","Train loss: 3.4766948223114014\n","Train loss: 4.026103973388672\n","Train loss: 3.680006504058838\n","Train loss: 4.428714275360107\n","Train loss: 3.7102890014648438\n","Train loss: 4.25846529006958\n","Train loss: 4.411534786224365\n","Train loss: 4.453743934631348\n","Train loss: 4.51137638092041\n","Train loss: 4.235801696777344\n","Train loss: 3.9940547943115234\n","Train loss: 3.898293972015381\n","Train loss: 4.528217315673828\n","Train loss: 3.7882959842681885\n","Train loss: 4.4129767417907715\n","Train loss: 4.445833206176758\n","Train loss: 4.520232677459717\n","Train loss: 3.6537439823150635\n","Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Pi0sbHV6dEOR"},"source":["### **테스트**"]},{"cell_type":"markdown","metadata":{"id":"WGarLWxXeJvz"},"source":["학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."]},{"cell_type":"code","metadata":{"id":"4A1wrl-L_RjF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414876304,"user_tz":-540,"elapsed":29496,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"f5d81c58-6bf4-432d-fef7-b403874c66d7"},"source":["for word in test_words:\r\n","  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n","  emb = cbow.embedding(input_id)\r\n","\r\n","  print(f\"Word: {word}\")\r\n","  print(emb.squeeze(0))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Word: 음식\n","tensor([ 0.6851,  0.4828, -0.3156,  0.2014,  0.0701, -1.5939, -1.4075, -0.0692,\n","         1.0438, -0.9316, -0.2956,  0.0892, -0.1724,  0.8037,  1.1360, -0.5427,\n","         1.3115,  0.5798,  0.3898,  0.7903,  0.0683, -2.6461, -0.5415, -0.4043,\n","        -0.5969, -0.8820, -1.3398,  1.2012, -0.8177, -1.8458, -0.2342, -0.8089,\n","        -0.1872, -0.0527,  1.1812, -2.2949,  0.9804,  0.2554,  2.3396, -1.2162,\n","        -0.8000,  0.2590,  0.3042,  0.6546,  0.4970,  0.6004, -2.1689,  1.6548,\n","        -0.6053,  1.2208,  1.2702, -1.0061,  0.0506, -0.6449, -1.0355, -0.5834,\n","         0.4870,  0.3367,  0.4290,  0.5942,  1.4122,  0.4739, -0.0477, -0.6388,\n","        -0.5289, -0.4685, -0.0247,  0.6007, -1.2811,  1.9487, -0.7122,  0.6895,\n","         1.5356, -0.4739,  0.5500, -0.7482, -1.0250, -1.0849,  0.5584, -0.8335,\n","         1.1975, -0.2579,  0.5554,  0.1727,  1.7219,  0.8122, -0.7458,  1.0250,\n","        -0.7357, -1.5008,  0.1059,  0.9300,  1.0316,  0.1732,  0.0671,  1.2842,\n","        -0.2883, -0.8419, -1.3026,  0.1010, -0.7718, -0.2808, -1.3433,  0.9775,\n","        -1.4862,  0.6736, -1.7790,  0.0280, -1.3097,  0.6465, -0.9369,  0.1611,\n","         0.5074, -0.6320, -2.2054,  0.2321, -0.3681, -0.3657, -2.3408, -0.4037,\n","         0.9395, -0.0501,  1.0015,  0.2610,  1.2324, -0.9829,  0.9562, -0.0842,\n","         0.0421, -0.3578,  1.5988, -0.9507,  0.0335,  2.1868, -0.8295, -0.2213,\n","         2.5084, -2.0606, -1.2325, -1.5936,  0.5086,  0.1887, -0.6922,  0.4697,\n","         1.3694, -0.6434, -0.4767, -0.4451, -0.4273,  0.0597,  0.9432, -1.2299,\n","         0.6671, -0.1201, -0.8211,  0.9390, -0.7129, -1.5986,  1.3201,  0.3299,\n","        -0.9607, -1.9487,  0.7449,  1.1401,  1.7412, -0.1868, -0.2875,  1.3408,\n","         1.2346,  0.6895, -1.1150,  0.0197,  1.0969,  0.1750, -0.3785,  0.2225,\n","         0.4917,  0.3266,  0.7838, -0.5810, -0.1501, -0.4743,  0.5952, -0.0543,\n","        -0.4475, -0.1842,  0.1024,  0.5464,  2.0716,  0.6765,  0.5483, -1.5624,\n","         1.4872,  0.5863,  0.3074,  0.0836, -1.1875, -1.1711,  0.8743, -0.3893,\n","         0.9939, -2.1172,  1.7199,  0.4427, -1.1706,  0.1828,  0.8494, -1.4068,\n","        -0.5704,  0.9736, -1.8269,  1.7010,  0.7269,  0.4180, -0.0161, -1.1051,\n","         0.6729,  1.4356,  1.0520,  0.6175, -1.5603, -0.2139,  0.1329,  1.4372,\n","         1.2518, -1.1936, -0.9419, -0.1921,  1.7195,  0.0546,  1.3894,  0.6721,\n","        -0.3296,  0.9766, -3.3989, -0.2169, -0.4557, -1.5960, -0.0417,  0.4814,\n","        -0.9305, -0.4997,  1.4857, -0.5628, -0.4477, -0.1164,  0.6973, -0.2168,\n","        -0.8557, -0.0940,  0.3068, -0.0688,  0.2676,  1.5549, -1.6525,  1.2826],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 맛\n","tensor([ 1.6531e-02, -1.0893e+00, -7.9467e-01, -1.2984e+00,  1.3237e-01,\n","         1.2020e-01, -8.5334e-01,  1.5421e+00, -4.0721e-01,  1.0943e+00,\n","        -5.7396e-01,  3.3489e-01, -2.3558e-02,  5.3852e-01,  8.9880e-01,\n","         7.1704e-01,  1.0287e+00, -9.5184e-01, -5.0396e-02,  7.3398e-01,\n","         9.9147e-01,  7.0628e-02, -5.1135e-02, -2.4046e-01,  8.2686e-01,\n","        -1.1355e-01, -1.2547e+00,  1.0227e+00, -1.9696e+00,  2.4285e-01,\n","         3.8862e-01, -2.0036e+00, -6.3838e-01, -2.8436e-01,  1.4604e+00,\n","         1.9570e-02,  5.2867e-01,  5.7868e-01, -1.1065e+00,  4.9345e-01,\n","         4.6496e-01,  4.3827e-02, -2.2200e-01, -6.9161e-01,  1.3967e+00,\n","         1.9750e+00,  2.2479e+00,  8.7345e-01, -7.9674e-01, -1.4355e+00,\n","         5.3789e-01,  7.1204e-01, -3.5485e-01,  5.1130e-02, -9.6499e-01,\n","         3.0181e-01, -1.0732e-01,  2.0582e+00,  2.6568e-01,  8.3698e-01,\n","         2.2503e-01,  1.6195e+00, -1.2643e+00, -1.2107e+00,  3.6441e-04,\n","        -2.3999e+00, -2.2236e-01, -3.4357e-01, -1.9383e+00, -8.9082e-01,\n","         3.1660e-01, -8.1722e-01, -3.9196e-01, -4.6051e-01, -1.0476e+00,\n","         1.1931e+00, -3.9544e-01,  4.0407e-01, -1.8200e-01,  3.2345e-01,\n","         1.5486e-01,  7.0949e-01,  1.1697e+00, -5.4895e-01,  1.8408e-01,\n","         7.9040e-01, -5.8611e-01, -6.3827e-01, -1.1917e+00,  1.4934e+00,\n","         7.4992e-01,  2.7611e-01, -5.2272e-02,  1.8848e+00,  4.9197e-01,\n","        -2.0814e-01,  1.4232e+00, -1.0053e+00, -6.9741e-01, -1.8806e+00,\n","        -1.1903e-01,  7.8897e-01,  6.8760e-01,  5.9094e-01,  2.2692e-01,\n","        -4.5625e-01, -2.7751e+00, -3.9000e-01, -4.4564e-01,  9.1363e-01,\n","         1.8229e+00,  1.1252e+00,  4.6668e-02, -3.8538e-01,  1.0185e+00,\n","        -1.7812e-01,  1.5174e-01, -7.2241e-01,  1.6029e+00, -4.6135e-01,\n","        -9.8338e-01, -6.4770e-01, -9.0041e-01, -7.3185e-01, -1.0169e+00,\n","        -1.0713e+00,  5.0793e-01, -4.4975e-01, -1.2829e+00, -3.8281e-01,\n","        -1.3338e+00,  4.7708e-01,  7.1418e-01,  1.2627e+00, -7.7088e-02,\n","        -1.2127e+00,  2.5740e-01, -6.2369e-01, -9.8085e-02,  3.3432e-01,\n","        -5.8522e-01,  1.2195e+00, -6.7107e-01,  5.9360e-01, -7.3980e-02,\n","        -6.8990e-01, -4.5406e-01,  1.7130e+00, -1.2041e+00, -1.2256e+00,\n","        -5.3975e-01,  5.9263e-01,  1.1393e+00, -1.5281e+00, -2.0968e-01,\n","        -1.2789e+00, -2.7697e-01,  3.1631e-01, -1.1764e+00,  2.0868e+00,\n","        -9.8494e-01,  2.9078e-01,  4.9272e-02,  2.6212e+00,  3.9262e-01,\n","         2.0698e-01,  2.3843e+00,  1.3421e+00, -3.3263e-01, -1.6644e+00,\n","        -9.1057e-02,  7.3633e-01, -8.8005e-01, -1.5030e+00,  1.3797e+00,\n","         1.0510e+00, -1.1767e+00,  6.4076e-01, -3.2837e-01,  1.3078e+00,\n","         3.5632e-01, -2.3279e-01,  1.1316e+00,  4.0425e-01, -7.7124e-01,\n","        -6.7352e-01,  6.1375e-01,  6.3601e-01,  2.0767e+00,  1.2541e+00,\n","        -2.7654e-01,  7.7308e-01, -2.2789e-01,  3.5633e-01, -1.4872e+00,\n","         1.7538e-01,  8.1377e-01,  1.1174e+00,  1.8156e+00, -1.8563e+00,\n","        -4.2337e-01, -1.5739e+00,  4.4401e-01, -8.2046e-01,  1.2241e+00,\n","         1.2031e+00, -2.8228e+00,  8.5753e-01,  4.6530e-01, -5.3372e-01,\n","         1.8068e-01, -4.2778e-01, -3.0143e-01,  6.8180e-01, -7.4585e-03,\n","        -2.9487e-01,  4.6333e-02, -3.1414e-01,  8.8579e-01, -9.0128e-01,\n","        -6.7319e-01,  2.1125e-01, -1.1993e+00,  2.7758e-01, -2.2374e-01,\n","         4.3408e-02, -9.4646e-01, -7.1786e-01, -5.2794e-02,  1.0306e+00,\n","        -9.1255e-01, -9.7442e-01,  1.4808e+00,  1.7789e+00,  6.9606e-01,\n","         1.5304e+00, -5.3498e-01,  5.9073e-02, -1.2720e+00,  1.4908e+00,\n","        -4.5804e-01,  2.0344e+00, -1.1112e+00,  1.3011e+00, -2.0844e-01,\n","         1.4597e-01, -1.3576e+00, -7.7518e-01, -1.2247e-01,  1.1644e+00,\n","         1.6579e+00, -2.5660e+00,  8.4549e-01,  1.0927e-01, -1.0111e+00,\n","         5.7530e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 서비스\n","tensor([ 0.7262, -0.6736,  0.3490,  1.8557,  1.5022,  0.1922,  0.2225, -0.0035,\n","         0.2227,  1.9149, -1.2457,  0.4721, -0.9202, -0.2934,  0.0207, -0.0691,\n","        -1.2772, -1.7622,  1.5001, -0.7199,  0.2186,  0.1643,  1.1756, -0.3681,\n","         0.0096, -1.9984, -1.1378, -0.5705,  0.1276,  0.0230, -0.5728,  0.4464,\n","         0.1816,  0.8569, -0.1667,  0.4578, -1.4237,  2.3108, -1.2772,  0.2933,\n","         0.1136,  0.9448,  0.1629, -0.6111, -0.6535,  0.3520,  1.1579,  0.3259,\n","        -0.3863,  1.9579, -0.5902, -0.0435, -1.0418, -0.9236, -1.3336,  0.1704,\n","         0.2900, -1.9262,  0.9518,  0.0445,  0.1420,  2.1244, -1.4468,  0.2488,\n","        -0.4275, -0.4354, -0.7191, -0.9787,  2.3547,  0.5693,  1.0504, -0.5258,\n","        -0.7202, -0.0775,  0.7723, -0.4684, -0.3505, -1.3064, -0.0335, -0.6426,\n","         1.3437, -0.3532,  1.5131,  0.5139, -0.0038,  1.2540,  0.7622, -0.2433,\n","         1.1609,  0.5033,  0.5896, -0.7392,  0.9184, -0.7966,  1.1515,  1.0099,\n","        -1.6931, -0.2921, -0.4036,  1.6432,  0.2225,  0.9730, -0.5194, -0.5468,\n","        -0.0899,  1.7660, -1.1677,  1.3037,  0.1546, -1.0963,  0.1031,  0.4240,\n","        -1.2174,  0.2688,  0.6060,  0.8489, -0.2744, -0.6686,  0.2719, -0.0801,\n","         2.3157, -1.0299, -0.2683,  0.0443,  0.5655,  0.8392, -2.7567, -0.7592,\n","         1.1016, -0.6688, -1.1796,  1.6021, -0.8591,  1.2423,  1.0629,  0.2351,\n","        -1.0720, -0.0686,  1.3040,  0.8743, -0.2959,  0.0608,  0.0470, -0.1803,\n","        -2.0572,  0.2737,  0.7105,  0.4388,  0.7963,  1.3364, -1.0718, -1.0590,\n","        -0.6258, -1.7465, -1.7995, -1.0200, -0.9963,  1.0813, -0.2839, -1.6171,\n","         0.4685,  0.9469,  1.4021,  0.2060,  1.1282,  0.2418, -1.3943, -1.2062,\n","         0.2870, -1.9983, -0.0840, -0.1170,  0.0568,  0.0857, -0.7442,  2.7720,\n","        -0.3325,  0.2063, -0.9479, -0.0179, -0.6120,  1.4217, -0.0289,  1.6849,\n","        -0.6233, -0.3311,  0.5749, -0.7049, -0.5248, -0.9431, -2.0883,  1.4291,\n","         1.4177, -0.1845,  1.1088, -0.4180, -1.5608,  1.5003,  0.1936,  0.0745,\n","        -0.5745, -0.6551, -0.3076, -1.4318,  0.0126,  0.7277,  0.3122, -1.2662,\n","         1.3499,  0.6217, -0.8456, -2.5543,  0.2954,  0.9700,  0.1372,  0.3617,\n","         1.0574, -0.7091, -0.7876,  1.2461,  0.0862, -1.1070, -1.8032, -0.7366,\n","         0.1209,  0.3019, -0.5519, -1.1605, -0.8256, -0.3742, -2.3115,  0.6083,\n","         0.6951, -1.2022, -0.8569, -1.7632,  1.5353, -0.8682,  0.0926,  0.2233,\n","        -0.0170,  1.0994,  0.7310, -1.1533,  0.0628,  0.4377,  0.5629,  0.8703,\n","        -1.0709,  0.1674,  0.7584, -0.3938, -0.3598,  0.4487, -0.0333,  0.0835],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 위생\n","tensor([-0.7714,  0.3701, -0.0266, -1.1638, -1.4997, -0.8435,  0.4484,  0.2117,\n","         1.3868, -0.5371,  1.1466, -0.5468, -0.3842,  0.2107, -0.4141,  0.6170,\n","         0.6639, -0.7688,  0.9274, -0.4626, -1.3075,  0.9394,  2.0198,  0.3338,\n","         1.1102, -1.9017, -0.5618, -0.6728,  0.9044, -0.6559,  2.9338, -0.1654,\n","         1.4730, -0.7577, -1.8448,  0.7126, -0.5839, -0.9061, -1.2527,  0.3608,\n","         0.1353, -0.5838,  0.3466,  0.5664,  0.5836, -2.1548, -0.5916,  1.0560,\n","        -0.4747,  0.5873, -0.2603,  0.0051,  0.8679,  1.9800, -1.2296,  0.0897,\n","         0.8794,  0.5581,  0.5792,  1.4617,  0.9206,  0.5930, -0.8142,  0.9663,\n","        -0.8729,  0.2633, -0.9506,  0.5496, -0.9531,  1.0622, -1.8629,  0.5804,\n","         0.5525,  2.4046,  1.9682,  1.7271, -1.2395,  1.0046,  0.3375, -1.1651,\n","        -0.1189,  0.0751, -2.5684, -0.0125,  0.9379,  0.6100, -0.9365,  0.1542,\n","        -0.2364, -0.3269, -0.6998, -0.5540, -1.5295,  0.1541,  0.3269, -0.0906,\n","        -0.3310, -0.6083, -1.4978,  1.1442,  1.2482,  1.7688,  0.4617, -1.3533,\n","        -0.5458,  1.8881,  0.7219,  1.1012, -1.2433, -1.1225,  1.1468,  0.5018,\n","        -1.4252,  0.1267, -1.0665,  0.1168, -0.9280, -0.9550, -0.7857, -0.8528,\n","        -0.1992, -0.5431,  0.2049, -0.3134,  0.7150,  0.7747, -1.6208, -1.1854,\n","         2.0863, -0.2941,  0.4671, -0.9960, -1.6661,  0.1053, -0.6065,  1.9026,\n","         0.3875,  1.1525, -0.0506,  0.9286, -0.7818, -2.3835, -1.3642,  0.0880,\n","        -0.9296, -0.3440, -0.6966,  1.5805,  0.5316, -0.2984, -1.5071,  1.8303,\n","        -0.9936,  0.0282, -0.4450,  0.6684,  0.3418, -0.4892, -0.3793,  1.1246,\n","         0.3436,  0.4604,  0.0192,  0.3380, -0.0572,  1.5619, -0.1676,  0.0771,\n","        -0.3054,  0.3342,  0.3383, -0.7969, -1.3395, -0.3267, -0.5779,  0.4830,\n","         1.6568, -0.7284, -0.4853,  0.5179,  1.8772, -1.1710,  1.4812,  0.1464,\n","         0.4490, -1.0987,  0.8628, -0.9582, -0.2089,  0.0867, -1.8887,  0.2093,\n","        -0.3643,  0.8925,  0.2886,  2.1304,  0.4467,  0.2471, -0.4258,  0.3498,\n","        -0.1379,  0.2677,  1.0504,  1.6620,  0.8609,  0.7157,  0.8736, -0.3186,\n","         0.0438, -0.2767,  0.5056,  0.7443, -0.7301, -0.4211,  0.2585,  0.0416,\n","         1.0661,  0.1472, -0.0596, -0.5329,  0.5958, -1.0383,  0.1826,  0.2308,\n","        -0.3851, -1.2117, -1.4785, -0.7630,  1.9376,  1.3043, -1.3539,  0.4907,\n","         1.3500,  1.3703, -0.3190,  0.1645, -0.8274, -0.4736,  0.4222, -1.9081,\n","         0.9072, -0.4380, -0.0874, -0.0975,  0.2132,  0.2136,  0.1450, -2.2283,\n","         0.0055,  1.1565, -0.1786,  0.4337,  0.7580,  2.3162,  1.5518, -0.6962],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)\n","Word: 가격\n","tensor([-6.7200e-01,  7.4431e-02, -7.1563e-01, -1.3670e+00,  5.7067e-01,\n","         5.4477e-01, -5.8952e-01, -6.9904e-02,  2.2157e-01, -6.1628e-01,\n","         1.2459e+00,  1.3608e+00, -1.1135e+00,  3.3309e-01,  5.6867e-01,\n","         8.6140e-01, -2.9133e-01,  5.1029e-01, -6.9016e-01, -8.7144e-01,\n","         1.1637e+00,  3.5127e-01,  6.1477e-02, -2.3996e+00,  1.6264e-01,\n","         9.7038e-01,  6.2454e-01,  2.2488e+00,  2.8498e-03,  2.1852e-01,\n","         6.8715e-01,  1.1529e+00,  2.4664e+00,  1.1132e+00, -1.2057e-01,\n","        -8.9756e-01,  1.3697e-01,  1.5312e+00,  8.9042e-01,  1.4495e+00,\n","         6.7171e-01,  1.7467e-01, -7.0268e-02,  4.6314e-03,  1.8497e+00,\n","        -3.6350e-01, -4.0424e-01, -1.3875e-01,  2.2423e-02,  8.2526e-01,\n","         1.1766e+00,  4.2189e-01,  2.9954e+00,  1.0738e-01,  2.1494e+00,\n","         9.5587e-01,  3.2836e-01, -8.5968e-01,  1.2102e+00, -8.7478e-01,\n","        -3.1163e-01,  3.4848e-01, -7.0708e-01, -6.9295e-01, -1.3831e+00,\n","        -9.3203e-01,  9.4955e-02, -1.2426e+00, -5.6748e-01,  1.4977e+00,\n","        -8.9083e-01, -5.3929e-01, -9.1974e-01,  4.6610e-01,  9.2328e-01,\n","        -1.0454e+00,  6.6210e-01,  4.9900e-01,  1.1937e+00, -2.0218e+00,\n","         4.8524e-01, -1.4762e+00, -6.2110e-01, -3.6794e-02, -7.7312e-01,\n","        -1.8645e+00, -7.6218e-01, -1.4204e-01,  9.4854e-01,  3.3156e-02,\n","        -1.2873e-01, -1.9792e-01,  3.1707e-01, -1.4032e+00, -8.2849e-01,\n","         1.2445e-01, -2.3250e+00,  1.9625e-01, -3.4413e-01,  1.1566e+00,\n","         5.4480e-02,  1.0160e-01, -1.1455e+00,  1.2083e+00,  3.4214e-02,\n","        -1.2174e+00, -4.4318e-01, -4.2970e-01, -2.1227e+00, -4.1384e-01,\n","         1.3864e+00,  9.0559e-01, -4.5747e-01, -1.1158e+00,  4.9566e-01,\n","         1.5916e-02, -1.7004e+00, -2.7355e-01, -2.2543e-01, -2.0656e-01,\n","        -3.4427e-01,  2.0591e-01,  1.0995e+00,  2.6083e-01, -2.0289e+00,\n","         1.1770e-01, -6.7901e-01, -9.5534e-01,  1.0759e+00, -1.6131e+00,\n","        -1.8025e-01, -1.1153e+00, -6.6770e-01, -1.0131e+00, -2.6719e-01,\n","         2.9544e-01,  6.9402e-01,  1.5472e+00, -4.5694e-01,  1.2868e+00,\n","        -3.1501e+00, -1.1023e+00,  6.4631e-01, -9.6185e-02, -1.2063e+00,\n","         7.1953e-01,  1.9014e-01,  1.2758e+00, -1.5726e+00,  1.7325e+00,\n","         1.2964e+00, -1.0456e+00, -1.8307e-01, -2.6446e-02,  2.3780e+00,\n","         3.7497e-01,  5.8371e-01, -7.4818e-01, -1.6072e+00, -2.8133e-01,\n","         3.2958e-02, -2.2332e+00, -1.1120e+00, -8.9610e-01,  1.1359e+00,\n","        -8.1823e-02,  4.6433e-01, -5.9067e-01,  1.7376e-01,  2.2049e-01,\n","         3.8089e-01,  1.1116e+00,  1.0956e+00, -1.1948e+00,  1.7416e-01,\n","         1.6900e+00, -4.9571e-01,  6.3682e-01, -2.4574e-01, -4.8277e-01,\n","        -1.8612e+00, -8.0687e-01,  1.0224e+00, -7.2680e-02, -1.5317e+00,\n","        -3.3530e-01,  1.4499e+00, -1.6044e+00, -1.3659e+00, -1.1463e+00,\n","        -1.0140e-02,  7.6177e-01, -1.1986e-01,  1.1233e-01,  1.3456e+00,\n","        -8.5860e-01, -1.9118e+00,  3.7886e-01,  1.6397e-01,  9.6731e-01,\n","        -8.7728e-01,  1.3798e+00, -1.9236e-02, -9.7794e-02, -6.8381e-01,\n","        -4.9180e-01, -3.1850e-01, -3.9583e-01,  5.8042e-01, -6.3674e-01,\n","         4.5137e-01,  1.0159e+00,  1.1647e-01,  6.4745e-01,  3.7231e-01,\n","         5.9943e-01, -2.4274e-02,  4.3746e-01, -7.7542e-01, -5.9960e-01,\n","        -3.0759e-01, -4.5525e-01,  3.9132e-01,  6.0974e-01,  4.2710e-01,\n","        -4.3839e-01,  6.2133e-01, -5.4795e-01, -8.1751e-01,  1.3447e+00,\n","        -1.2131e+00,  5.7501e-01, -7.4456e-01, -5.3473e-03,  1.5280e-01,\n","         1.7393e+00,  8.5715e-01, -1.2239e+00, -1.3903e+00,  4.0256e-01,\n","        -2.3257e+00,  3.3477e-01,  1.0026e+00, -1.0934e+00,  4.9697e-01,\n","         7.7254e-01, -5.9933e-02, -3.4811e-01, -1.2912e-02,  1.2819e+00,\n","         4.1275e-01, -1.3088e+00, -9.1303e-01,  3.3197e-01, -8.3110e-01,\n","         1.0013e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_l5cPRZZe-R4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613414876305,"user_tz":-540,"elapsed":29491,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"4b1984e1-e3b4-4fd8-fae1-c77e58fa4236"},"source":["for word in test_words:\r\n","  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n","  emb = skipgram.embedding(input_id)\r\n","\r\n","  print(f\"Word: {word}\")\r\n","  print(max(emb.squeeze(0)))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Word: 음식\n","tensor(2.7137, device='cuda:0', grad_fn=<UnbindBackward>)\n","Word: 맛\n","tensor(2.8000, device='cuda:0', grad_fn=<UnbindBackward>)\n","Word: 서비스\n","tensor(2.5724, device='cuda:0', grad_fn=<UnbindBackward>)\n","Word: 위생\n","tensor(2.8582, device='cuda:0', grad_fn=<UnbindBackward>)\n","Word: 가격\n","tensor(3.4897, device='cuda:0', grad_fn=<UnbindBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PmN4IchwisOO","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1613414936955,"user_tz":-540,"elapsed":90137,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"1f879781-b519-4593-f3ae-df47550d61fd"},"source":["!apt-get install -qq texlive texlive-xetex texlive-latex-extra pandoc\r\n","!pip install -qq pypandoc\r\n","\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","\r\n","!jupyter nbconvert --to PDF '/content/drive/My Drive/Colab Notebooks/1_naive_bayes.ipynb의 사본'"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Extracting templates from packages: 100%\n","Preconfiguring packages ...\n","Selecting previously unselected package fonts-droid-fallback.\n","(Reading database ... 146425 files and directories currently installed.)\n","Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n","Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n","Selecting previously unselected package fonts-lato.\n","Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n","Unpacking fonts-lato (2.0-2) ...\n","Selecting previously unselected package poppler-data.\n","Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n","Unpacking poppler-data (0.4.8-2) ...\n","Selecting previously unselected package tex-common.\n","Preparing to unpack .../03-tex-common_6.09_all.deb ...\n","Unpacking tex-common (6.09) ...\n","Selecting previously unselected package fonts-lmodern.\n","Preparing to unpack .../04-fonts-lmodern_2.004.5-3_all.deb ...\n","Unpacking fonts-lmodern (2.004.5-3) ...\n","Selecting previously unselected package fonts-noto-mono.\n","Preparing to unpack .../05-fonts-noto-mono_20171026-2_all.deb ...\n","Unpacking fonts-noto-mono (20171026-2) ...\n","Selecting previously unselected package fonts-texgyre.\n","Preparing to unpack .../06-fonts-texgyre_20160520-1_all.deb ...\n","Unpacking fonts-texgyre (20160520-1) ...\n","Selecting previously unselected package javascript-common.\n","Preparing to unpack .../07-javascript-common_11_all.deb ...\n","Unpacking javascript-common (11) ...\n","Selecting previously unselected package libcupsfilters1:amd64.\n","Preparing to unpack .../08-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n","Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n","Selecting previously unselected package libcupsimage2:amd64.\n","Preparing to unpack .../09-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n","Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n","Selecting previously unselected package libijs-0.35:amd64.\n","Preparing to unpack .../10-libijs-0.35_0.35-13_amd64.deb ...\n","Unpacking libijs-0.35:amd64 (0.35-13) ...\n","Selecting previously unselected package libjbig2dec0:amd64.\n","Preparing to unpack .../11-libjbig2dec0_0.13-6_amd64.deb ...\n","Unpacking libjbig2dec0:amd64 (0.13-6) ...\n","Selecting previously unselected package libgs9-common.\n","Preparing to unpack .../12-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.14_all.deb ...\n","Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n","Selecting previously unselected package libgs9:amd64.\n","Preparing to unpack .../13-libgs9_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n","Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n","Selecting previously unselected package libjs-jquery.\n","Preparing to unpack .../14-libjs-jquery_3.2.1-1_all.deb ...\n","Unpacking libjs-jquery (3.2.1-1) ...\n","Selecting previously unselected package libkpathsea6:amd64.\n","Preparing to unpack .../15-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package libpotrace0.\n","Preparing to unpack .../16-libpotrace0_1.14-2_amd64.deb ...\n","Unpacking libpotrace0 (1.14-2) ...\n","Selecting previously unselected package libptexenc1:amd64.\n","Preparing to unpack .../17-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package rubygems-integration.\n","Preparing to unpack .../18-rubygems-integration_1.11_all.deb ...\n","Unpacking rubygems-integration (1.11) ...\n","Selecting previously unselected package ruby2.5.\n","Preparing to unpack .../19-ruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n","Unpacking ruby2.5 (2.5.1-1ubuntu1.7) ...\n","Selecting previously unselected package ruby.\n","Preparing to unpack .../20-ruby_1%3a2.5.1_amd64.deb ...\n","Unpacking ruby (1:2.5.1) ...\n","Selecting previously unselected package rake.\n","Preparing to unpack .../21-rake_12.3.1-1ubuntu0.1_all.deb ...\n","Unpacking rake (12.3.1-1ubuntu0.1) ...\n","Selecting previously unselected package ruby-did-you-mean.\n","Preparing to unpack .../22-ruby-did-you-mean_1.2.0-2_all.deb ...\n","Unpacking ruby-did-you-mean (1.2.0-2) ...\n","Selecting previously unselected package ruby-minitest.\n","Preparing to unpack .../23-ruby-minitest_5.10.3-1_all.deb ...\n","Unpacking ruby-minitest (5.10.3-1) ...\n","Selecting previously unselected package ruby-net-telnet.\n","Preparing to unpack .../24-ruby-net-telnet_0.1.1-2_all.deb ...\n","Unpacking ruby-net-telnet (0.1.1-2) ...\n","Selecting previously unselected package ruby-power-assert.\n","Preparing to unpack .../25-ruby-power-assert_0.3.0-1_all.deb ...\n","Unpacking ruby-power-assert (0.3.0-1) ...\n","Selecting previously unselected package ruby-test-unit.\n","Preparing to unpack .../26-ruby-test-unit_3.2.5-1_all.deb ...\n","Unpacking ruby-test-unit (3.2.5-1) ...\n","Selecting previously unselected package libruby2.5:amd64.\n","Preparing to unpack .../27-libruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n","Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n","Selecting previously unselected package libsynctex1:amd64.\n","Preparing to unpack .../28-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package libtexlua52:amd64.\n","Preparing to unpack .../29-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package libtexluajit2:amd64.\n","Preparing to unpack .../30-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package libzzip-0-13:amd64.\n","Preparing to unpack .../31-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n","Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n","Selecting previously unselected package lmodern.\n","Preparing to unpack .../32-lmodern_2.004.5-3_all.deb ...\n","Unpacking lmodern (2.004.5-3) ...\n","Selecting previously unselected package preview-latex-style.\n","Preparing to unpack .../33-preview-latex-style_11.91-1ubuntu1_all.deb ...\n","Unpacking preview-latex-style (11.91-1ubuntu1) ...\n","Selecting previously unselected package t1utils.\n","Preparing to unpack .../34-t1utils_1.41-2_amd64.deb ...\n","Unpacking t1utils (1.41-2) ...\n","Selecting previously unselected package tex-gyre.\n","Preparing to unpack .../35-tex-gyre_20160520-1_all.deb ...\n","Unpacking tex-gyre (20160520-1) ...\n","Selecting previously unselected package texlive-binaries.\n","Preparing to unpack .../36-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n","Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n","Selecting previously unselected package texlive-base.\n","Preparing to unpack .../37-texlive-base_2017.20180305-1_all.deb ...\n","Unpacking texlive-base (2017.20180305-1) ...\n","Selecting previously unselected package texlive-fonts-recommended.\n","Preparing to unpack .../38-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n","Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n","Selecting previously unselected package texlive-latex-base.\n","Preparing to unpack .../39-texlive-latex-base_2017.20180305-1_all.deb ...\n","Unpacking texlive-latex-base (2017.20180305-1) ...\n","Selecting previously unselected package texlive-latex-recommended.\n","Preparing to unpack .../40-texlive-latex-recommended_2017.20180305-1_all.deb ...\n","Unpacking texlive-latex-recommended (2017.20180305-1) ...\n","Selecting previously unselected package texlive.\n","Preparing to unpack .../41-texlive_2017.20180305-1_all.deb ...\n","Unpacking texlive (2017.20180305-1) ...\n","Selecting previously unselected package texlive-pictures.\n","Preparing to unpack .../42-texlive-pictures_2017.20180305-1_all.deb ...\n","Unpacking texlive-pictures (2017.20180305-1) ...\n","Selecting previously unselected package texlive-latex-extra.\n","Preparing to unpack .../43-texlive-latex-extra_2017.20180305-2_all.deb ...\n","Unpacking texlive-latex-extra (2017.20180305-2) ...\n","Selecting previously unselected package texlive-plain-generic.\n","Preparing to unpack .../44-texlive-plain-generic_2017.20180305-2_all.deb ...\n","Unpacking texlive-plain-generic (2017.20180305-2) ...\n","Selecting previously unselected package tipa.\n","Preparing to unpack .../45-tipa_2%3a1.3-20_all.deb ...\n","Unpacking tipa (2:1.3-20) ...\n","Selecting previously unselected package texlive-xetex.\n","Preparing to unpack .../46-texlive-xetex_2017.20180305-1_all.deb ...\n","Unpacking texlive-xetex (2017.20180305-1) ...\n","Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n","Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up libjs-jquery (3.2.1-1) ...\n","Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n","Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up tex-common (6.09) ...\n","update-language: texlive-base not installed and configured, doing nothing!\n","Setting up poppler-data (0.4.8-2) ...\n","Setting up tex-gyre (20160520-1) ...\n","Setting up preview-latex-style (11.91-1ubuntu1) ...\n","Setting up fonts-texgyre (20160520-1) ...\n","Setting up fonts-noto-mono (20171026-2) ...\n","Setting up fonts-lato (2.0-2) ...\n","Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n","Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n","Setting up libjbig2dec0:amd64 (0.13-6) ...\n","Setting up ruby-did-you-mean (1.2.0-2) ...\n","Setting up t1utils (1.41-2) ...\n","Setting up ruby-net-telnet (0.1.1-2) ...\n","Setting up libijs-0.35:amd64 (0.35-13) ...\n","Setting up rubygems-integration (1.11) ...\n","Setting up libpotrace0 (1.14-2) ...\n","Setting up javascript-common (11) ...\n","Setting up ruby-minitest (5.10.3-1) ...\n","Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n","Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n","Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n","Setting up fonts-lmodern (2.004.5-3) ...\n","Setting up ruby-power-assert (0.3.0-1) ...\n","Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n","update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n","update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n","Setting up texlive-base (2017.20180305-1) ...\n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n","mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n","mktexlsr: Updating /var/lib/texmf/ls-R... \n","mktexlsr: Done.\n","tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n","tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n","tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n","tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n","Setting up texlive-fonts-recommended (2017.20180305-1) ...\n","Setting up texlive-plain-generic (2017.20180305-2) ...\n","Setting up texlive-latex-base (2017.20180305-1) ...\n","Setting up lmodern (2.004.5-3) ...\n","Setting up texlive-latex-recommended (2017.20180305-1) ...\n","Setting up texlive-pictures (2017.20180305-1) ...\n","Setting up tipa (2:1.3-20) ...\n","Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n","Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n","update-fmtutil has updated the following file(s):\n","\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n","\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n","If you want to activate the changes in the above file(s),\n","you should run fmtutil-sys or fmtutil.\n","Setting up texlive (2017.20180305-1) ...\n","Setting up texlive-latex-extra (2017.20180305-2) ...\n","Setting up texlive-xetex (2017.20180305-1) ...\n","Setting up ruby2.5 (2.5.1-1ubuntu1.7) ...\n","Setting up ruby (1:2.5.1) ...\n","Setting up ruby-test-unit (3.2.5-1) ...\n","Setting up rake (12.3.1-1ubuntu0.1) ...\n","Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n","/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n","\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n","Processing triggers for tex-common (6.09) ...\n","Running updmap-sys. This may take some time... done.\n","Running mktexlsr /var/lib/texmf ... done.\n","Building format(s) --all.\n","\tThis may take some time... done.\n","  Building wheel for pypandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-07edb7464e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jupyter nbconvert --to PDF '/content/drive/My Drive/Colab Notebooks/1_naive_bayes.ipynb의 사본'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}