{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Graph-10]Using_GDL_Library-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFLiGyKDKUmF"
      },
      "source": [
        "# 실습 10. \r\n",
        "\r\n",
        "**from dgl.nn import SAGEConv** 를 직접 구현하고, 이를 이용하여 GraphSAGE 모델을 학습시켜보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp2Pmzp9MThS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34150644-050b-42c0-844e-7ff380f3ebc2"
      },
      "source": [
        "!pip install dgl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/b6/5450e9bb80842ab58a6ee8c0da8c7d738465703bceb576bd7e9782c65391/dgl-0.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl) (4.4.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2020.12.5)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2wYGnS9MUOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c85e1a-1a08-47ae-8c90-b85860c9733c"
      },
      "source": [
        "import numpy as np                        \r\n",
        "import time\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import dgl\r\n",
        "from dgl.data import CoraGraphDataset\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "import dgl.function as fn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4EWxJtfMM6a"
      },
      "source": [
        "# 하이퍼파라미터 정의\r\n",
        "learningRate = 1e-2\r\n",
        "numEpochs = 50\r\n",
        "numHiddenDim = 128\r\n",
        "numLayers = 2\r\n",
        "weightDecay = 5e-4"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfKIrNuqRpVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b0ad84-739e-484e-d727-7314b3fa28f8"
      },
      "source": [
        "'''\r\n",
        "    Cora 데이터셋은 2708개의 논문(노드), 10556개의 인용관계(엣지)로 이루어졌습니다. \r\n",
        "    NumFeat은 각 노드를 나타내는 특성을 말합니다. \r\n",
        "    Cora 데이터셋은 각 노드가 1433개의 특성을 가지고, 개개의 특성은 '1'혹은 '0'으로 나타내어지며 특정 단어의 논문 등장 여부를 나타냅니다.\r\n",
        "    즉, 2708개의 논문에서 특정 단어 1433개를 뽑아서, 1433개의 단어의 등장 여부를 통해 각 노드를 표현합니다.\r\n",
        "    \r\n",
        "    노드의 라벨은 총 7개가 존재하고, 각 라벨은 논문의 주제를 나타냅니다\r\n",
        "    [Case_Based, Genetic_Algorithms, Neural_Networks, Probabilistic_Methods, Reinforcement_Learning, Rule_Learning, Theory]\r\n",
        "\r\n",
        "    2708개의 노드 중, 학습에는 140개의 노드를 사용하고 모델을 테스트하는 데에는 1000개를 사용합니다.\r\n",
        "    본 실습에서는 Validation을 진행하지않습니다.\r\n",
        "\r\n",
        "    요약하자면, 앞서 학습시킬 모델은 Cora 데이터셋의 \r\n",
        "    [논문 내 등장 단어들, 논문들 사이의 인용관계]를 활용하여 논문의 주제를 예측하는 모델입니다.\r\n",
        "'''\r\n",
        "\r\n",
        "# Cora Graph Dataset 불러오기\r\n",
        "G = CoraGraphDataset()\r\n",
        "numClasses = G.num_classes\r\n",
        "\r\n",
        "G = G[0]\r\n",
        "# 노드들의 feauture & feature의 차원\r\n",
        "features = G.ndata['feat']\r\n",
        "inputFeatureDim = features.shape[1]\r\n",
        "\r\n",
        "# 각 노드들의 실제 라벨\r\n",
        "labels = G.ndata['label']\r\n",
        "\r\n",
        "# 학습/테스트에 사용할 노드들에 대한 표시\r\n",
        "trainMask = G.ndata['train_mask']        \r\n",
        "testMask = G.ndata['test_mask']"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "Extracting file to /root/.dgl/cora_v2\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNlgAxzmMx4q"
      },
      "source": [
        "# 모델 학습 결과를 평가할 함수\r\n",
        "def evaluateTrain(model, features, labels, mask):\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        logits = model(features)\r\n",
        "        logits = logits[mask]\r\n",
        "        labels = labels[mask]\r\n",
        "        _, indices = torch.max(logits, dim=1)\r\n",
        "        correct = torch.sum(indices == labels)\r\n",
        "        return correct.item() * 1.0 / len(labels)\r\n",
        "\r\n",
        "def evaluateTest(model, features, labels, mask):\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        logits = model(features)\r\n",
        "        logits = logits[mask]\r\n",
        "        labels = labels[mask]\r\n",
        "        _, indices = torch.max(logits, dim=1)\r\n",
        "        macro_f1 = f1_score(labels, indices, average = 'macro')\r\n",
        "        correct = torch.sum(indices == labels)\r\n",
        "        return correct.item() * 1.0 / len(labels), macro_f1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px-uArTVMztb"
      },
      "source": [
        "def train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs):\r\n",
        "    executionTime=[]\r\n",
        "    \r\n",
        "    for epoch in range(numEpochs):\r\n",
        "        model.train()\r\n",
        "\r\n",
        "        startTime = time.time()\r\n",
        "            \r\n",
        "        logits = model(features)                                    # 포워딩\r\n",
        "        loss = lossFunction(logits[trainMask], labels[trainMask])   # 모델의 예측값과 실제 라벨을 비교하여 loss 값 계산\r\n",
        "\r\n",
        "        optimizer.zero_grad()                                       \r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        executionTime.append(time.time() - startTime)\r\n",
        "\r\n",
        "        acc = evaluateTrain(model, features, labels, trainMask)\r\n",
        "\r\n",
        "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f}\".format(epoch, np.mean(executionTime), loss.item(), acc))\r\n",
        "\r\n",
        "def test(model, feautures, labels, testMask):\r\n",
        "    acc, macro_f1 = evaluateTest(model, features, labels, testMask)\r\n",
        "    print(\"Test Accuracy {:.4f}\".format(acc))\r\n",
        "    print(\"Test macro-f1 {:.4f}\".format(macro_f1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvFtl2-jIfiW"
      },
      "source": [
        "class SAGEConv(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    in_feats: 인풋 feature의 사이즈\r\n",
        "    out_feats: 아웃풋 feature의 사이즈\r\n",
        "    activation: None이 아니라면, 노드 피쳐의 업데이트를 위해서 해당 activation function을 적용한다.\r\n",
        "    \"\"\"\r\n",
        "    '''\r\n",
        "        ref:\r\n",
        "        https://arxiv.org/pdf/1706.02216.pdf \r\n",
        "        https://docs.dgl.ai/en/0.4.x/_modules/dgl/nn/pytorch/conv/sageconv.html\r\n",
        "    '''\r\n",
        "    \r\n",
        "    def __init__(self, in_feats, out_feats, activation):\r\n",
        "        super(SAGEConv, self).__init__()\r\n",
        "        self._in_feats = in_feats\r\n",
        "        self._out_feats = out_feats\r\n",
        "        self.activation = activation\r\n",
        "\r\n",
        "        self.W = nn.Linear(in_feats+in_feats, out_feats, bias=True) # 벡터 2개를 연결하므로 입력 임베딩 차원 크기의 2배로 입력\r\n",
        "\r\n",
        "    def forward(self, graph, feature):\r\n",
        "        graph.ndata['h'] = feature                                            \r\n",
        "        graph.update_all(fn.copy_src('h', 'm'), fn.sum('m', 'neigh'))                \r\n",
        "\r\n",
        "        # Aggregate & Noramlization\r\n",
        "        degs = graph.in_degrees().to(feature)\r\n",
        "        hkNeigh = graph.ndata['neigh'] / degs.unsqueeze(-1)\r\n",
        "        \r\n",
        "        hk = self.W(torch.cat((graph.ndata['h'], hkNeigh), dim=-1))\r\n",
        "\r\n",
        "        if self.activation != None:\r\n",
        "            hk = self.activation(hk)\r\n",
        "\r\n",
        "        return hk\r\n",
        "\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7_VGTIZMUQp"
      },
      "source": [
        "class GraphSAGE(nn.Module):\r\n",
        "    '''\r\n",
        "        graph               : 학습할 그래프\r\n",
        "        inFeatDim           : 데이터의 feature의 차원\r\n",
        "        numHiddenDim        : 모델의 hidden 차원\r\n",
        "        numClasses          : 예측할 라벨의 경우의 수\r\n",
        "        numLayers           : 인풋, 아웃풋 레이어를 제외하고 중간 레이어의 갯수\r\n",
        "        activationFunction  : 활성화 함수의 종류\r\n",
        "    '''\r\n",
        "    def __init__(self, graph, inFeatDim, numHiddenDim, numClasses, numLayers, activationFunction):\r\n",
        "        super(GraphSAGE, self).__init__()\r\n",
        "        self.layers = nn.ModuleList()\r\n",
        "        self.graph = graph\r\n",
        "\r\n",
        "        # 인풋 레이어\r\n",
        "        self.layers.append(SAGEConv(inFeatDim, numHiddenDim, activationFunction))\r\n",
        "       \r\n",
        "        # 히든 레이어\r\n",
        "        for i in range(numLayers):\r\n",
        "            self.layers.append(SAGEConv(numHiddenDim, numHiddenDim, activationFunction))\r\n",
        "        # dropout 없음, SAGEConv 정의 해서 사용\r\n",
        "        # 출력 레이어\r\n",
        "        self.layers.append(SAGEConv(numHiddenDim, numClasses, activation=None))\r\n",
        "\r\n",
        "    def forward(self, features):\r\n",
        "        x = features\r\n",
        "        for layer in self.layers:\r\n",
        "            x = layer(self.graph, x)\r\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RSfUBNJN_4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4033d548-c0ec-4ac0-9547-85daf8a7ecbe"
      },
      "source": [
        "# 모델 생성\r\n",
        "model = GraphSAGE(G, inputFeatureDim, numHiddenDim, numClasses, numLayers, F.relu)\r\n",
        "print(model)\r\n",
        "\r\n",
        "lossFunction = torch.nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# 옵티마이저 초기화\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GraphSAGE(\n",
            "  (layers): ModuleList(\n",
            "    (0): SAGEConv(\n",
            "      (W): Linear(in_features=2866, out_features=128, bias=True)\n",
            "    )\n",
            "    (1): SAGEConv(\n",
            "      (W): Linear(in_features=256, out_features=128, bias=True)\n",
            "    )\n",
            "    (2): SAGEConv(\n",
            "      (W): Linear(in_features=256, out_features=128, bias=True)\n",
            "    )\n",
            "    (3): SAGEConv(\n",
            "      (W): Linear(in_features=256, out_features=7, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0i0GwVwMUSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6559a4e2-5d0e-445f-ff5e-c9345bf5feca"
      },
      "source": [
        "train(model, lossFunction, features, labels, trainMask, optimizer, numEpochs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 00000 | Time(s) 0.4281 | Loss 1.9467 | Accuracy 0.2571\n",
            "Epoch 00001 | Time(s) 0.2970 | Loss 1.9457 | Accuracy 0.2786\n",
            "Epoch 00002 | Time(s) 0.2528 | Loss 1.9434 | Accuracy 0.2714\n",
            "Epoch 00003 | Time(s) 0.2291 | Loss 1.9358 | Accuracy 0.4714\n",
            "Epoch 00004 | Time(s) 0.2145 | Loss 1.9061 | Accuracy 0.2214\n",
            "Epoch 00005 | Time(s) 0.2045 | Loss 1.8365 | Accuracy 0.2929\n",
            "Epoch 00006 | Time(s) 0.1979 | Loss 1.7145 | Accuracy 0.3571\n",
            "Epoch 00007 | Time(s) 0.1921 | Loss 1.5620 | Accuracy 0.2929\n",
            "Epoch 00008 | Time(s) 0.1880 | Loss 1.4169 | Accuracy 0.2857\n",
            "Epoch 00009 | Time(s) 0.1846 | Loss 1.2649 | Accuracy 0.4143\n",
            "Epoch 00010 | Time(s) 0.1823 | Loss 1.2059 | Accuracy 0.4071\n",
            "Epoch 00011 | Time(s) 0.1798 | Loss 1.1917 | Accuracy 0.6071\n",
            "Epoch 00012 | Time(s) 0.1777 | Loss 0.9313 | Accuracy 0.6000\n",
            "Epoch 00013 | Time(s) 0.1760 | Loss 0.9067 | Accuracy 0.5429\n",
            "Epoch 00014 | Time(s) 0.1751 | Loss 0.8754 | Accuracy 0.9143\n",
            "Epoch 00015 | Time(s) 0.1739 | Loss 0.5815 | Accuracy 0.7714\n",
            "Epoch 00016 | Time(s) 0.1726 | Loss 0.6251 | Accuracy 0.9571\n",
            "Epoch 00017 | Time(s) 0.1714 | Loss 0.3203 | Accuracy 0.9429\n",
            "Epoch 00018 | Time(s) 0.1705 | Loss 0.2825 | Accuracy 0.9857\n",
            "Epoch 00019 | Time(s) 0.1696 | Loss 0.1180 | Accuracy 0.9500\n",
            "Epoch 00020 | Time(s) 0.1689 | Loss 0.1269 | Accuracy 0.9929\n",
            "Epoch 00021 | Time(s) 0.1683 | Loss 0.0476 | Accuracy 0.9929\n",
            "Epoch 00022 | Time(s) 0.1677 | Loss 0.0334 | Accuracy 0.9929\n",
            "Epoch 00023 | Time(s) 0.1671 | Loss 0.0323 | Accuracy 1.0000\n",
            "Epoch 00024 | Time(s) 0.1666 | Loss 0.0135 | Accuracy 1.0000\n",
            "Epoch 00025 | Time(s) 0.1667 | Loss 0.0048 | Accuracy 1.0000\n",
            "Epoch 00026 | Time(s) 0.1664 | Loss 0.0057 | Accuracy 1.0000\n",
            "Epoch 00027 | Time(s) 0.1659 | Loss 0.0057 | Accuracy 1.0000\n",
            "Epoch 00028 | Time(s) 0.1655 | Loss 0.0030 | Accuracy 1.0000\n",
            "Epoch 00029 | Time(s) 0.1651 | Loss 0.0018 | Accuracy 1.0000\n",
            "Epoch 00030 | Time(s) 0.1647 | Loss 0.0023 | Accuracy 1.0000\n",
            "Epoch 00031 | Time(s) 0.1644 | Loss 0.0016 | Accuracy 1.0000\n",
            "Epoch 00032 | Time(s) 0.1641 | Loss 0.0010 | Accuracy 1.0000\n",
            "Epoch 00033 | Time(s) 0.1638 | Loss 0.0008 | Accuracy 1.0000\n",
            "Epoch 00034 | Time(s) 0.1635 | Loss 0.0007 | Accuracy 1.0000\n",
            "Epoch 00035 | Time(s) 0.1633 | Loss 0.0007 | Accuracy 1.0000\n",
            "Epoch 00036 | Time(s) 0.1630 | Loss 0.0009 | Accuracy 1.0000\n",
            "Epoch 00037 | Time(s) 0.1628 | Loss 0.0011 | Accuracy 1.0000\n",
            "Epoch 00038 | Time(s) 0.1627 | Loss 0.0010 | Accuracy 1.0000\n",
            "Epoch 00039 | Time(s) 0.1626 | Loss 0.0009 | Accuracy 1.0000\n",
            "Epoch 00040 | Time(s) 0.1624 | Loss 0.0009 | Accuracy 1.0000\n",
            "Epoch 00041 | Time(s) 0.1623 | Loss 0.0010 | Accuracy 1.0000\n",
            "Epoch 00042 | Time(s) 0.1622 | Loss 0.0010 | Accuracy 1.0000\n",
            "Epoch 00043 | Time(s) 0.1622 | Loss 0.0010 | Accuracy 1.0000\n",
            "Epoch 00044 | Time(s) 0.1621 | Loss 0.0011 | Accuracy 1.0000\n",
            "Epoch 00045 | Time(s) 0.1620 | Loss 0.0011 | Accuracy 1.0000\n",
            "Epoch 00046 | Time(s) 0.1618 | Loss 0.0013 | Accuracy 1.0000\n",
            "Epoch 00047 | Time(s) 0.1617 | Loss 0.0014 | Accuracy 1.0000\n",
            "Epoch 00048 | Time(s) 0.1616 | Loss 0.0015 | Accuracy 1.0000\n",
            "Epoch 00049 | Time(s) 0.1615 | Loss 0.0016 | Accuracy 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng-_-vQCMUUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1592a9ba-5784-4532-8db5-9e586b756d28"
      },
      "source": [
        "test(model, features, labels, testMask)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy 0.6930\n",
            "Test macro-f1 0.7006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNN4yTAWJKcJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}