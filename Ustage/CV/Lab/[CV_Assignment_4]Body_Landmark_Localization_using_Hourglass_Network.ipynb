{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[CV_Assignment_4]Body_Landmark_Localization_using_Hourglass_Network.ipynb","provenance":[{"file_id":"19WdbrENpGOyq2uCWZzcnUtGtjPDZ6-PM","timestamp":1615529561009}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1UL2tS-EQZKN"},"source":["# Assignment 4 : Body Landmark Localization using Hourglass Network\n"]},{"cell_type":"code","metadata":{"id":"3JNiQ2H2nS-r","executionInfo":{"status":"ok","timestamp":1616488340179,"user_tz":-540,"elapsed":1000,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["# Seed\n","import torch\n","import numpy as np\n","import random\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","np.random.seed(0)\n","random.seed(0)\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F-nCnWRyQh2P"},"source":["### **4.1 Hourglass Module Implementation**\n","\n","아래 **Fig. 3.**에 표현되어 있는 **Hourglass module**을 구현하고자 합니다. 이미 선언되어 있는 layer들을 이용하여 figure 상의 layer 구성과 동일하게 tensor가 forward 될 수 있도록 ```def forward``` 부분을 완성해주세요.\n","\n","\n","- Fig. 4. Right에 표현된 supervision layer는 해당 과제에서는 고려하지 않습니다.\n","- The figures are from [the original hourglass paper](https://arxiv.org/abs/1603.06937) [Newell et al.].\n","\n","<img src='https://drive.google.com/uc?id=19-S7TwZ62joUR8W9031xjn3jMZyTevpw'  width=\"700\">\n","\n","<img src='https://drive.google.com/uc?id=1ols0VZ7TGZCMDM7sKzCJq3bByHsOU9up'  width=\"700\">"]},{"cell_type":"markdown","metadata":{"id":"pRXBDC0qkhkP"},"source":["아래의 코드는 Hourglass 모듈을 나타내는 클래스입니다. 위의 Figure를 참고하여 **TO DO** 과제를 채워주세요 :)\n","\n","### - **TO DO** : ```class Hourglass```는 하나의 Hourglass 모듈을 의미하며 이전에 선언한 ```class ResidualBlock```을 기본 convolution block으로 사용합니다. Hourglass 내부에 사용되는 layer는 이미 ```def __init__```에 선언이 되어 있지만 `**``def forward``` 부분은 완성되지 않아 선언된 layer들을 구성에 맞게 연결**해주어야 합니다. Fig. 3.을 참고하여 Hourglass 모듈을 올바르게 구현해주세요 :)"]},{"cell_type":"code","metadata":{"id":"zehDRYDCkTsK","executionInfo":{"status":"ok","timestamp":1616488340744,"user_tz":-540,"elapsed":1556,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResidualBlock(nn.Module):\n","  def __init__(self, num_channels=256):\n","    super(ResidualBlock, self).__init__()\n","\n","    self.bn1 = nn.BatchNorm2d(num_channels)\n","    self.conv1 = nn.Conv2d(num_channels, num_channels//2, kernel_size=1, bias=True)\n","\n","    self.bn2 = nn.BatchNorm2d(num_channels//2)\n","    self.conv2 = nn.Conv2d(num_channels//2, num_channels//2, kernel_size=3, stride=1,\n","                              padding=1, bias=True)\n","\n","    self.bn3 = nn.BatchNorm2d(num_channels//2)\n","    self.conv3 = nn.Conv2d(num_channels//2, num_channels, kernel_size=1, bias=True)\n","\n","    self.relu = nn.ReLU(inplace=True)\n","\n","  def forward(self, x):\n","    residual = x\n","\n","    out = self.bn1(x)\n","    out = self.relu(out)\n","    out = self.conv1(out)\n","\n","    out = self.bn2(out)\n","    out = self.relu(out)\n","    out = self.conv2(out)\n","\n","    out = self.bn3(out)\n","    out = self.relu(out)\n","    out = self.conv3(out)\n","\n","    out += residual\n","\n","    return out"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiK_5kEYQsda","executionInfo":{"status":"ok","timestamp":1616488340745,"user_tz":-540,"elapsed":1551,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["class Hourglass(nn.Module):\n","  def __init__(self, block, num_channels=256):\n","    super(Hourglass, self).__init__()\n","\n","    self.downconv_1 = block(num_channels)\n","    self.pool_1 = nn.MaxPool2d(kernel_size=2)\n","    self.downconv_2 = block(num_channels)\n","    self.pool_2 = nn.MaxPool2d(kernel_size=2)\n","    self.downconv_3 = block(num_channels)\n","    self.pool_3 = nn.MaxPool2d(kernel_size=2)\n","    self.downconv_4 = block(num_channels)\n","    self.pool_4 = nn.MaxPool2d(kernel_size=2)\n","\n","    self.midconv_1 = block(num_channels)\n","    self.midconv_2 = block(num_channels)\n","    self.midconv_3 = block(num_channels)\n","    \n","    self.skipconv_1 = block(num_channels)\n","    self.skipconv_2 = block(num_channels)\n","    self.skipconv_3 = block(num_channels)\n","    self.skipconv_4 = block(num_channels)\n","\n","    self.upconv_1 = block(num_channels)\n","    self.upconv_2 = block(num_channels)\n","    self.upconv_3 = block(num_channels)\n","    self.upconv_4 = block(num_channels)\n","\n","  def forward(self, x):\n","    x1 = self.downconv_1(x)\n","    x  = self.pool_1(x1)\n","\n","    '''======================================================='''\n","    '''======================== TO DO ========================'''\n","    x2 = self.downconv_2(x)\n","    x  = self.pool_2(x2)\n","    x3 = self.downconv_3(x)\n","    x  = self.pool_3(x3)\n","    x4 = self.downconv_4(x)\n","    x  = self.pool_4(x4)\n","\n","    x = self.midconv_1(x)\n","    x = self.midconv_2(x)\n","    x = self.midconv_3(x)\n","\n","    x4 = self.skipconv_1(x4)\n","    x = F.upsample(x, scale_factor=2)\n","    x = x + x4\n","    x = self.upconv_1(x)\n","\n","    x3 = self.skipconv_1(x3)\n","    x = F.upsample(x, scale_factor=2)\n","    x = x + x3\n","    x = self.upconv_2(x)\n","\n","    x2 = self.skipconv_1(x2)\n","    x = F.upsample(x, scale_factor=2)\n","    x = x + x2\n","    x = self.upconv_3(x)\n","\n","    x1 = self.skipconv_1(x1)\n","    x = F.upsample(x, scale_factor=2)\n","    x = x + x1\n","    x = self.upconv_4(x)\n","    '''======================== TO DO ========================'''\n","    '''======================================================='''\n","\n","    return x"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VZfXyBDfldbE"},"source":["----\n","[torchsummary](https://github.com/sksq96/pytorch-summary)는 PyTorch로 구현한 네트워크를 직관적으로 확인할 수 있는 라이브러리입니다.\n","\n","해당 라이브러리를 이용하여 각 feature map의 dimension과 각각의 layer가 몇개의 parameter 수를 가지고 있는지 확인해 봅시다!"]},{"cell_type":"code","metadata":{"id":"i1-T5hNpYhI1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616488341139,"user_tz":-540,"elapsed":1939,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"2944ca4e-ee2c-45e7-8a93-12a5d7e6e3a7"},"source":["# Let's summary the implemented hourglass architecture using torchsummary library.\n","hg = Hourglass(ResidualBlock)\n","\n","from torchsummary import summary\n","summary(hg, input_size=(256,64,64), device='cpu')"],"execution_count":24,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","       BatchNorm2d-1          [-1, 256, 64, 64]             512\n","              ReLU-2          [-1, 256, 64, 64]               0\n","            Conv2d-3          [-1, 128, 64, 64]          32,896\n","       BatchNorm2d-4          [-1, 128, 64, 64]             256\n","              ReLU-5          [-1, 128, 64, 64]               0\n","            Conv2d-6          [-1, 128, 64, 64]         147,584\n","       BatchNorm2d-7          [-1, 128, 64, 64]             256\n","              ReLU-8          [-1, 128, 64, 64]               0\n","            Conv2d-9          [-1, 256, 64, 64]          33,024\n","    ResidualBlock-10          [-1, 256, 64, 64]               0\n","        MaxPool2d-11          [-1, 256, 32, 32]               0\n","      BatchNorm2d-12          [-1, 256, 32, 32]             512\n","             ReLU-13          [-1, 256, 32, 32]               0\n","           Conv2d-14          [-1, 128, 32, 32]          32,896\n","      BatchNorm2d-15          [-1, 128, 32, 32]             256\n","             ReLU-16          [-1, 128, 32, 32]               0\n","           Conv2d-17          [-1, 128, 32, 32]         147,584\n","      BatchNorm2d-18          [-1, 128, 32, 32]             256\n","             ReLU-19          [-1, 128, 32, 32]               0\n","           Conv2d-20          [-1, 256, 32, 32]          33,024\n","    ResidualBlock-21          [-1, 256, 32, 32]               0\n","        MaxPool2d-22          [-1, 256, 16, 16]               0\n","      BatchNorm2d-23          [-1, 256, 16, 16]             512\n","             ReLU-24          [-1, 256, 16, 16]               0\n","           Conv2d-25          [-1, 128, 16, 16]          32,896\n","      BatchNorm2d-26          [-1, 128, 16, 16]             256\n","             ReLU-27          [-1, 128, 16, 16]               0\n","           Conv2d-28          [-1, 128, 16, 16]         147,584\n","      BatchNorm2d-29          [-1, 128, 16, 16]             256\n","             ReLU-30          [-1, 128, 16, 16]               0\n","           Conv2d-31          [-1, 256, 16, 16]          33,024\n","    ResidualBlock-32          [-1, 256, 16, 16]               0\n","        MaxPool2d-33            [-1, 256, 8, 8]               0\n","      BatchNorm2d-34            [-1, 256, 8, 8]             512\n","             ReLU-35            [-1, 256, 8, 8]               0\n","           Conv2d-36            [-1, 128, 8, 8]          32,896\n","      BatchNorm2d-37            [-1, 128, 8, 8]             256\n","             ReLU-38            [-1, 128, 8, 8]               0\n","           Conv2d-39            [-1, 128, 8, 8]         147,584\n","      BatchNorm2d-40            [-1, 128, 8, 8]             256\n","             ReLU-41            [-1, 128, 8, 8]               0\n","           Conv2d-42            [-1, 256, 8, 8]          33,024\n","    ResidualBlock-43            [-1, 256, 8, 8]               0\n","        MaxPool2d-44            [-1, 256, 4, 4]               0\n","      BatchNorm2d-45            [-1, 256, 4, 4]             512\n","             ReLU-46            [-1, 256, 4, 4]               0\n","           Conv2d-47            [-1, 128, 4, 4]          32,896\n","      BatchNorm2d-48            [-1, 128, 4, 4]             256\n","             ReLU-49            [-1, 128, 4, 4]               0\n","           Conv2d-50            [-1, 128, 4, 4]         147,584\n","      BatchNorm2d-51            [-1, 128, 4, 4]             256\n","             ReLU-52            [-1, 128, 4, 4]               0\n","           Conv2d-53            [-1, 256, 4, 4]          33,024\n","    ResidualBlock-54            [-1, 256, 4, 4]               0\n","      BatchNorm2d-55            [-1, 256, 4, 4]             512\n","             ReLU-56            [-1, 256, 4, 4]               0\n","           Conv2d-57            [-1, 128, 4, 4]          32,896\n","      BatchNorm2d-58            [-1, 128, 4, 4]             256\n","             ReLU-59            [-1, 128, 4, 4]               0\n","           Conv2d-60            [-1, 128, 4, 4]         147,584\n","      BatchNorm2d-61            [-1, 128, 4, 4]             256\n","             ReLU-62            [-1, 128, 4, 4]               0\n","           Conv2d-63            [-1, 256, 4, 4]          33,024\n","    ResidualBlock-64            [-1, 256, 4, 4]               0\n","      BatchNorm2d-65            [-1, 256, 4, 4]             512\n","             ReLU-66            [-1, 256, 4, 4]               0\n","           Conv2d-67            [-1, 128, 4, 4]          32,896\n","      BatchNorm2d-68            [-1, 128, 4, 4]             256\n","             ReLU-69            [-1, 128, 4, 4]               0\n","           Conv2d-70            [-1, 128, 4, 4]         147,584\n","      BatchNorm2d-71            [-1, 128, 4, 4]             256\n","             ReLU-72            [-1, 128, 4, 4]               0\n","           Conv2d-73            [-1, 256, 4, 4]          33,024\n","    ResidualBlock-74            [-1, 256, 4, 4]               0\n","      BatchNorm2d-75            [-1, 256, 8, 8]             512\n","             ReLU-76            [-1, 256, 8, 8]               0\n","           Conv2d-77            [-1, 128, 8, 8]          32,896\n","      BatchNorm2d-78            [-1, 128, 8, 8]             256\n","             ReLU-79            [-1, 128, 8, 8]               0\n","           Conv2d-80            [-1, 128, 8, 8]         147,584\n","      BatchNorm2d-81            [-1, 128, 8, 8]             256\n","             ReLU-82            [-1, 128, 8, 8]               0\n","           Conv2d-83            [-1, 256, 8, 8]          33,024\n","    ResidualBlock-84            [-1, 256, 8, 8]               0\n","      BatchNorm2d-85            [-1, 256, 8, 8]             512\n","             ReLU-86            [-1, 256, 8, 8]               0\n","           Conv2d-87            [-1, 128, 8, 8]          32,896\n","      BatchNorm2d-88            [-1, 128, 8, 8]             256\n","             ReLU-89            [-1, 128, 8, 8]               0\n","           Conv2d-90            [-1, 128, 8, 8]         147,584\n","      BatchNorm2d-91            [-1, 128, 8, 8]             256\n","             ReLU-92            [-1, 128, 8, 8]               0\n","           Conv2d-93            [-1, 256, 8, 8]          33,024\n","    ResidualBlock-94            [-1, 256, 8, 8]               0\n","      BatchNorm2d-95          [-1, 256, 16, 16]             512\n","             ReLU-96          [-1, 256, 16, 16]               0\n","           Conv2d-97          [-1, 128, 16, 16]          32,896\n","      BatchNorm2d-98          [-1, 128, 16, 16]             256\n","             ReLU-99          [-1, 128, 16, 16]               0\n","          Conv2d-100          [-1, 128, 16, 16]         147,584\n","     BatchNorm2d-101          [-1, 128, 16, 16]             256\n","            ReLU-102          [-1, 128, 16, 16]               0\n","          Conv2d-103          [-1, 256, 16, 16]          33,024\n","   ResidualBlock-104          [-1, 256, 16, 16]               0\n","     BatchNorm2d-105          [-1, 256, 16, 16]             512\n","            ReLU-106          [-1, 256, 16, 16]               0\n","          Conv2d-107          [-1, 128, 16, 16]          32,896\n","     BatchNorm2d-108          [-1, 128, 16, 16]             256\n","            ReLU-109          [-1, 128, 16, 16]               0\n","          Conv2d-110          [-1, 128, 16, 16]         147,584\n","     BatchNorm2d-111          [-1, 128, 16, 16]             256\n","            ReLU-112          [-1, 128, 16, 16]               0\n","          Conv2d-113          [-1, 256, 16, 16]          33,024\n","   ResidualBlock-114          [-1, 256, 16, 16]               0\n","     BatchNorm2d-115          [-1, 256, 32, 32]             512\n","            ReLU-116          [-1, 256, 32, 32]               0\n","          Conv2d-117          [-1, 128, 32, 32]          32,896\n","     BatchNorm2d-118          [-1, 128, 32, 32]             256\n","            ReLU-119          [-1, 128, 32, 32]               0\n","          Conv2d-120          [-1, 128, 32, 32]         147,584\n","     BatchNorm2d-121          [-1, 128, 32, 32]             256\n","            ReLU-122          [-1, 128, 32, 32]               0\n","          Conv2d-123          [-1, 256, 32, 32]          33,024\n","   ResidualBlock-124          [-1, 256, 32, 32]               0\n","     BatchNorm2d-125          [-1, 256, 32, 32]             512\n","            ReLU-126          [-1, 256, 32, 32]               0\n","          Conv2d-127          [-1, 128, 32, 32]          32,896\n","     BatchNorm2d-128          [-1, 128, 32, 32]             256\n","            ReLU-129          [-1, 128, 32, 32]               0\n","          Conv2d-130          [-1, 128, 32, 32]         147,584\n","     BatchNorm2d-131          [-1, 128, 32, 32]             256\n","            ReLU-132          [-1, 128, 32, 32]               0\n","          Conv2d-133          [-1, 256, 32, 32]          33,024\n","   ResidualBlock-134          [-1, 256, 32, 32]               0\n","     BatchNorm2d-135          [-1, 256, 64, 64]             512\n","            ReLU-136          [-1, 256, 64, 64]               0\n","          Conv2d-137          [-1, 128, 64, 64]          32,896\n","     BatchNorm2d-138          [-1, 128, 64, 64]             256\n","            ReLU-139          [-1, 128, 64, 64]               0\n","          Conv2d-140          [-1, 128, 64, 64]         147,584\n","     BatchNorm2d-141          [-1, 128, 64, 64]             256\n","            ReLU-142          [-1, 128, 64, 64]               0\n","          Conv2d-143          [-1, 256, 64, 64]          33,024\n","   ResidualBlock-144          [-1, 256, 64, 64]               0\n","     BatchNorm2d-145          [-1, 256, 64, 64]             512\n","            ReLU-146          [-1, 256, 64, 64]               0\n","          Conv2d-147          [-1, 128, 64, 64]          32,896\n","     BatchNorm2d-148          [-1, 128, 64, 64]             256\n","            ReLU-149          [-1, 128, 64, 64]               0\n","          Conv2d-150          [-1, 128, 64, 64]         147,584\n","     BatchNorm2d-151          [-1, 128, 64, 64]             256\n","            ReLU-152          [-1, 128, 64, 64]               0\n","          Conv2d-153          [-1, 256, 64, 64]          33,024\n","   ResidualBlock-154          [-1, 256, 64, 64]               0\n","================================================================\n","Total params: 3,217,920\n","Trainable params: 3,217,920\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 4.00\n","Forward/backward pass size (MB): 226.44\n","Params size (MB): 12.28\n","Estimated Total Size (MB): 242.71\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oWXn9Pncl6Qt"},"source":["----\n","아래는 네트워크 구현이 정확하게 되었는지를 확인하기 위한 코드입니다.\n","\n","<br>Assignment1와 마찬가지로 아래의 코드는 **Hourglass 모듈의 중간 feature map들의 shape**을 바탕으로 일련의 연산을 수행하여 **하나의 값**을 계산합니다.\n","<br>채점을 위하여 아래의 코드 결과로 얻은 값을 **edwith에 제출**해주세요 :)\n","\n","(주의 : 정확한 채점을 위하여 아래 코드는 수정하지 마세요!)\n","\n","<br>예를 들어, 아래와 같은 실행 결과를 얻으셨다면 edwith 퀴즈에 7777을 선택해주세요.\n","```python\n","\"Your answer is : 7777\"\n","```"]},{"cell_type":"code","metadata":{"id":"YURxVV1jmDkM","executionInfo":{"status":"ok","timestamp":1616488341140,"user_tz":-540,"elapsed":1931,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["import base64, copy\n","\n","class Calculator:\n","  '''\n","  NOTE : DO NOT MODIFY THE CODE BELOW.\n","  '''\n","  def __init__(self, model):\n","    self.answer = 0\n","    modules = [b'bWlkY29udl8x\\n', b'dXBjb252XzM=\\n']\n","    layer = b'Y29udjI=\\n'\n","    for m in modules:\n","      self.hook = model._modules[base64.decodebytes(m).decode()]._modules[base64.decodebytes(layer).decode()].register_forward_hook(self.hook_fn)\n","    \n","  def hook_fn(self, module, input, output):\n","    self.answer += self._get_answer(output)\n","  \n","  def _get_answer(self, l):\n","    _, A, B, C = l.shape\n","    return A*(B-C//3)\n","    \n","  def unregister_forward_hook(self):\n","    self.hook.remove()\n","  \n","\n","def calc_anwser(model):\n","  # NOTE : DO NOT MODIFY THE CODE BELOW.\n","  model_test = copy.deepcopy(model)\n","  ans_calculator = Calculator(model_test)\n","\n","  x = torch.rand(1,256,64,64)\n","  model_test(x)\n","\n","  print(\"Your answer is : %d\" % ans_calculator.answer)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"M_Q6L3X6mEiP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616488341560,"user_tz":-540,"elapsed":2344,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"6eb92bd2-ed2d-4297-c457-b2277053fe93"},"source":["calc_anwser(hg)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Your answer is : 3200\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tOETu9-yd_CV"},"source":["----\n","### **4.2 Human Pose Estimation**\n","\n","[Stacked Hourglass Network](https://arxiv.org/abs/1603.06937)를 이용하여 human pose estimation task를 수행하여 봅시다!\n","\n","<img src='https://drive.google.com/uc?id=1gJPaBX8uVWY9FnNRf2H3rmP1xYsd73eR'  width=\"900\">"]},{"cell_type":"markdown","metadata":{"id":"Gp1OcSEZwhcn"},"source":["##### **>>> 4.2.1 Stacked Hourglass Network**\n","아래 코드는 stacked hourglass network의 전체 코드입니다. ([원본 github 링크](https://github.com/bearpaw/pytorch-pose))\n","\n","- 3.1에서 Hourglass 모듈을 구현할 때 일일이 layer를 쌓는 것 대신에 for loop와 [nn.ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html)를 이용하여 더욱 직관적이고 명료한 코드 작성이 가능하다는 것도 한번 확인해보세요 :)"]},{"cell_type":"code","metadata":{"id":"ooCFffn9xavs","executionInfo":{"status":"ok","timestamp":1616488341561,"user_tz":-540,"elapsed":2337,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["'''\n","Hourglass network inserted in the pre-activated Resnet\n","Use lr=0.01 for current version\n","(c) YANG, Wei\n","'''\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# from .preresnet import BasicBlock, Bottleneck\n","\n","\n","__all__ = ['HourglassNet', 'hg']\n","\n","class Bottleneck(nn.Module):\n","    expansion = 2\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","\n","        self.bn1 = nn.BatchNorm2d(inplanes)\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n","                               padding=1, bias=True)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=True)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.bn1(x)\n","        out = self.relu(out)\n","        out = self.conv1(out)\n","\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","\n","        out = self.bn3(out)\n","        out = self.relu(out)\n","        out = self.conv3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","\n","        return out\n","\n","\n","class Hourglass(nn.Module):\n","    def __init__(self, block, num_blocks, planes, depth):\n","        super(Hourglass, self).__init__()\n","        self.depth = depth\n","        self.block = block\n","        self.hg = self._make_hour_glass(block, num_blocks, planes, depth)\n","\n","    def _make_residual(self, block, num_blocks, planes):\n","        layers = []\n","        for i in range(0, num_blocks):\n","            layers.append(block(planes*block.expansion, planes))\n","        return nn.Sequential(*layers)\n","\n","    def _make_hour_glass(self, block, num_blocks, planes, depth):\n","        hg = []\n","        for i in range(depth):\n","            res = []\n","            for j in range(3):\n","                res.append(self._make_residual(block, num_blocks, planes))\n","            if i == 0:\n","                res.append(self._make_residual(block, num_blocks, planes))\n","            hg.append(nn.ModuleList(res))\n","        return nn.ModuleList(hg)\n","\n","    def _hour_glass_forward(self, n, x):\n","        up1 = self.hg[n-1][0](x)\n","        low1 = F.max_pool2d(x, 2, stride=2)\n","        low1 = self.hg[n-1][1](low1)\n","\n","        if n > 1:\n","            low2 = self._hour_glass_forward(n-1, low1)\n","        else:\n","            low2 = self.hg[n-1][3](low1)\n","        low3 = self.hg[n-1][2](low2)\n","        up2 = F.interpolate(low3, scale_factor=2)\n","        out = up1 + up2\n","        return out\n","\n","    def forward(self, x):\n","        return self._hour_glass_forward(self.depth, x)\n","\n","\n","class HourglassNet(nn.Module):\n","    '''Hourglass model from Newell et al ECCV 2016'''\n","    def __init__(self, block, num_stacks=2, num_blocks=4, num_classes=16):\n","        super(HourglassNet, self).__init__()\n","\n","        self.inplanes = 64\n","        self.num_feats = 128\n","        self.num_stacks = num_stacks\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n","                               bias=True)\n","        self.bn1 = nn.BatchNorm2d(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_residual(block, self.inplanes, 1)\n","        self.layer2 = self._make_residual(block, self.inplanes, 1)\n","        self.layer3 = self._make_residual(block, self.num_feats, 1)\n","        self.maxpool = nn.MaxPool2d(2, stride=2)\n","\n","        # build hourglass modules\n","        ch = self.num_feats*block.expansion\n","        hg, res, fc, score, fc_, score_ = [], [], [], [], [], []\n","        for i in range(num_stacks):\n","            hg.append(Hourglass(block, num_blocks, self.num_feats, 4))\n","            res.append(self._make_residual(block, self.num_feats, num_blocks))\n","            fc.append(self._make_fc(ch, ch))\n","            score.append(nn.Conv2d(ch, num_classes, kernel_size=1, bias=True))\n","            if i < num_stacks-1:\n","                fc_.append(nn.Conv2d(ch, ch, kernel_size=1, bias=True))\n","                score_.append(nn.Conv2d(num_classes, ch, kernel_size=1, bias=True))\n","        self.hg = nn.ModuleList(hg)\n","        self.res = nn.ModuleList(res)\n","        self.fc = nn.ModuleList(fc)\n","        self.score = nn.ModuleList(score)\n","        self.fc_ = nn.ModuleList(fc_)\n","        self.score_ = nn.ModuleList(score_)\n","\n","    def _make_residual(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=True),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_fc(self, inplanes, outplanes):\n","        bn = nn.BatchNorm2d(inplanes)\n","        conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=True)\n","        return nn.Sequential(\n","                conv,\n","                bn,\n","                self.relu,\n","            )\n","\n","    def forward(self, x):\n","        out = []\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.layer1(x)\n","        x = self.maxpool(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","\n","        for i in range(self.num_stacks):\n","            y = self.hg[i](x)\n","            y = self.res[i](y)\n","            y = self.fc[i](y)\n","            score = self.score[i](y)\n","            out.append(score)\n","            if i < self.num_stacks-1:\n","                fc_ = self.fc_[i](y)\n","                score_ = self.score_[i](score)\n","                x = x + fc_ + score_\n","\n","        return out"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZw8Uolpbzn6","executionInfo":{"status":"ok","timestamp":1616488341561,"user_tz":-540,"elapsed":2332,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["model = HourglassNet(Bottleneck, num_stacks=1, num_blocks=2, num_classes=22).cuda()"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B6aQQIGZxljN"},"source":["----\n","##### **>>> 4.2.2 Custom Body Landmark Dataset**\n","해당 과제에서는 이미지 속 인물의 여러 신체 부위를 keypoint 형태로 예측하는 네트워크를 학습시키고자 합니다. (학습 시간 단축을 위해 일부 데이터만 사용)"]},{"cell_type":"markdown","metadata":{"id":"22ehOuP0seQg"},"source":["과제를 수행하기 앞서 별도로 전달해드린 **데이터셋 활용 가이드**를 따라 **```APY191016001_Body_Landmarks_Dataset_Shared.tar.gz```** 압축 파일을 구글 드라이브에 **바로가기 추가**해주세요 :)\n","\n","\n","<br></br>**주의!** 해당 과정은 **데이터 저작권 보호**를 위해 로컬로 직접 데이터를 <U>**다운로드 받는 것을 금지**</U>하기 때문입니다. 또한 교육이 종료된 이후에 해당 데이터셋을 **구글 드라이브에서 파기**할 것을 원칙으로 합니다.\n","<img src='https://drive.google.com/uc?id=14_MWEN5u9mAvO1le_wYVPWdeduUnwW1B'  width=\"700\">"]},{"cell_type":"code","metadata":{"id":"HFBV7FShE4q-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616488342355,"user_tz":-540,"elapsed":3121,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"e9c2f8f4-e9d5-4b0b-c794-fb386ad34480"},"source":["# Mount the google drive to access the dataset.\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VGgSgaaBQ80i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616488342356,"user_tz":-540,"elapsed":3115,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"093dcdd9-08c2-4bb1-bcd2-705c911eaeaf"},"source":["# 저장하신 압축 파일의 경로에 맞게 아래의 압축 해제 명령어를 수정해주세요. (!tar -zxvf 압축파일_경로 -C 저장할_폴더)\n","\n","!unzip /content/gdrive/MyDrive/CV/datasets/APY191016001_Body_Landmarks_Dataset_Shared_Subset_20p.zip -d /content/BodyLandmarkData"],"execution_count":30,"outputs":[{"output_type":"stream","text":["unzip:  cannot find or open /content/gdrive/MyDrive/CV/datasets/APY191016001_Body_Landmarks_Dataset_Shared_Subset_20p.zip, /content/gdrive/MyDrive/CV/datasets/APY191016001_Body_Landmarks_Dataset_Shared_Subset_20p.zip.zip or /content/gdrive/MyDrive/CV/datasets/APY191016001_Body_Landmarks_Dataset_Shared_Subset_20p.zip.ZIP.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ChDPHjqgFE_3","executionInfo":{"status":"ok","timestamp":1616488342356,"user_tz":-540,"elapsed":3109,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["# Hyper-paramter Settings\n","data_root = '/content/BodyLandmarkData/data'\n","log_dir   = '/content/BodyLandmarkData/log'\n","\n","epochs = 3\n","batch_size = 8\n","lr = 1e-3\n","input_size = 320"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5YzW4g_It3ix"},"source":["학습을 위해서는 제공받은 데이터셋의 landmark 정보를 parsing하여 heatmap 형태로 나타내어야 합니다.\n","\n","Gaussin heatmap 형태로 keypoint를 나타내기 위하여 7강 강의 자료의 24번째 슬라이드를 참고하여 **TO DO**를 채워주세요 "]},{"cell_type":"code","metadata":{"id":"x28F_kjzRyus","executionInfo":{"status":"ok","timestamp":1616488343137,"user_tz":-540,"elapsed":3884,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["# Dataset\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","\n","import os\n","import cv2\n","import json\n","import numpy as np\n","from glob import glob\n","\n","class BodyLandmarkDataset(Dataset):\n","  def __init__(self, data_root, is_Train=True, input_size=224, transform=None):\n","    super(BodyLandmarkDataset, self).__init__()\n","\n","    self.img_list = self._load_img_list(data_root, is_Train)\n","\n","    self.len = len(self.img_list)\n","    self.input_size = input_size\n","    self.hm_size = input_size//4\n","    self.transform = transform\n","    \n","    self.n_landmarks = 22\n","    self.sigma = 1.5\n","\n","  def __getitem__(self, index):\n","    img_path = self.img_list[index]\n","    anno_path = img_path.replace('.jpg', '.json')\n","    \n","    # Image Loading\n","    img = cv2.imread(img_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = img/255.\n","    \n","    org_size = img.shape[:2]\n","\n","    if self.transform:\n","      img = self.transform(img)\n","\n","    # Ground Truth\n","    heatmap = self._get_heatmaps_from_json(anno_path, org_size)\n","\n","    return img, heatmap\n","\n","  def __len__(self):\n","    return self.len\n","  \n","  def _load_img_list(self, data_root, is_Train):\n","    # Change the name of directory which has inconsistent naming rule.\n","    full_img_list = glob(os.path.join(data_root, 'single', '*', '*color.jpg'))\n","    \n","    # ID < 400 for Training\n","    # 400 < ID for Validation\n","    if is_Train:\n","      return [path for path in full_img_list if (self._load_img_ID(path) < 400)]\n","    else:\n","      return [path for path in full_img_list if (400 < self._load_img_ID(path))]\n","\n","  def _load_img_ID(self, path):\n","    return int(path.split(os.sep)[-2].strip('id_1'))\n","\n","  def _get_heatmaps_from_json(self, anno_path, org_size):\n","    # Parse point annotation\n","    with open(anno_path, 'r') as json_file:\n","      pts = json.load(json_file)\n","    pts = np.array([(pt['pt_x'], pt['pt_y']) for pt in pts['DataList'][0]['coordinates']])\n","\n","    pts[:,0] = pts[:,0] / org_size[1] * self.hm_size\n","    pts[:,1] = pts[:,1] / org_size[0] * self.hm_size\n","\n","    heatmap = np.zeros((self.n_landmarks, self.hm_size, self.hm_size), dtype=np.float32)\n","    for i, pt in enumerate(pts):\n","      heatmap[i] = self._draw_labelmap(heatmap[i], org_size, pt, self.sigma)\n","    \n","    return heatmap\n","\n","  def _draw_labelmap(self, heatmap, org_size, pt, sigma):\n","    # Draw a 2D gaussian\n","    # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n","    H, W = heatmap.shape[:2]\n","\n","    # Check that any part of the gaussian is in-bounds\n","    ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n","    br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n","    if (ul[0] >= heatmap.shape[1] or ul[1] >= heatmap.shape[0] or\n","            br[0] < 0 or br[1] < 0):\n","        # If not, just return the image as is\n","        return heatmap, 0\n","\n","    # Generate gaussian\n","    size = 6 * sigma + 1\n","    x = np.arange(0, size, 1, float)\n","    y = x[:, np.newaxis]\n","    x0 = y0 = size // 2\n","    # The gaussian is not normalized, we want the center value to equal 1\n","\n","    '''======================================================='''\n","    '''======================== TO DO ========================'''\n","    g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n","    '''======================== TO DO ========================'''\n","    '''======================================================='''\n","\n","    # Usable gaussian range\n","    g_x = max(0, -ul[0]), min(br[0], heatmap.shape[1]) - ul[0]\n","    g_y = max(0, -ul[1]), min(br[1], heatmap.shape[0]) - ul[1]\n","    # Image range\n","    heatmap_x = max(0, ul[0]), min(br[0], heatmap.shape[1])\n","    heatmap_y = max(0, ul[1]), min(br[1], heatmap.shape[0])\n","\n","    heatmap[heatmap_y[0]:heatmap_y[1], heatmap_x[0]:heatmap_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n","    return heatmap\n","    \n","    return anno_path"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHBs5pAOWKoP","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"error","timestamp":1616488343145,"user_tz":-540,"elapsed":3886,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"7cfede6a-b8f2-48c0-a786-4ac5bbe649ee"},"source":["# Dataset and Data Loader\n","MEAN = [0.485, 0.456, 0.406]\n","STD  = [0.229, 0.224, 0.225]\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((input_size, input_size)),\n","    transforms.Normalize(mean=MEAN,\n","                          std=STD)\n","])\n","\n","train_dataset = BodyLandmarkDataset(data_root, is_Train=True, input_size=input_size, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\n","\n","valid_dataset = BodyLandmarkDataset(data_root, is_Train=False, input_size=input_size, transform=transform)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True, shuffle=False)"],"execution_count":33,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-232637c37cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBodyLandmarkDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_Train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBodyLandmarkDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_Train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m--> 104\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}]},{"cell_type":"code","metadata":{"id":"osvJGX4n4Miy","executionInfo":{"status":"aborted","timestamp":1616488343141,"user_tz":-540,"elapsed":3873,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["# Misc\n","\n","class AverageMeter(object):\n","  \"\"\"Computes and stores the average and current value\"\"\"\n","  def __init__(self):\n","      self.reset()\n","\n","  def reset(self):\n","    self.val = 0\n","    self.avg = 0\n","    self.sum = 0\n","    self.count = 0\n","\n","  def update(self, val, n=1):\n","    self.val = val\n","    self.sum += val * n\n","    self.count += n\n","    self.avg = self.sum / self.count"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4edgVN4ovA0z"},"source":["----\n","#### **>>> 4.2.3 Training**\n","\n","```BodyLandmarkDataset```을 활용하여 Hourglass network를 학습할 시간입니다.\n","\n","### - **TO DO Main (1)** : VGG-11을 본격적으로 학습하는 과정입니다. 주석에 적힌 내용을 따라 loss function인 ```criterion```과 ```optimizer```를 활용하여 빈 부분을 채워주세요.\n","\n","### - **TO DO Main (2)** : 학습된 VGG-11을 validation dataset에 대해 평가하는 과정입니다. Validation 과정에서는 <U>gradient 계산과 backpropagation이 필요 없다</U>는 것에 주목하여 빈 부분을 채워주세요."]},{"cell_type":"code","metadata":{"id":"yewH3ejYbsG5","executionInfo":{"status":"aborted","timestamp":1616488343142,"user_tz":-540,"elapsed":3867,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["# Loss function and Optimizer\n","from torch.optim import Adam\n","\n","criterion = nn.MSELoss()\n","optimizer = Adam(model.parameters(), lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7WybHwZbtfY","executionInfo":{"status":"aborted","timestamp":1616488343143,"user_tz":-540,"elapsed":3863,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["# Main\n","os.makedirs(log_dir, exist_ok=True)\n","\n","with open(os.path.join(log_dir, 'train_log.csv'), 'w') as log:\n","  for epoch in range(epochs):\n","    train_loss, valid_loss = AverageMeter(), AverageMeter()\n","\n","    # Training\n","    for iter, (img, hm_gt) in enumerate(train_loader):\n","      '''================================================================'''\n","      '''======================== TO DO Main (1) ========================'''\n","      # optimizer에 저장된 미분값을 0으로 초기화\n","      optimizer.zero_grad()\n","\n","      # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\n","      img, hm_gt = torch.tensor(img).float().cuda(), torch.tensor(hm_gt).cuda()\n","      # img.float().cuda(), hm_gt.cuda()\n","\n","      # 모델에 이미지 forward\n","      pred_logit = model(img)\n","\n","      # loss 값 계산\n","      loss = 0\n","      for pred in pred_logit:\n","        loss += criterion(pred, hm_gt)\n","\n","      # Backpropagation\n","      loss.backward()\n","      optimizer.step()\n","      '''======================== TO DO Main (1) ========================'''\n","      '''================================================================'''\n","\n","      # Log Update\n","      train_loss.update(loss.item(), len(img))\n","      print(\"\\rEpoch [%3d/%3d] | Iter [%3d/%3d] | Train Loss %.4f\" % (epoch+1, epochs, iter+1, len(train_loader), train_loss.avg), end='')\n","\n","    # Validation\n","    for iter, (img, hm_gt) in enumerate(valid_loader):\n","      '''================================================================'''\n","      '''======================== TO DO Main (2) ========================'''\n","      # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\n","      img, hm_gt = torch.tensor(img).float().cuda(), torch.tensor(hm_gt).cuda()\n","\n","      # 모델에 이미지 forward (gradient 계산 X)\n","      with torch.no_grad():\n","        pred_logit = model(img)\n","\n","      # loss 값 계산\n","      loss = 0\n","      for pred in pred_logit:\n","        loss += criterion(pred, hm_gt)\n","      '''======================== TO DO Main (2) ========================'''\n","      '''================================================================'''\n","\n","      # Log Update\n","      valid_loss.update(loss.item(), len(img))\n"," \n","    print(\"\\nEpoch [%3d/%3d] | Valid Loss %.4f\" % (epoch+1, epochs, valid_loss.avg))\n","    \n","    # Log Writing\n","    log.write('%d,%.4f,%.4f\\n'%(epoch, train_loss.avg, valid_loss.avg))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mBOCszlcvpoN"},"source":["#### **>>> 4.3.3 Visualization**\n","학습된 모델을 바탕으로 샘플 이미지에 대한 keypoint 예측 결과를 시각화하는 단계입니다.\n","\n","- **TO DO** : 아래의 시각화 코드를 활용하여 샘플 이미지에 대한 예측 결과를 시각화해주세요. ```matplotlib```을 이용하여 시각화한 그래프가 해당 colab notebook 파일에 남아있어야 하며 해당 과정을 마치신 뒤에 edwith의 댓글로 colab link를 남겨주세요. (아래 예시 그림 참고) <img src='https://drive.google.com/uc?id=1zCiRG-vQ2lSantORSmQYbqH75rB9Rb5f'  width=\"400\">\n","\n","\n","- **TO DO Main** : 주석을 참고하여 inference를 위한 코드를 완성해주세요.\n","\n","- **TO DO Decoding** : 예측된 heatmap에서 좌표값 (x,y)를 얻어내는 코드를 완성해주세요.\n","<br>(1) ```pred_hm``` 변수는 (channels, height, width) shape을 가집니다.\n","<br>(2) ```hm``` 변수는 ```pred_hm```의 각 channel을 나타내며 (height, width) shape을 가집니다."]},{"cell_type":"code","metadata":{"id":"1eoFBtv_U_ar","executionInfo":{"status":"aborted","timestamp":1616488343144,"user_tz":-540,"elapsed":3858,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}}},"source":["import matplotlib.pyplot as plt\n","\n","n_vis = 5\n","\n","# Visualize the result of validation dataset\n","for iter, (imgs, hm_gt) in enumerate(valid_loader):\n","  '''============================================================'''\n","  '''======================== TO DO Main ========================'''\n","  # GPU 연산을 위해 이미지 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)\n","  imgs = torch.tensor(imgs).float().cuda()\n","  \n","  # 모델에 이미지 forward (gradient 계산 X)\n","  with torch.no_grad():\n","    preds = model(imgs)[0]\n","  '''======================== TO DO Main ========================'''\n","  '''============================================================'''\n","\n","\n","  # for each sample in a batch\n","  imgs = imgs.cpu().numpy()\n","  for img, pred_hm in zip(imgs, preds):\n","    # Re-convert pre-processed input image to original format\n","    img = np.moveaxis(img, 0, -1)\n","    img = (img * STD) + MEAN\n","    img = (img*255).astype(np.uint8).copy()\n","\n","    for hm in pred_hm:\n","      '''======================================================='''\n","      '''==================== TO DO Decoding ==================='''\n","\n","      y, x = (hm==torch.max(hm)).nonzero()[0] #nonzero: 2d array에서 0(=false)이 아닌 값을 찾음\n","      '''==================== TO DO Decoding ==================='''\n","      '''======================================================='''\n","      cv2.circle(img, (x*4, y*4), 3, (255,0,0), -1)\n","    \n","    plt.imshow(img)\n","    plt.show()\n","  \n","\n","  if iter == (n_vis-1):\n","    break"],"execution_count":null,"outputs":[]}]}