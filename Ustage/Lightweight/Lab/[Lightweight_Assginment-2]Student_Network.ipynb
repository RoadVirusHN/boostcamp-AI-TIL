{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"[Lightweight_Assginment-2]Student_Network.ipynb","provenance":[{"file_id":"1Ff3EOl0M7_EYmNv9zF1aE6qQTBhMssRI","timestamp":1615909915610},{"file_id":"1UXaQ-BaLchyP14NmEEQnditHOTcLXD3Q","timestamp":1615180816079},{"file_id":"18ZFh9h793-mpuAnUmFjpuyKdQOLJ3JWU","timestamp":1614665296099}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ChzaoF9qJ6mO"},"source":["# 경량화 과제_2\n","경량화 두번째 과제 파일 입니다.\n","이번 과제는 Student Network의 학습을 진행해 보겠습니다.\n","세부 내용은 다음과 같습니다. \n","\n","```\n","과제2.  Student Network의 학습\n","1. Dataset & Dataloader 만들기 \n","2. Student Network 정의 하기\n","3. Student Network 학습시키기\n","\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"yGqfEGA1sLH3"},"source":["# GPU 할당확인 \n","import torch\n","\n","print('CUDA GPU availalbe : {}'.format(torch.cuda.is_available()))\n","try:\n","    print('{} GPU(s) is(are) allocated'.format(torch.cuda.device_count()))\n","except:\n","    print('GPUs are not allocated. Current runtime is on CPU.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OAADCCLmg-bL"},"source":["## 드라이브 연결 및 PATH 설정 \n","\n","드라이브 마운트와 PATH설정을 하는 단계입니다. 예제는 아래와 같은 상황에서 돌아가도록 맞춰졌으며, \n","자신의 드라이브 환경에 맞추어 설정하시면 되겠습니다. \n","\n","```\n","dataset\n","  |\n","  |-  teacher.pth\n","  |-  경량화 과제_2(문제)\n","  |-  APY170401435_subset_13gb\n","```\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DB4vHB2DKc3R","executionInfo":{"status":"ok","timestamp":1615915275981,"user_tz":-540,"elapsed":904,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"2187ef92-d05d-4e68-f536-3cd4a600cc4b"},"source":["# 코랩 내 드라이브와 연결\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zs4blXeksMvk","executionInfo":{"status":"ok","timestamp":1615915275982,"user_tz":-540,"elapsed":900,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"0a239b86-ad08-4b7c-d5c0-21af100a5941"},"source":["# Path 설정\n","\n","import os\n","if not os.path.exists('/content/gdrive/My Drive/dataset/'):\n","  os.mkdir('/content/gdrive/My Drive/dataset/')\n","os.chdir('/content/gdrive/My Drive/dataset/') # Data_Path\n","current_path = os.getcwd() # current folder\n","print('current_path', current_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["current_path /content/gdrive/My Drive/dataset\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lamerfwXI2ml"},"source":["## 파일이동 확인 \n","정상적으로 파일이 옮겨졌는지 확인 합니다. "]},{"cell_type":"code","metadata":{"id":"ZJPDKCbP8_WD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615915276224,"user_tz":-540,"elapsed":1137,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"e984e03a-927d-4c3a-89d7-6ec8e566f160"},"source":["from glob import glob\n","\n","image = sorted(glob('/content/gdrive/My Drive/dataset/total/*.jpg'))\n","meta = sorted(glob('/content/gdrive/My Drive/dataset/total/*.metadata'))\n","\n","if len(image) == len(meta):\n","    print('train data ({} files) unzip succeed'.format(len(image)))\n","else:\n","    print('train data unzip failed')\n","    print('압축을 확인해주세요.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train data (3881 files) unzip succeed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jaEkAnq9M1rJ"},"source":["## 사용자 정의함수 \n"]},{"cell_type":"code","metadata":{"id":"aluNej1wsM7m"},"source":["# import \n","import numpy as np\n","from PIL import Image\n","import json\n","from glob import glob\n","from pathlib import Path\n","\n","import cv2\n","import os\n","import random\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import imutils\n","import time\n","from datetime import timedelta\n","from collections import deque, defaultdict\n","import re \n","import matplotlib.image as mpimg\n","from collections import OrderedDict\n","from skimage import io, transform\n","from math import *\n","import xml.etree.ElementTree as ET \n","from tqdm import tqdm\n","\n","# torch \n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import torchvision.transforms.functional as TF\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61sZ9MH2SMGm"},"source":["# Dataset / Dataloader\n","\n","\n","1. Image_Dataset_ : dataset 데이터셋을 정의하는 객체\n","2. get_loader : Dataloader 객체를 return하는 함수\n","\n"]},{"cell_type":"code","metadata":{"id":"BE0r9YYWsM99"},"source":["class AttributeDict(dict):\n","    def __init__(self):\n","        self.__dict__ = self\n","        \n","\n","class ConfigTree:\n","    def __init__(self):\n","        self.DATASET = AttributeDict()\n","        self.SYSTEM = AttributeDict()\n","        self.TRAIN = AttributeDict()\n","        self.MODEL = AttributeDict()\n","        self.KD = AttributeDict()\n","\n","class Image_Dataset_(Dataset):\n","    def __init__(self, root, cfg=None, transform=None):\n","        \"\"\"\n","        root : './data/'\n","        \"\"\"\n","        super(Image_Dataset_, self).__init__()\n","        self.root = root\n","        self.cfg = cfg\n","        self.transform = transform\n","        self.images = sorted(glob(self.root + '/*.jpg'))\n","        self.annotations = sorted(glob(self.root + '/*.metadata'))\n","        \n","    def __getitem__(self, idx):\n","# ---------------------------------------------------------------------------- #\n","#  1. getitem 완성시키기\n","# * 주의 * 밑에 사용된 get_gt, label2logit 함수를 이용하여 label 을 만들어야함.\n","#   1) .metadata를 get_gt로 읽기\n","#   2) label2logt을 활용하여 1) 값 int 로 변경 \n","# ---------------------------------------------------------------------------- #\n","\n","       # 채우기 #\n","        metadata = self.annotations[idx]\n","        gt_class = self.get_gt(metadata)\n","        label = self.label2logit(gt_class)\n","        image = cv2.imread(self.images[idx], cv2.IMREAD_COLOR)\n","        image = Image.fromarray(image).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        return image, label\n","        \n","    def __len__(self):\n","        assert len(self.images) == len(self.annotations), \"# of image files and # of meatadata files do not match\"\n","        return len(self.images)\n","    \n","     \n","# ---------------------------------------------------------------------------- #\n","#  과제 1 에서 작성한 함수를 적용\n","# .metadata 를 읽어서 22번째 값(male or female) 을 가져오기 ... \n","# ---------------------------------------------------------------------------- #\n","\n","    def get_gt(self, meta_file):\n","        gt_class = None\n","        meta_file = Path(meta_file)\n","        f = open(meta_file, \"r\")\n","        \n","\n","        # 채우기 #\n","        idx = 0\n","        for line in f.readlines():\n","          if idx == 22:\n","            gt_class = line.split()[1]\n","            break\n","          idx += 1\n","\n","        return gt_class\n","    \n","    def label2logit(self,label):\n","        str_dict = {\n","            'female': 0,\n","            'male':1\n","        }\n","        return str_dict[label]\n","\n","def get_loader(dataset, cfg, shuffle=True):\n","    return DataLoader(dataset, batch_size=cfg.TRAIN.BATCH_SIZE, shuffle=shuffle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XT31FPBgU55D"},"source":["# data_root = \"./total\"\n","\n","# dataset = Image_Dataset_(data_root)\n","\n","# for idx in tqdm(range(len(dataset))):\n","#    _, label = dataset.__getitem__(idx)\n","#    if label in [0,1]:         \n","#      pass\n","#    else:\n","#      print(\"Check __getitem__\")\n","#      break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mJu0pqBpSZQ8"},"source":["# Image Augmentation \n"," get_augmentation : image transform 객체를 return하는 함수\n","- resize_crop \n","- random_flip\n","- color_jitter\n"]},{"cell_type":"code","metadata":{"id":"GhRuQP7_SYi9"},"source":["def get_augmentation(size=224, use_flip=True, use_color_jitter=False, use_gray_scale=False, use_normalize=False):\n","    resize_crop = transforms.RandomResizedCrop(size=size)\n","    random_flip = transforms.RandomHorizontalFlip(p=0.5)\n","    color_jitter = transforms.RandomApply([\n","        transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n","    ], p=0.8)\n","    \n","    gray_scale = transforms.RandomGrayscale(p=0.2)\n","    normalize = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","    to_tensor = transforms.ToTensor()\n","    \n","    transforms_array = np.array([resize_crop, random_flip, color_jitter, gray_scale, to_tensor, normalize])\n","    transforms_mask = np.array([True, use_flip, use_color_jitter, use_gray_scale, True, use_normalize])\n","    \n","    transform = transforms.Compose(transforms_array[transforms_mask])\n","    \n","    return transform\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D_21uHH6_c0B"},"source":["## Student model\n","\n","정의 해보기 \n","\n"]},{"cell_type":"code","metadata":{"id":"wnQwsAd0_a29"},"source":["# ---------------------------------------------------------------------------- #\n","# 1. (코드 채우기) 학습에 사용 될 모델(Student)을 정의해 봅시다. \n","# *주의* 마지막 레이어는 2개가 사용되어야 됩니다. \n","# ---------------------------------------------------------------------------- #\n","\n","class Student(nn.Module):\n","  def __init__(self):\n","      super().__init__()\n","      # self.cnn1 = nn.Conv2d(in_channels=3, out_channels=128,kernel_size=3,stride=1,padding=2)\n","      # self.norm1 = nn.BatchNorm2d(num_features=128)\n","      # self.relu1 = nn.ReLU()\n","      # self.drop1 = nn.Dropout2d(p=0.1) \n","      # conv_block = nn.Sequential(      \n","      #     nn.Conv2d(in_channels=128, out_channels=64,kernel_size=3,stride=1,padding=1),\n","      #     nn.BatchNorm2d(num_features=64),\n","      #     nn.ReLU(),\n","      #     nn.Dropout2d(p=0.1)\n","      #     )\n","      # self.layer2 = conv_block\n","      # self.fc = nn.Linear(in_features=226,out_features=2,bias=True)\n","      self.layer = torchvision.models.resnet34(pretrained=True)\n","      num_ftrs = self.layer.fc.in_features\n","      self.layer.fc = nn.Linear(num_ftrs,2, bias=True)\n","\n","  def forward(self, x):\n","    # x = self.cnn1(x)\n","    # x = self.norm1(x)\n","    # x = self.relu1(x)\n","    # x = self.drop1(x)\n","    # x = self.layer2(x)\n","    # print(x.shape)\n","    # x = x.view(-1, 226)\n","    # x = self.fc(x)\n","    # print(x.shape)\n","    # x = torch.sigmoid(x)\n","    # print(x.shape)\n","    x= self.layer(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rpdXGArVPLHB","executionInfo":{"status":"ok","timestamp":1615916807050,"user_tz":-540,"elapsed":1093,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"668b4d9e-ef5e-4e28-a1e5-84f87d98090d"},"source":["Student()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Student(\n","  (layer): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (4): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (5): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=512, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":137}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p15amNYkMZzT","executionInfo":{"status":"ok","timestamp":1615916810746,"user_tz":-540,"elapsed":1427,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"b5734e58-05bb-40f9-fb19-5d3ca315f179"},"source":["# 모델 확인하기 \n","from torchsummary import summary\n","\n","try:\n","  # assert Student().model.fc.out_features ==2\n","  summary(Student(), (3,224, 224),device='cpu')\n","except  RuntimeError as err:\n","  print(err)\n","  print('Student Network를 재정의 해주세요.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","       BasicBlock-11           [-1, 64, 56, 56]               0\n","           Conv2d-12           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-13           [-1, 64, 56, 56]             128\n","             ReLU-14           [-1, 64, 56, 56]               0\n","           Conv2d-15           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-16           [-1, 64, 56, 56]             128\n","             ReLU-17           [-1, 64, 56, 56]               0\n","       BasicBlock-18           [-1, 64, 56, 56]               0\n","           Conv2d-19           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-20           [-1, 64, 56, 56]             128\n","             ReLU-21           [-1, 64, 56, 56]               0\n","           Conv2d-22           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-23           [-1, 64, 56, 56]             128\n","             ReLU-24           [-1, 64, 56, 56]               0\n","       BasicBlock-25           [-1, 64, 56, 56]               0\n","           Conv2d-26          [-1, 128, 28, 28]          73,728\n","      BatchNorm2d-27          [-1, 128, 28, 28]             256\n","             ReLU-28          [-1, 128, 28, 28]               0\n","           Conv2d-29          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-30          [-1, 128, 28, 28]             256\n","           Conv2d-31          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-32          [-1, 128, 28, 28]             256\n","             ReLU-33          [-1, 128, 28, 28]               0\n","       BasicBlock-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-36          [-1, 128, 28, 28]             256\n","             ReLU-37          [-1, 128, 28, 28]               0\n","           Conv2d-38          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-39          [-1, 128, 28, 28]             256\n","             ReLU-40          [-1, 128, 28, 28]               0\n","       BasicBlock-41          [-1, 128, 28, 28]               0\n","           Conv2d-42          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-43          [-1, 128, 28, 28]             256\n","             ReLU-44          [-1, 128, 28, 28]               0\n","           Conv2d-45          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-46          [-1, 128, 28, 28]             256\n","             ReLU-47          [-1, 128, 28, 28]               0\n","       BasicBlock-48          [-1, 128, 28, 28]               0\n","           Conv2d-49          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-50          [-1, 128, 28, 28]             256\n","             ReLU-51          [-1, 128, 28, 28]               0\n","           Conv2d-52          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-53          [-1, 128, 28, 28]             256\n","             ReLU-54          [-1, 128, 28, 28]               0\n","       BasicBlock-55          [-1, 128, 28, 28]               0\n","           Conv2d-56          [-1, 256, 14, 14]         294,912\n","      BatchNorm2d-57          [-1, 256, 14, 14]             512\n","             ReLU-58          [-1, 256, 14, 14]               0\n","           Conv2d-59          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-60          [-1, 256, 14, 14]             512\n","           Conv2d-61          [-1, 256, 14, 14]          32,768\n","      BatchNorm2d-62          [-1, 256, 14, 14]             512\n","             ReLU-63          [-1, 256, 14, 14]               0\n","       BasicBlock-64          [-1, 256, 14, 14]               0\n","           Conv2d-65          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-66          [-1, 256, 14, 14]             512\n","             ReLU-67          [-1, 256, 14, 14]               0\n","           Conv2d-68          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-69          [-1, 256, 14, 14]             512\n","             ReLU-70          [-1, 256, 14, 14]               0\n","       BasicBlock-71          [-1, 256, 14, 14]               0\n","           Conv2d-72          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-73          [-1, 256, 14, 14]             512\n","             ReLU-74          [-1, 256, 14, 14]               0\n","           Conv2d-75          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-76          [-1, 256, 14, 14]             512\n","             ReLU-77          [-1, 256, 14, 14]               0\n","       BasicBlock-78          [-1, 256, 14, 14]               0\n","           Conv2d-79          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-80          [-1, 256, 14, 14]             512\n","             ReLU-81          [-1, 256, 14, 14]               0\n","           Conv2d-82          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-83          [-1, 256, 14, 14]             512\n","             ReLU-84          [-1, 256, 14, 14]               0\n","       BasicBlock-85          [-1, 256, 14, 14]               0\n","           Conv2d-86          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-87          [-1, 256, 14, 14]             512\n","             ReLU-88          [-1, 256, 14, 14]               0\n","           Conv2d-89          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-90          [-1, 256, 14, 14]             512\n","             ReLU-91          [-1, 256, 14, 14]               0\n","       BasicBlock-92          [-1, 256, 14, 14]               0\n","           Conv2d-93          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-94          [-1, 256, 14, 14]             512\n","             ReLU-95          [-1, 256, 14, 14]               0\n","           Conv2d-96          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-97          [-1, 256, 14, 14]             512\n","             ReLU-98          [-1, 256, 14, 14]               0\n","       BasicBlock-99          [-1, 256, 14, 14]               0\n","          Conv2d-100            [-1, 512, 7, 7]       1,179,648\n","     BatchNorm2d-101            [-1, 512, 7, 7]           1,024\n","            ReLU-102            [-1, 512, 7, 7]               0\n","          Conv2d-103            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-104            [-1, 512, 7, 7]           1,024\n","          Conv2d-105            [-1, 512, 7, 7]         131,072\n","     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n","            ReLU-107            [-1, 512, 7, 7]               0\n","      BasicBlock-108            [-1, 512, 7, 7]               0\n","          Conv2d-109            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-110            [-1, 512, 7, 7]           1,024\n","            ReLU-111            [-1, 512, 7, 7]               0\n","          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n","            ReLU-114            [-1, 512, 7, 7]               0\n","      BasicBlock-115            [-1, 512, 7, 7]               0\n","          Conv2d-116            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n","            ReLU-118            [-1, 512, 7, 7]               0\n","          Conv2d-119            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-120            [-1, 512, 7, 7]           1,024\n","            ReLU-121            [-1, 512, 7, 7]               0\n","      BasicBlock-122            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n","          Linear-124                    [-1, 2]           1,026\n","          ResNet-125                    [-1, 2]               0\n","================================================================\n","Total params: 21,285,698\n","Trainable params: 21,285,698\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 96.28\n","Params size (MB): 81.20\n","Estimated Total Size (MB): 178.05\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XKtz5a1zSsB6"},"source":["## Train\n","\n","- Trainer : config에서 주어진 실험 조건들로 학습을 행하는 객체"]},{"cell_type":"markdown","metadata":{"id":"ru4KaDOj3vFa"},"source":[""]},{"cell_type":"code","metadata":{"id":"Z62oQNucSrfJ"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from datetime import timedelta\n","import time\n","from tqdm import tqdm\n","\n","class Trainer:\n","    def __init__(self, model, optimizer, cfg, device, train_loader, val_loader):\n","        self.cfg = cfg\n","        self.device = device\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.save_path = cfg.BASE_SAVE_DIR\n","        \n","        print(\"[INFO] using gpu {}\".format(cfg.SYSTEM.GPU))\n","\n","        self.model = model.to(device)\n","        \n","        update_params = [p for p in model.parameters() if p.requires_grad]\n","        self.optimizer = optimizer(update_params, lr=cfg.TRAIN.BASE_LR, weight_decay=cfg.TRAIN.WEIGHT_DECAY)\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.total_step = 0\n","        self.training_time = 0\n","        \n","        print(model)\n","        print()\n","        print(optimizer)\n","        print()\n","        \n","    def train_eval(self, name='teacher'):\n","        print(\"training start!\\n\")\n","        for epoch in tqdm(range(self.cfg.TRAIN.EPOCH)):\n","            self.train_one_epoch(epoch)\n","            self.test_one_epoch(epoch)\n","            self.save_model(epoch,name)\n","    \n","    def train_one_epoch(self,epoch):\n","        print('\\nEpoch: %d' % epoch)\n","        self.model.train()\n","        \n","        train_loss = 0\n","        correct = 0\n","        total = 0\n","        for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n","            inputs, targets = inputs.to(self.device), targets.to(self.device)\n","            self.optimizer.zero_grad()\n","            outputs = self.model(inputs)\n","            loss = self.criterion(outputs, targets)\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            if batch_idx % self.cfg.TRAIN.PERIOD ==0:\n","                print('Loss: %.3f | Acc: %.3f%% ' %(train_loss/(batch_idx+1),100.*correct/total))\n","        \n","    def test_one_epoch(self,epoch):\n","        self.model.eval()\n","        \n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        \n","        with torch.no_grad():\n","            for batch_idx, (inputs, targets) in enumerate(self.val_loader):\n","                inputs, targets = inputs.to(self.device), targets.to(self.device)\n","                outputs = self.model(inputs)\n","                loss = self.criterion(outputs, targets)\n","\n","                test_loss += loss.item()\n","                _, predicted = outputs.max(1)\n","                total += targets.size(0)\n","                correct += predicted.eq(targets).sum().item()\n","                \n","        print(f'Accuracy of the network on the {total} test images: %d %%' % (\n","        100 * correct / total))\n","        \n","    \n","    def save_model(self, epoch, name='student'):\n","        print('saved model {}'.format(self.save_path)) \n","        if not os.path.exists(self.save_path):\n","            os.mkdir(self.save_path)\n","        file_name = '{}_{}.pth'.format(name, epoch)\n","        file_path = os.path.join(self.save_path, file_name)\n","        torch.save(self.model.state_dict(), file_path)\n","            \n","        return None      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PJAzbWaUTD_7"},"source":["# Config\n","\n","모델 변수(즉, 모델의 학습 조건을 설정해주는 공간 ) "]},{"cell_type":"code","metadata":{"id":"gEU_UTcdSCr0"},"source":["# config\n","config = ConfigTree()\n","config.SYSTEM.GPU = 0 # GPU 번호 \n","config.BASE_SAVE_DIR = './outputs' # 모델 파라미터가 저장되는 위치\n","\n","config.DATASET.ROOT = \"./total\" # 데이터 위치\n","config.DATASET.NUM_CLASSES = 2 # 분류해야 하는 클래스 종류의 수 \n","config.DATASET.RATIO = 0.3 # train, test split 비율 \n","\n","\n","\n","\n","# ---------------------------------------------------------------------------- #\n","#  밑에 있는 hyper-parameter 값을 조정하여 정확도 올려 보도록 합니다. \n","# ---------------------------------------------------------------------------- #\n","\n","# 사용할 augmentation\n","config.TRAIN.AUGMENTATION = {'size' : 224,\n","                             'use_flip' : True,\n","                             'use_color_jitter' : False,\n","                             'use_normalize' : True\n","} \n","config.TRAIN.WEIGHT_DECAY = 0.9\n","config.TRAIN.EPOCH = 30 # 총 학습 에폭 \n","config.TRAIN.BATCH_SIZE = 32 # 배치 사이즈 \n","config.TRAIN.BASE_LR = 1e-04 # 러닝 레이트 \n","config.TRAIN.PERIOD = 1 # loss 측정 주기\n","config.MODEL.OPTIM = 'SGD' # 'ASGD, Adam, rmsprop etc...'\n","\n","config.TRAIN.STUDENT_MODEL = Student()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PkOJZzwBS9iY"},"source":["## main \n","\n","학습 코드를 실행 "]},{"cell_type":"code","metadata":{"id":"ne5psROhR11H"},"source":["def main(config):\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  data_root = config.DATASET.ROOT\n","  transform = get_augmentation(**config.TRAIN.AUGMENTATION)\n","  dataset = Image_Dataset_(data_root, config, transform=transform)\n","\n","  len_valid_set = int(config.DATASET.RATIO*len(dataset))\n","  len_train_set = len(dataset) - len_valid_set\n","\n","  print(\"The length of Train set is {}\".format(len_train_set))\n","  print(\"The length of Valid set is {}\".format(len_valid_set))\n","\n","  train_dataset , valid_dataset = torch.utils.data.random_split(dataset ,\n","                                                                [len_train_set, len_valid_set])\n","\n","  # shuffle and batch the datasets\n","  train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                            batch_size=config.TRAIN.BATCH_SIZE,\n","                                            shuffle=True)\n","  valid_loader = torch.utils.data.DataLoader(valid_dataset,\n","                                            batch_size=config.TRAIN.BATCH_SIZE,\n","                                            shuffle=False)\n","\n","  # define model(student)\n","  model = config.TRAIN.STUDENT_MODEL\n","  optimizer = optim.__dict__[config.MODEL.OPTIM]\n","\n","  # fitting ... \n","  trainer = Trainer(model, optimizer, config, device,\n","                    train_loader, valid_loader)\n","  trainer.train_eval()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XDX1K_DAJ154"},"source":["## Student Network 학습 시키기"]},{"cell_type":"code","metadata":{"id":"STF-b1v_J1Ba","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615933848451,"user_tz":-540,"elapsed":15225136,"user":{"displayName":"윤준석","photoUrl":"","userId":"07467358081291211272"}},"outputId":"ebd6b7cb-8d61-4ff7-e365-bda4d48c4880"},"source":["main(config)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","\n","\n","\n","  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["The length of Train set is 2717\n","The length of Valid set is 1164\n","[INFO] using gpu 0\n","Student(\n","  (layer): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (3): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (4): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (5): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (2): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=512, out_features=2, bias=True)\n","  )\n",")\n","\n","<class 'torch.optim.sgd.SGD'>\n","\n","training start!\n","\n","\n","Epoch: 0\n","Loss: 0.663 | Acc: 53.125% \n","Loss: 0.668 | Acc: 59.375% \n","Loss: 0.678 | Acc: 58.333% \n","Loss: 0.705 | Acc: 53.906% \n","Loss: 0.701 | Acc: 55.625% \n","Loss: 0.708 | Acc: 54.167% \n","Loss: 0.705 | Acc: 54.464% \n","Loss: 0.709 | Acc: 54.297% \n","Loss: 0.714 | Acc: 53.472% \n","Loss: 0.713 | Acc: 52.812% \n","Loss: 0.707 | Acc: 54.261% \n","Loss: 0.705 | Acc: 54.948% \n","Loss: 0.709 | Acc: 53.606% \n","Loss: 0.708 | Acc: 53.125% \n","Loss: 0.709 | Acc: 53.125% \n","Loss: 0.707 | Acc: 53.711% \n","Loss: 0.704 | Acc: 53.493% \n","Loss: 0.705 | Acc: 53.299% \n","Loss: 0.705 | Acc: 53.618% \n","Loss: 0.702 | Acc: 53.906% \n","Loss: 0.697 | Acc: 54.464% \n","Loss: 0.699 | Acc: 53.977% \n","Loss: 0.700 | Acc: 53.804% \n","Loss: 0.700 | Acc: 53.646% \n","Loss: 0.699 | Acc: 53.500% \n","Loss: 0.699 | Acc: 53.966% \n","Loss: 0.700 | Acc: 53.241% \n","Loss: 0.701 | Acc: 52.790% \n","Loss: 0.701 | Acc: 52.802% \n","Loss: 0.700 | Acc: 53.125% \n","Loss: 0.701 | Acc: 53.024% \n","Loss: 0.703 | Acc: 52.734% \n","Loss: 0.702 | Acc: 52.841% \n","Loss: 0.700 | Acc: 52.665% \n","Loss: 0.701 | Acc: 52.679% \n","Loss: 0.700 | Acc: 52.778% \n","Loss: 0.700 | Acc: 52.787% \n","Loss: 0.699 | Acc: 52.878% \n","Loss: 0.699 | Acc: 52.644% \n","Loss: 0.699 | Acc: 52.969% \n","Loss: 0.698 | Acc: 53.201% \n","Loss: 0.699 | Acc: 52.976% \n","Loss: 0.700 | Acc: 52.689% \n","Loss: 0.697 | Acc: 53.196% \n","Loss: 0.699 | Acc: 52.986% \n","Loss: 0.697 | Acc: 53.057% \n","Loss: 0.697 | Acc: 53.125% \n","Loss: 0.696 | Acc: 53.320% \n","Loss: 0.697 | Acc: 53.253% \n","Loss: 0.695 | Acc: 53.438% \n","Loss: 0.694 | Acc: 53.799% \n","Loss: 0.693 | Acc: 54.026% \n","Loss: 0.693 | Acc: 54.009% \n","Loss: 0.694 | Acc: 53.877% \n","Loss: 0.693 | Acc: 53.977% \n","Loss: 0.693 | Acc: 54.074% \n","Loss: 0.693 | Acc: 54.221% \n","Loss: 0.692 | Acc: 54.364% \n","Loss: 0.692 | Acc: 54.502% \n","Loss: 0.692 | Acc: 54.531% \n","Loss: 0.692 | Acc: 54.559% \n","Loss: 0.692 | Acc: 54.587% \n","Loss: 0.692 | Acc: 54.514% \n","Loss: 0.691 | Acc: 54.639% \n","Loss: 0.690 | Acc: 54.904% \n","Loss: 0.690 | Acc: 54.877% \n","Loss: 0.690 | Acc: 54.757% \n","Loss: 0.691 | Acc: 54.688% \n","Loss: 0.692 | Acc: 54.484% \n","Loss: 0.691 | Acc: 54.688% \n","Loss: 0.691 | Acc: 54.710% \n","Loss: 0.691 | Acc: 54.688% \n","Loss: 0.691 | Acc: 54.666% \n","Loss: 0.690 | Acc: 54.645% \n","Loss: 0.690 | Acc: 54.708% \n","Loss: 0.691 | Acc: 54.770% \n","Loss: 0.690 | Acc: 54.830% \n","Loss: 0.690 | Acc: 54.928% \n","Loss: 0.690 | Acc: 54.826% \n","Loss: 0.689 | Acc: 54.805% \n","Loss: 0.689 | Acc: 54.630% \n","Loss: 0.690 | Acc: 54.421% \n","Loss: 0.691 | Acc: 54.330% \n","Loss: 0.690 | Acc: 54.501% \n","Loss: 0.689 | Acc: 54.582% \n","Accuracy of the network on the 1164 test images: 57 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n","  3%|▎         | 1/30 [17:24<8:25:03, 1044.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 1\n","Loss: 0.635 | Acc: 65.625% \n","Loss: 0.628 | Acc: 64.062% \n","Loss: 0.625 | Acc: 67.708% \n","Loss: 0.652 | Acc: 63.281% \n","Loss: 0.641 | Acc: 65.625% \n","Loss: 0.651 | Acc: 63.021% \n","Loss: 0.653 | Acc: 62.946% \n","Loss: 0.654 | Acc: 62.109% \n","Loss: 0.658 | Acc: 60.764% \n","Loss: 0.657 | Acc: 60.312% \n","Loss: 0.657 | Acc: 60.227% \n","Loss: 0.654 | Acc: 60.677% \n","Loss: 0.654 | Acc: 60.577% \n","Loss: 0.653 | Acc: 60.268% \n","Loss: 0.650 | Acc: 60.833% \n","Loss: 0.650 | Acc: 60.547% \n","Loss: 0.653 | Acc: 60.662% \n","Loss: 0.654 | Acc: 60.417% \n","Loss: 0.657 | Acc: 60.033% \n","Loss: 0.656 | Acc: 60.000% \n","Loss: 0.656 | Acc: 59.673% \n","Loss: 0.658 | Acc: 59.091% \n","Loss: 0.658 | Acc: 59.103% \n","Loss: 0.661 | Acc: 58.984% \n","Loss: 0.659 | Acc: 59.375% \n","Loss: 0.659 | Acc: 59.375% \n","Loss: 0.657 | Acc: 59.259% \n","Loss: 0.654 | Acc: 60.045% \n","Loss: 0.656 | Acc: 60.129% \n","Loss: 0.657 | Acc: 59.688% \n","Loss: 0.659 | Acc: 59.476% \n","Loss: 0.660 | Acc: 58.984% \n","Loss: 0.662 | Acc: 58.617% \n","Loss: 0.663 | Acc: 58.272% \n","Loss: 0.665 | Acc: 58.214% \n","Loss: 0.665 | Acc: 58.420% \n","Loss: 0.665 | Acc: 58.361% \n","Loss: 0.666 | Acc: 58.306% \n","Loss: 0.667 | Acc: 58.253% \n","Loss: 0.668 | Acc: 58.359% \n","Loss: 0.668 | Acc: 58.232% \n","Loss: 0.668 | Acc: 58.557% \n","Loss: 0.668 | Acc: 58.430% \n","Loss: 0.668 | Acc: 58.665% \n","Loss: 0.668 | Acc: 58.681% \n","Loss: 0.667 | Acc: 58.696% \n","Loss: 0.668 | Acc: 58.710% \n","Loss: 0.668 | Acc: 58.724% \n","Loss: 0.667 | Acc: 58.865% \n","Loss: 0.666 | Acc: 59.000% \n","Loss: 0.667 | Acc: 58.946% \n","Loss: 0.667 | Acc: 59.075% \n","Loss: 0.668 | Acc: 58.844% \n","Loss: 0.668 | Acc: 58.854% \n","Loss: 0.668 | Acc: 58.864% \n","Loss: 0.668 | Acc: 59.040% \n","Loss: 0.667 | Acc: 59.265% \n","Loss: 0.665 | Acc: 59.537% \n","Loss: 0.664 | Acc: 59.693% \n","Loss: 0.664 | Acc: 59.740% \n","Loss: 0.664 | Acc: 59.887% \n","Loss: 0.664 | Acc: 59.778% \n","Loss: 0.664 | Acc: 59.772% \n","Loss: 0.664 | Acc: 59.766% \n","Loss: 0.665 | Acc: 59.663% \n","Loss: 0.665 | Acc: 59.706% \n","Loss: 0.664 | Acc: 59.935% \n","Loss: 0.663 | Acc: 59.972% \n","Loss: 0.662 | Acc: 60.100% \n","Loss: 0.664 | Acc: 60.089% \n","Loss: 0.663 | Acc: 60.299% \n","Loss: 0.663 | Acc: 60.330% \n","Loss: 0.662 | Acc: 60.360% \n","Loss: 0.661 | Acc: 60.600% \n","Loss: 0.662 | Acc: 60.417% \n","Loss: 0.663 | Acc: 60.403% \n","Loss: 0.663 | Acc: 60.308% \n","Loss: 0.664 | Acc: 60.296% \n","Loss: 0.664 | Acc: 60.324% \n","Loss: 0.663 | Acc: 60.469% \n","Loss: 0.663 | Acc: 60.610% \n","Loss: 0.663 | Acc: 60.480% \n","Loss: 0.663 | Acc: 60.354% \n","Loss: 0.663 | Acc: 60.268% \n","Loss: 0.663 | Acc: 60.287% \n","Accuracy of the network on the 1164 test images: 63 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n","  7%|▋         | 2/30 [25:37<6:50:19, 879.25s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 2\n","Loss: 0.622 | Acc: 68.750% \n","Loss: 0.642 | Acc: 62.500% \n","Loss: 0.634 | Acc: 63.542% \n","Loss: 0.648 | Acc: 61.719% \n","Loss: 0.647 | Acc: 61.875% \n","Loss: 0.635 | Acc: 64.583% \n","Loss: 0.634 | Acc: 65.625% \n","Loss: 0.637 | Acc: 65.625% \n","Loss: 0.638 | Acc: 64.583% \n","Loss: 0.638 | Acc: 64.062% \n","Loss: 0.636 | Acc: 64.205% \n","Loss: 0.642 | Acc: 63.802% \n","Loss: 0.639 | Acc: 64.663% \n","Loss: 0.639 | Acc: 64.732% \n","Loss: 0.640 | Acc: 64.583% \n","Loss: 0.642 | Acc: 64.453% \n","Loss: 0.642 | Acc: 64.890% \n","Loss: 0.645 | Acc: 64.236% \n","Loss: 0.644 | Acc: 64.474% \n","Loss: 0.642 | Acc: 64.688% \n","Loss: 0.642 | Acc: 64.583% \n","Loss: 0.642 | Acc: 64.489% \n","Loss: 0.643 | Acc: 64.402% \n","Loss: 0.641 | Acc: 64.714% \n","Loss: 0.640 | Acc: 64.625% \n","Loss: 0.641 | Acc: 64.303% \n","Loss: 0.640 | Acc: 64.583% \n","Loss: 0.639 | Acc: 64.621% \n","Loss: 0.641 | Acc: 64.440% \n","Loss: 0.639 | Acc: 64.375% \n","Loss: 0.638 | Acc: 64.819% \n","Loss: 0.637 | Acc: 64.941% \n","Loss: 0.639 | Acc: 64.394% \n","Loss: 0.639 | Acc: 64.246% \n","Loss: 0.637 | Acc: 64.643% \n","Loss: 0.637 | Acc: 64.670% \n","Loss: 0.639 | Acc: 64.358% \n","Loss: 0.639 | Acc: 64.391% \n","Loss: 0.638 | Acc: 64.744% \n","Loss: 0.637 | Acc: 64.766% \n","Loss: 0.636 | Acc: 64.939% \n","Loss: 0.637 | Acc: 64.881% \n","Loss: 0.636 | Acc: 64.971% \n","Loss: 0.637 | Acc: 64.702% \n","Loss: 0.638 | Acc: 64.514% \n","Loss: 0.641 | Acc: 63.995% \n","Loss: 0.642 | Acc: 63.963% \n","Loss: 0.642 | Acc: 63.867% \n","Loss: 0.642 | Acc: 63.712% \n","Loss: 0.643 | Acc: 63.500% \n","Loss: 0.643 | Acc: 63.358% \n","Loss: 0.643 | Acc: 63.401% \n","Loss: 0.646 | Acc: 62.972% \n","Loss: 0.646 | Acc: 62.847% \n","Loss: 0.646 | Acc: 62.670% \n","Loss: 0.647 | Acc: 62.612% \n","Loss: 0.648 | Acc: 62.500% \n","Loss: 0.647 | Acc: 62.662% \n","Loss: 0.648 | Acc: 62.394% \n","Loss: 0.648 | Acc: 62.135% \n","Loss: 0.648 | Acc: 62.193% \n","Loss: 0.647 | Acc: 62.248% \n","Loss: 0.647 | Acc: 62.401% \n","Loss: 0.647 | Acc: 62.354% \n","Loss: 0.647 | Acc: 62.452% \n","Loss: 0.647 | Acc: 62.358% \n","Loss: 0.646 | Acc: 62.547% \n","Loss: 0.646 | Acc: 62.454% \n","Loss: 0.648 | Acc: 62.047% \n","Loss: 0.648 | Acc: 61.964% \n","Loss: 0.649 | Acc: 61.884% \n","Loss: 0.648 | Acc: 61.936% \n","Loss: 0.648 | Acc: 61.943% \n","Loss: 0.649 | Acc: 61.740% \n","Loss: 0.649 | Acc: 61.625% \n","Loss: 0.649 | Acc: 61.595% \n","Loss: 0.649 | Acc: 61.769% \n","Loss: 0.648 | Acc: 61.819% \n","Loss: 0.647 | Acc: 61.946% \n","Loss: 0.647 | Acc: 61.836% \n","Loss: 0.647 | Acc: 61.921% \n","Loss: 0.646 | Acc: 61.928% \n","Loss: 0.647 | Acc: 61.785% \n","Loss: 0.647 | Acc: 61.830% \n","Loss: 0.647 | Acc: 61.870% \n","Accuracy of the network on the 1164 test images: 64 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 10%|█         | 3/30 [33:44<5:42:42, 761.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 3\n","Loss: 0.600 | Acc: 75.000% \n","Loss: 0.606 | Acc: 75.000% \n","Loss: 0.621 | Acc: 67.708% \n","Loss: 0.628 | Acc: 65.625% \n","Loss: 0.632 | Acc: 65.000% \n","Loss: 0.636 | Acc: 63.542% \n","Loss: 0.622 | Acc: 65.625% \n","Loss: 0.627 | Acc: 64.453% \n","Loss: 0.632 | Acc: 64.236% \n","Loss: 0.631 | Acc: 65.000% \n","Loss: 0.633 | Acc: 63.920% \n","Loss: 0.628 | Acc: 65.104% \n","Loss: 0.631 | Acc: 64.663% \n","Loss: 0.633 | Acc: 64.062% \n","Loss: 0.635 | Acc: 63.542% \n","Loss: 0.639 | Acc: 63.086% \n","Loss: 0.638 | Acc: 62.500% \n","Loss: 0.636 | Acc: 62.674% \n","Loss: 0.636 | Acc: 62.993% \n","Loss: 0.636 | Acc: 63.281% \n","Loss: 0.634 | Acc: 63.244% \n","Loss: 0.634 | Acc: 62.926% \n","Loss: 0.635 | Acc: 62.772% \n","Loss: 0.634 | Acc: 62.760% \n","Loss: 0.633 | Acc: 63.250% \n","Loss: 0.633 | Acc: 63.582% \n","Loss: 0.636 | Acc: 62.731% \n","Loss: 0.635 | Acc: 62.946% \n","Loss: 0.634 | Acc: 63.470% \n","Loss: 0.632 | Acc: 63.646% \n","Loss: 0.631 | Acc: 63.810% \n","Loss: 0.629 | Acc: 64.355% \n","Loss: 0.627 | Acc: 64.773% \n","Loss: 0.629 | Acc: 64.614% \n","Loss: 0.628 | Acc: 65.000% \n","Loss: 0.627 | Acc: 65.191% \n","Loss: 0.626 | Acc: 65.118% \n","Loss: 0.628 | Acc: 64.967% \n","Loss: 0.626 | Acc: 65.144% \n","Loss: 0.627 | Acc: 65.078% \n","Loss: 0.625 | Acc: 65.549% \n","Loss: 0.626 | Acc: 65.402% \n","Loss: 0.625 | Acc: 65.698% \n","Loss: 0.624 | Acc: 65.767% \n","Loss: 0.623 | Acc: 65.903% \n","Loss: 0.623 | Acc: 65.897% \n","Loss: 0.622 | Acc: 65.957% \n","Loss: 0.625 | Acc: 65.560% \n","Loss: 0.624 | Acc: 65.625% \n","Loss: 0.625 | Acc: 65.500% \n","Loss: 0.624 | Acc: 65.809% \n","Loss: 0.624 | Acc: 65.745% \n","Loss: 0.624 | Acc: 65.743% \n","Loss: 0.623 | Acc: 65.972% \n","Loss: 0.622 | Acc: 66.080% \n","Loss: 0.623 | Acc: 65.960% \n","Loss: 0.624 | Acc: 65.789% \n","Loss: 0.624 | Acc: 65.841% \n","Loss: 0.624 | Acc: 65.678% \n","Loss: 0.625 | Acc: 65.521% \n","Loss: 0.626 | Acc: 65.471% \n","Loss: 0.626 | Acc: 65.423% \n","Loss: 0.626 | Acc: 65.575% \n","Loss: 0.627 | Acc: 65.332% \n","Loss: 0.626 | Acc: 65.433% \n","Loss: 0.626 | Acc: 65.483% \n","Loss: 0.627 | Acc: 65.392% \n","Loss: 0.627 | Acc: 65.165% \n","Loss: 0.626 | Acc: 65.353% \n","Loss: 0.626 | Acc: 65.357% \n","Loss: 0.626 | Acc: 65.273% \n","Loss: 0.626 | Acc: 65.451% \n","Loss: 0.626 | Acc: 65.411% \n","Loss: 0.627 | Acc: 65.414% \n","Loss: 0.628 | Acc: 65.375% \n","Loss: 0.627 | Acc: 65.543% \n","Loss: 0.627 | Acc: 65.544% \n","Loss: 0.628 | Acc: 65.425% \n","Loss: 0.628 | Acc: 65.427% \n","Loss: 0.628 | Acc: 65.547% \n","Loss: 0.628 | Acc: 65.432% \n","Loss: 0.628 | Acc: 65.511% \n","Loss: 0.628 | Acc: 65.474% \n","Loss: 0.628 | Acc: 65.625% \n","Loss: 0.628 | Acc: 65.661% \n","Accuracy of the network on the 1164 test images: 66 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 13%|█▎        | 4/30 [41:52<4:54:24, 679.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 4\n","Loss: 0.623 | Acc: 62.500% \n","Loss: 0.628 | Acc: 65.625% \n","Loss: 0.629 | Acc: 64.583% \n","Loss: 0.639 | Acc: 62.500% \n","Loss: 0.640 | Acc: 63.750% \n","Loss: 0.633 | Acc: 64.583% \n","Loss: 0.626 | Acc: 66.071% \n","Loss: 0.636 | Acc: 64.453% \n","Loss: 0.636 | Acc: 64.931% \n","Loss: 0.635 | Acc: 65.000% \n","Loss: 0.638 | Acc: 64.773% \n","Loss: 0.637 | Acc: 63.802% \n","Loss: 0.638 | Acc: 64.423% \n","Loss: 0.634 | Acc: 65.402% \n","Loss: 0.627 | Acc: 66.667% \n","Loss: 0.626 | Acc: 66.992% \n","Loss: 0.626 | Acc: 67.279% \n","Loss: 0.625 | Acc: 67.708% \n","Loss: 0.627 | Acc: 67.434% \n","Loss: 0.623 | Acc: 68.438% \n","Loss: 0.625 | Acc: 68.006% \n","Loss: 0.625 | Acc: 68.040% \n","Loss: 0.622 | Acc: 68.750% \n","Loss: 0.619 | Acc: 69.271% \n","Loss: 0.619 | Acc: 68.750% \n","Loss: 0.620 | Acc: 68.149% \n","Loss: 0.623 | Acc: 67.940% \n","Loss: 0.622 | Acc: 68.192% \n","Loss: 0.621 | Acc: 68.211% \n","Loss: 0.622 | Acc: 68.021% \n","Loss: 0.623 | Acc: 67.944% \n","Loss: 0.621 | Acc: 67.969% \n","Loss: 0.620 | Acc: 68.277% \n","Loss: 0.621 | Acc: 68.199% \n","Loss: 0.620 | Acc: 68.214% \n","Loss: 0.620 | Acc: 68.056% \n","Loss: 0.619 | Acc: 68.243% \n","Loss: 0.618 | Acc: 68.339% \n","Loss: 0.618 | Acc: 68.269% \n","Loss: 0.618 | Acc: 68.594% \n","Loss: 0.618 | Acc: 68.902% \n","Loss: 0.619 | Acc: 68.676% \n","Loss: 0.620 | Acc: 68.459% \n","Loss: 0.619 | Acc: 68.466% \n","Loss: 0.619 | Acc: 68.403% \n","Loss: 0.618 | Acc: 68.478% \n","Loss: 0.619 | Acc: 68.351% \n","Loss: 0.619 | Acc: 68.229% \n","Loss: 0.619 | Acc: 68.367% \n","Loss: 0.618 | Acc: 68.312% \n","Loss: 0.619 | Acc: 68.015% \n","Loss: 0.620 | Acc: 67.849% \n","Loss: 0.620 | Acc: 67.630% \n","Loss: 0.620 | Acc: 67.708% \n","Loss: 0.620 | Acc: 67.670% \n","Loss: 0.620 | Acc: 67.634% \n","Loss: 0.620 | Acc: 67.489% \n","Loss: 0.620 | Acc: 67.672% \n","Loss: 0.620 | Acc: 67.797% \n","Loss: 0.620 | Acc: 67.812% \n","Loss: 0.620 | Acc: 67.725% \n","Loss: 0.620 | Acc: 67.591% \n","Loss: 0.620 | Acc: 67.758% \n","Loss: 0.620 | Acc: 67.725% \n","Loss: 0.620 | Acc: 67.740% \n","Loss: 0.621 | Acc: 67.756% \n","Loss: 0.621 | Acc: 67.677% \n","Loss: 0.622 | Acc: 67.555% \n","Loss: 0.622 | Acc: 67.482% \n","Loss: 0.621 | Acc: 67.768% \n","Loss: 0.620 | Acc: 67.958% \n","Loss: 0.619 | Acc: 68.012% \n","Loss: 0.618 | Acc: 68.065% \n","Loss: 0.618 | Acc: 68.243% \n","Loss: 0.619 | Acc: 68.083% \n","Loss: 0.619 | Acc: 68.092% \n","Loss: 0.618 | Acc: 68.101% \n","Loss: 0.618 | Acc: 68.189% \n","Loss: 0.618 | Acc: 68.275% \n","Loss: 0.616 | Acc: 68.438% \n","Loss: 0.617 | Acc: 68.364% \n","Loss: 0.618 | Acc: 68.293% \n","Loss: 0.618 | Acc: 68.110% \n","Loss: 0.618 | Acc: 68.266% \n","Loss: 0.618 | Acc: 68.311% \n","Accuracy of the network on the 1164 test images: 69 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 17%|█▋        | 5/30 [50:03<4:19:32, 622.89s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 5\n","Loss: 0.605 | Acc: 65.625% \n","Loss: 0.593 | Acc: 67.188% \n","Loss: 0.592 | Acc: 68.750% \n","Loss: 0.590 | Acc: 71.875% \n","Loss: 0.599 | Acc: 71.875% \n","Loss: 0.596 | Acc: 71.875% \n","Loss: 0.602 | Acc: 72.321% \n","Loss: 0.597 | Acc: 71.875% \n","Loss: 0.593 | Acc: 72.569% \n","Loss: 0.595 | Acc: 73.125% \n","Loss: 0.591 | Acc: 73.580% \n","Loss: 0.596 | Acc: 72.396% \n","Loss: 0.593 | Acc: 72.837% \n","Loss: 0.597 | Acc: 72.321% \n","Loss: 0.595 | Acc: 72.500% \n","Loss: 0.595 | Acc: 72.266% \n","Loss: 0.596 | Acc: 71.875% \n","Loss: 0.596 | Acc: 71.875% \n","Loss: 0.593 | Acc: 72.039% \n","Loss: 0.596 | Acc: 71.406% \n","Loss: 0.597 | Acc: 71.577% \n","Loss: 0.598 | Acc: 71.165% \n","Loss: 0.594 | Acc: 71.739% \n","Loss: 0.598 | Acc: 71.484% \n","Loss: 0.600 | Acc: 71.250% \n","Loss: 0.598 | Acc: 71.274% \n","Loss: 0.597 | Acc: 71.528% \n","Loss: 0.596 | Acc: 71.429% \n","Loss: 0.596 | Acc: 71.659% \n","Loss: 0.598 | Acc: 71.146% \n","Loss: 0.598 | Acc: 71.069% \n","Loss: 0.598 | Acc: 70.898% \n","Loss: 0.600 | Acc: 70.455% \n","Loss: 0.601 | Acc: 70.404% \n","Loss: 0.599 | Acc: 70.893% \n","Loss: 0.600 | Acc: 70.399% \n","Loss: 0.600 | Acc: 70.101% \n","Loss: 0.598 | Acc: 70.477% \n","Loss: 0.599 | Acc: 70.032% \n","Loss: 0.599 | Acc: 69.922% \n","Loss: 0.599 | Acc: 69.893% \n","Loss: 0.599 | Acc: 70.089% \n","Loss: 0.598 | Acc: 70.131% \n","Loss: 0.598 | Acc: 70.170% \n","Loss: 0.597 | Acc: 70.208% \n","Loss: 0.599 | Acc: 69.973% \n","Loss: 0.600 | Acc: 69.814% \n","Loss: 0.599 | Acc: 69.987% \n","Loss: 0.600 | Acc: 69.962% \n","Loss: 0.600 | Acc: 70.000% \n","Loss: 0.599 | Acc: 69.975% \n","Loss: 0.599 | Acc: 69.892% \n","Loss: 0.598 | Acc: 69.929% \n","Loss: 0.598 | Acc: 69.965% \n","Loss: 0.598 | Acc: 70.057% \n","Loss: 0.598 | Acc: 70.089% \n","Loss: 0.597 | Acc: 70.121% \n","Loss: 0.596 | Acc: 70.312% \n","Loss: 0.596 | Acc: 70.498% \n","Loss: 0.597 | Acc: 70.365% \n","Loss: 0.597 | Acc: 70.184% \n","Loss: 0.597 | Acc: 70.212% \n","Loss: 0.597 | Acc: 70.238% \n","Loss: 0.597 | Acc: 69.971% \n","Loss: 0.596 | Acc: 70.096% \n","Loss: 0.597 | Acc: 70.076% \n","Loss: 0.597 | Acc: 70.056% \n","Loss: 0.597 | Acc: 69.991% \n","Loss: 0.598 | Acc: 69.701% \n","Loss: 0.598 | Acc: 69.777% \n","Loss: 0.599 | Acc: 69.718% \n","Loss: 0.599 | Acc: 69.705% \n","Loss: 0.598 | Acc: 69.863% \n","Loss: 0.599 | Acc: 69.721% \n","Loss: 0.599 | Acc: 69.750% \n","Loss: 0.598 | Acc: 69.901% \n","Loss: 0.598 | Acc: 69.846% \n","Loss: 0.598 | Acc: 69.712% \n","Loss: 0.598 | Acc: 69.739% \n","Loss: 0.598 | Acc: 69.688% \n","Loss: 0.599 | Acc: 69.522% \n","Loss: 0.599 | Acc: 69.665% \n","Loss: 0.598 | Acc: 69.804% \n","Loss: 0.598 | Acc: 69.903% \n","Loss: 0.597 | Acc: 70.077% \n","Accuracy of the network on the 1164 test images: 71 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 20%|██        | 6/30 [58:10<3:52:50, 582.10s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 6\n","Loss: 0.563 | Acc: 71.875% \n","Loss: 0.594 | Acc: 68.750% \n","Loss: 0.593 | Acc: 71.875% \n","Loss: 0.586 | Acc: 72.656% \n","Loss: 0.606 | Acc: 70.000% \n","Loss: 0.604 | Acc: 70.833% \n","Loss: 0.604 | Acc: 70.982% \n","Loss: 0.601 | Acc: 71.875% \n","Loss: 0.592 | Acc: 72.917% \n","Loss: 0.589 | Acc: 73.750% \n","Loss: 0.587 | Acc: 73.295% \n","Loss: 0.588 | Acc: 73.958% \n","Loss: 0.587 | Acc: 74.279% \n","Loss: 0.589 | Acc: 73.661% \n","Loss: 0.589 | Acc: 73.542% \n","Loss: 0.585 | Acc: 73.828% \n","Loss: 0.592 | Acc: 72.794% \n","Loss: 0.594 | Acc: 72.569% \n","Loss: 0.592 | Acc: 73.026% \n","Loss: 0.591 | Acc: 73.281% \n","Loss: 0.593 | Acc: 73.065% \n","Loss: 0.591 | Acc: 73.438% \n","Loss: 0.594 | Acc: 72.826% \n","Loss: 0.593 | Acc: 72.786% \n","Loss: 0.593 | Acc: 72.625% \n","Loss: 0.591 | Acc: 72.837% \n","Loss: 0.589 | Acc: 72.801% \n","Loss: 0.588 | Acc: 73.103% \n","Loss: 0.586 | Acc: 73.276% \n","Loss: 0.585 | Acc: 73.333% \n","Loss: 0.584 | Acc: 73.387% \n","Loss: 0.586 | Acc: 73.145% \n","Loss: 0.585 | Acc: 73.390% \n","Loss: 0.586 | Acc: 73.346% \n","Loss: 0.586 | Acc: 73.125% \n","Loss: 0.588 | Acc: 72.830% \n","Loss: 0.587 | Acc: 72.889% \n","Loss: 0.591 | Acc: 72.204% \n","Loss: 0.590 | Acc: 72.196% \n","Loss: 0.591 | Acc: 71.953% \n","Loss: 0.591 | Acc: 71.799% \n","Loss: 0.591 | Acc: 71.801% \n","Loss: 0.590 | Acc: 72.020% \n","Loss: 0.592 | Acc: 71.662% \n","Loss: 0.591 | Acc: 71.736% \n","Loss: 0.591 | Acc: 71.739% \n","Loss: 0.590 | Acc: 71.875% \n","Loss: 0.590 | Acc: 72.005% \n","Loss: 0.590 | Acc: 72.194% \n","Loss: 0.589 | Acc: 72.375% \n","Loss: 0.590 | Acc: 72.243% \n","Loss: 0.590 | Acc: 72.236% \n","Loss: 0.588 | Acc: 72.347% \n","Loss: 0.589 | Acc: 72.164% \n","Loss: 0.589 | Acc: 72.216% \n","Loss: 0.588 | Acc: 72.210% \n","Loss: 0.589 | Acc: 72.094% \n","Loss: 0.589 | Acc: 72.091% \n","Loss: 0.590 | Acc: 71.981% \n","Loss: 0.589 | Acc: 72.135% \n","Loss: 0.590 | Acc: 72.080% \n","Loss: 0.590 | Acc: 71.976% \n","Loss: 0.590 | Acc: 72.024% \n","Loss: 0.590 | Acc: 72.119% \n","Loss: 0.589 | Acc: 72.260% \n","Loss: 0.589 | Acc: 72.348% \n","Loss: 0.590 | Acc: 72.062% \n","Loss: 0.589 | Acc: 72.289% \n","Loss: 0.588 | Acc: 72.464% \n","Loss: 0.588 | Acc: 72.500% \n","Loss: 0.587 | Acc: 72.579% \n","Loss: 0.587 | Acc: 72.569% \n","Loss: 0.586 | Acc: 72.774% \n","Loss: 0.586 | Acc: 72.804% \n","Loss: 0.586 | Acc: 72.833% \n","Loss: 0.586 | Acc: 72.821% \n","Loss: 0.586 | Acc: 72.971% \n","Loss: 0.585 | Acc: 73.037% \n","Loss: 0.585 | Acc: 73.141% \n","Loss: 0.585 | Acc: 73.008% \n","Loss: 0.585 | Acc: 73.148% \n","Loss: 0.584 | Acc: 73.133% \n","Loss: 0.584 | Acc: 73.117% \n","Loss: 0.584 | Acc: 73.140% \n","Loss: 0.585 | Acc: 72.985% \n","Accuracy of the network on the 1164 test images: 73 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 23%|██▎       | 7/30 [1:06:18<3:32:23, 554.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 7\n","Loss: 0.628 | Acc: 62.500% \n","Loss: 0.602 | Acc: 68.750% \n","Loss: 0.602 | Acc: 68.750% \n","Loss: 0.593 | Acc: 71.094% \n","Loss: 0.593 | Acc: 71.875% \n","Loss: 0.592 | Acc: 70.833% \n","Loss: 0.606 | Acc: 67.857% \n","Loss: 0.607 | Acc: 67.188% \n","Loss: 0.602 | Acc: 67.014% \n","Loss: 0.594 | Acc: 68.750% \n","Loss: 0.595 | Acc: 68.750% \n","Loss: 0.597 | Acc: 68.490% \n","Loss: 0.589 | Acc: 70.192% \n","Loss: 0.587 | Acc: 70.759% \n","Loss: 0.587 | Acc: 70.833% \n","Loss: 0.591 | Acc: 70.312% \n","Loss: 0.590 | Acc: 70.221% \n","Loss: 0.592 | Acc: 69.792% \n","Loss: 0.594 | Acc: 69.079% \n","Loss: 0.592 | Acc: 69.375% \n","Loss: 0.590 | Acc: 69.792% \n","Loss: 0.588 | Acc: 70.455% \n","Loss: 0.587 | Acc: 70.924% \n","Loss: 0.587 | Acc: 71.224% \n","Loss: 0.588 | Acc: 71.250% \n","Loss: 0.586 | Acc: 71.514% \n","Loss: 0.585 | Acc: 71.644% \n","Loss: 0.583 | Acc: 72.098% \n","Loss: 0.583 | Acc: 72.198% \n","Loss: 0.582 | Acc: 72.396% \n","Loss: 0.582 | Acc: 72.177% \n","Loss: 0.582 | Acc: 72.070% \n","Loss: 0.584 | Acc: 71.780% \n","Loss: 0.585 | Acc: 71.783% \n","Loss: 0.584 | Acc: 71.786% \n","Loss: 0.583 | Acc: 72.049% \n","Loss: 0.582 | Acc: 71.959% \n","Loss: 0.583 | Acc: 71.957% \n","Loss: 0.581 | Acc: 72.356% \n","Loss: 0.580 | Acc: 72.422% \n","Loss: 0.581 | Acc: 72.485% \n","Loss: 0.579 | Acc: 72.619% \n","Loss: 0.579 | Acc: 72.820% \n","Loss: 0.580 | Acc: 72.656% \n","Loss: 0.579 | Acc: 72.778% \n","Loss: 0.579 | Acc: 72.826% \n","Loss: 0.579 | Acc: 72.540% \n","Loss: 0.580 | Acc: 72.526% \n","Loss: 0.579 | Acc: 72.704% \n","Loss: 0.581 | Acc: 72.500% \n","Loss: 0.579 | Acc: 72.855% \n","Loss: 0.579 | Acc: 72.776% \n","Loss: 0.578 | Acc: 72.877% \n","Loss: 0.578 | Acc: 72.975% \n","Loss: 0.577 | Acc: 73.068% \n","Loss: 0.577 | Acc: 73.158% \n","Loss: 0.577 | Acc: 73.191% \n","Loss: 0.576 | Acc: 73.222% \n","Loss: 0.575 | Acc: 73.464% \n","Loss: 0.575 | Acc: 73.281% \n","Loss: 0.575 | Acc: 73.309% \n","Loss: 0.576 | Acc: 73.185% \n","Loss: 0.576 | Acc: 73.115% \n","Loss: 0.576 | Acc: 73.096% \n","Loss: 0.575 | Acc: 73.173% \n","Loss: 0.575 | Acc: 73.248% \n","Loss: 0.574 | Acc: 73.368% \n","Loss: 0.574 | Acc: 73.483% \n","Loss: 0.575 | Acc: 73.460% \n","Loss: 0.575 | Acc: 73.304% \n","Loss: 0.575 | Acc: 73.371% \n","Loss: 0.575 | Acc: 73.394% \n","Loss: 0.575 | Acc: 73.416% \n","Loss: 0.575 | Acc: 73.438% \n","Loss: 0.574 | Acc: 73.458% \n","Loss: 0.574 | Acc: 73.602% \n","Loss: 0.574 | Acc: 73.661% \n","Loss: 0.574 | Acc: 73.598% \n","Loss: 0.574 | Acc: 73.616% \n","Loss: 0.574 | Acc: 73.555% \n","Loss: 0.575 | Acc: 73.495% \n","Loss: 0.574 | Acc: 73.514% \n","Loss: 0.574 | Acc: 73.607% \n","Loss: 0.574 | Acc: 73.661% \n","Loss: 0.574 | Acc: 73.537% \n","Accuracy of the network on the 1164 test images: 73 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 27%|██▋       | 8/30 [1:14:27<3:15:58, 534.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 8\n","Loss: 0.543 | Acc: 78.125% \n","Loss: 0.573 | Acc: 73.438% \n","Loss: 0.578 | Acc: 69.792% \n","Loss: 0.574 | Acc: 69.531% \n","Loss: 0.566 | Acc: 71.875% \n","Loss: 0.568 | Acc: 71.875% \n","Loss: 0.561 | Acc: 73.661% \n","Loss: 0.572 | Acc: 72.266% \n","Loss: 0.570 | Acc: 72.917% \n","Loss: 0.572 | Acc: 73.438% \n","Loss: 0.575 | Acc: 73.580% \n","Loss: 0.569 | Acc: 74.219% \n","Loss: 0.566 | Acc: 74.279% \n","Loss: 0.563 | Acc: 74.554% \n","Loss: 0.562 | Acc: 74.792% \n","Loss: 0.563 | Acc: 74.805% \n","Loss: 0.562 | Acc: 75.184% \n","Loss: 0.562 | Acc: 75.174% \n","Loss: 0.562 | Acc: 75.329% \n","Loss: 0.559 | Acc: 75.625% \n","Loss: 0.559 | Acc: 75.446% \n","Loss: 0.557 | Acc: 75.852% \n","Loss: 0.559 | Acc: 75.272% \n","Loss: 0.560 | Acc: 75.000% \n","Loss: 0.558 | Acc: 75.125% \n","Loss: 0.558 | Acc: 75.120% \n","Loss: 0.558 | Acc: 75.116% \n","Loss: 0.559 | Acc: 74.554% \n","Loss: 0.559 | Acc: 74.784% \n","Loss: 0.561 | Acc: 74.688% \n","Loss: 0.562 | Acc: 74.496% \n","Loss: 0.563 | Acc: 74.219% \n","Loss: 0.565 | Acc: 74.148% \n","Loss: 0.566 | Acc: 74.173% \n","Loss: 0.566 | Acc: 74.107% \n","Loss: 0.565 | Acc: 74.219% \n","Loss: 0.565 | Acc: 73.902% \n","Loss: 0.567 | Acc: 73.684% \n","Loss: 0.565 | Acc: 73.718% \n","Loss: 0.564 | Acc: 73.828% \n","Loss: 0.565 | Acc: 73.857% \n","Loss: 0.565 | Acc: 73.661% \n","Loss: 0.563 | Acc: 73.983% \n","Loss: 0.563 | Acc: 74.077% \n","Loss: 0.563 | Acc: 74.097% \n","Loss: 0.563 | Acc: 74.185% \n","Loss: 0.562 | Acc: 74.335% \n","Loss: 0.563 | Acc: 74.154% \n","Loss: 0.564 | Acc: 73.980% \n","Loss: 0.565 | Acc: 73.812% \n","Loss: 0.566 | Acc: 73.652% \n","Loss: 0.567 | Acc: 73.558% \n","Loss: 0.568 | Acc: 73.526% \n","Loss: 0.566 | Acc: 73.727% \n","Loss: 0.566 | Acc: 73.693% \n","Loss: 0.566 | Acc: 73.772% \n","Loss: 0.566 | Acc: 73.958% \n","Loss: 0.565 | Acc: 74.030% \n","Loss: 0.565 | Acc: 74.047% \n","Loss: 0.565 | Acc: 74.062% \n","Loss: 0.565 | Acc: 74.078% \n","Loss: 0.565 | Acc: 74.042% \n","Loss: 0.564 | Acc: 74.157% \n","Loss: 0.563 | Acc: 74.268% \n","Loss: 0.563 | Acc: 74.279% \n","Loss: 0.563 | Acc: 74.290% \n","Loss: 0.563 | Acc: 74.347% \n","Loss: 0.563 | Acc: 74.311% \n","Loss: 0.562 | Acc: 74.275% \n","Loss: 0.562 | Acc: 74.241% \n","Loss: 0.561 | Acc: 74.384% \n","Loss: 0.561 | Acc: 74.523% \n","Loss: 0.561 | Acc: 74.615% \n","Loss: 0.561 | Acc: 74.662% \n","Loss: 0.560 | Acc: 74.708% \n","Loss: 0.560 | Acc: 74.836% \n","Loss: 0.559 | Acc: 74.838% \n","Loss: 0.559 | Acc: 74.880% \n","Loss: 0.560 | Acc: 74.644% \n","Loss: 0.560 | Acc: 74.609% \n","Loss: 0.562 | Acc: 74.344% \n","Loss: 0.562 | Acc: 74.314% \n","Loss: 0.562 | Acc: 74.473% \n","Loss: 0.562 | Acc: 74.442% \n","Loss: 0.562 | Acc: 74.531% \n","Accuracy of the network on the 1164 test images: 75 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 30%|███       | 9/30 [1:22:39<3:02:36, 521.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 9\n","Loss: 0.570 | Acc: 78.125% \n","Loss: 0.555 | Acc: 78.125% \n","Loss: 0.591 | Acc: 68.750% \n","Loss: 0.603 | Acc: 68.750% \n","Loss: 0.591 | Acc: 70.000% \n","Loss: 0.595 | Acc: 69.792% \n","Loss: 0.588 | Acc: 70.536% \n","Loss: 0.586 | Acc: 69.531% \n","Loss: 0.581 | Acc: 70.833% \n","Loss: 0.579 | Acc: 70.312% \n","Loss: 0.580 | Acc: 70.170% \n","Loss: 0.575 | Acc: 71.354% \n","Loss: 0.574 | Acc: 71.875% \n","Loss: 0.576 | Acc: 72.098% \n","Loss: 0.574 | Acc: 72.500% \n","Loss: 0.572 | Acc: 73.047% \n","Loss: 0.572 | Acc: 72.794% \n","Loss: 0.571 | Acc: 72.396% \n","Loss: 0.569 | Acc: 72.533% \n","Loss: 0.564 | Acc: 72.969% \n","Loss: 0.564 | Acc: 73.065% \n","Loss: 0.565 | Acc: 73.153% \n","Loss: 0.562 | Acc: 73.370% \n","Loss: 0.560 | Acc: 73.828% \n","Loss: 0.560 | Acc: 73.500% \n","Loss: 0.560 | Acc: 73.678% \n","Loss: 0.560 | Acc: 73.611% \n","Loss: 0.561 | Acc: 73.438% \n","Loss: 0.562 | Acc: 73.168% \n","Loss: 0.562 | Acc: 73.229% \n","Loss: 0.559 | Acc: 73.891% \n","Loss: 0.558 | Acc: 73.926% \n","Loss: 0.560 | Acc: 73.864% \n","Loss: 0.558 | Acc: 74.081% \n","Loss: 0.557 | Acc: 74.554% \n","Loss: 0.557 | Acc: 74.653% \n","Loss: 0.555 | Acc: 74.831% \n","Loss: 0.553 | Acc: 75.164% \n","Loss: 0.554 | Acc: 75.080% \n","Loss: 0.555 | Acc: 75.156% \n","Loss: 0.555 | Acc: 75.076% \n","Loss: 0.556 | Acc: 74.926% \n","Loss: 0.555 | Acc: 75.000% \n","Loss: 0.556 | Acc: 74.929% \n","Loss: 0.555 | Acc: 75.139% \n","Loss: 0.554 | Acc: 75.340% \n","Loss: 0.554 | Acc: 75.133% \n","Loss: 0.554 | Acc: 75.326% \n","Loss: 0.554 | Acc: 75.383% \n","Loss: 0.554 | Acc: 75.312% \n","Loss: 0.555 | Acc: 75.245% \n","Loss: 0.555 | Acc: 75.481% \n","Loss: 0.554 | Acc: 75.472% \n","Loss: 0.555 | Acc: 75.405% \n","Loss: 0.556 | Acc: 75.227% \n","Loss: 0.555 | Acc: 75.335% \n","Loss: 0.554 | Acc: 75.713% \n","Loss: 0.555 | Acc: 75.377% \n","Loss: 0.554 | Acc: 75.530% \n","Loss: 0.552 | Acc: 75.833% \n","Loss: 0.552 | Acc: 75.922% \n","Loss: 0.552 | Acc: 76.058% \n","Loss: 0.551 | Acc: 76.190% \n","Loss: 0.551 | Acc: 76.172% \n","Loss: 0.552 | Acc: 75.962% \n","Loss: 0.551 | Acc: 75.947% \n","Loss: 0.551 | Acc: 75.933% \n","Loss: 0.550 | Acc: 75.965% \n","Loss: 0.549 | Acc: 76.087% \n","Loss: 0.549 | Acc: 76.071% \n","Loss: 0.549 | Acc: 75.968% \n","Loss: 0.550 | Acc: 75.868% \n","Loss: 0.549 | Acc: 75.942% \n","Loss: 0.550 | Acc: 75.887% \n","Loss: 0.550 | Acc: 75.833% \n","Loss: 0.550 | Acc: 75.822% \n","Loss: 0.549 | Acc: 75.812% \n","Loss: 0.551 | Acc: 75.641% \n","Loss: 0.551 | Acc: 75.752% \n","Loss: 0.550 | Acc: 75.859% \n","Loss: 0.550 | Acc: 75.810% \n","Loss: 0.550 | Acc: 75.724% \n","Loss: 0.550 | Acc: 75.489% \n","Loss: 0.551 | Acc: 75.409% \n","Loss: 0.551 | Acc: 75.451% \n","Accuracy of the network on the 1164 test images: 75 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 33%|███▎      | 10/30 [1:30:52<2:50:58, 512.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 10\n","Loss: 0.621 | Acc: 65.625% \n","Loss: 0.578 | Acc: 70.312% \n","Loss: 0.577 | Acc: 70.833% \n","Loss: 0.569 | Acc: 71.875% \n","Loss: 0.563 | Acc: 73.125% \n","Loss: 0.552 | Acc: 74.479% \n","Loss: 0.544 | Acc: 76.786% \n","Loss: 0.547 | Acc: 75.781% \n","Loss: 0.555 | Acc: 74.306% \n","Loss: 0.558 | Acc: 74.375% \n","Loss: 0.562 | Acc: 73.011% \n","Loss: 0.562 | Acc: 72.917% \n","Loss: 0.561 | Acc: 72.837% \n","Loss: 0.561 | Acc: 72.545% \n","Loss: 0.558 | Acc: 73.125% \n","Loss: 0.557 | Acc: 73.633% \n","Loss: 0.558 | Acc: 73.713% \n","Loss: 0.561 | Acc: 73.090% \n","Loss: 0.560 | Acc: 73.355% \n","Loss: 0.556 | Acc: 73.750% \n","Loss: 0.556 | Acc: 73.958% \n","Loss: 0.553 | Acc: 74.290% \n","Loss: 0.552 | Acc: 74.592% \n","Loss: 0.551 | Acc: 74.479% \n","Loss: 0.548 | Acc: 74.625% \n","Loss: 0.548 | Acc: 74.639% \n","Loss: 0.546 | Acc: 74.769% \n","Loss: 0.547 | Acc: 74.554% \n","Loss: 0.548 | Acc: 74.246% \n","Loss: 0.545 | Acc: 74.688% \n","Loss: 0.546 | Acc: 74.899% \n","Loss: 0.546 | Acc: 74.902% \n","Loss: 0.545 | Acc: 75.189% \n","Loss: 0.544 | Acc: 75.092% \n","Loss: 0.546 | Acc: 74.911% \n","Loss: 0.547 | Acc: 74.740% \n","Loss: 0.547 | Acc: 74.747% \n","Loss: 0.549 | Acc: 74.424% \n","Loss: 0.549 | Acc: 74.439% \n","Loss: 0.549 | Acc: 74.375% \n","Loss: 0.550 | Acc: 74.238% \n","Loss: 0.551 | Acc: 74.182% \n","Loss: 0.550 | Acc: 74.419% \n","Loss: 0.550 | Acc: 74.361% \n","Loss: 0.549 | Acc: 74.375% \n","Loss: 0.550 | Acc: 74.389% \n","Loss: 0.550 | Acc: 74.335% \n","Loss: 0.551 | Acc: 74.219% \n","Loss: 0.551 | Acc: 74.235% \n","Loss: 0.550 | Acc: 74.312% \n","Loss: 0.549 | Acc: 74.326% \n","Loss: 0.549 | Acc: 74.279% \n","Loss: 0.549 | Acc: 74.175% \n","Loss: 0.549 | Acc: 74.248% \n","Loss: 0.550 | Acc: 74.148% \n","Loss: 0.550 | Acc: 74.051% \n","Loss: 0.552 | Acc: 73.904% \n","Loss: 0.553 | Acc: 73.653% \n","Loss: 0.552 | Acc: 73.729% \n","Loss: 0.552 | Acc: 73.854% \n","Loss: 0.552 | Acc: 73.924% \n","Loss: 0.552 | Acc: 73.992% \n","Loss: 0.550 | Acc: 74.206% \n","Loss: 0.550 | Acc: 74.268% \n","Loss: 0.549 | Acc: 74.471% \n","Loss: 0.547 | Acc: 74.716% \n","Loss: 0.547 | Acc: 74.860% \n","Loss: 0.548 | Acc: 74.770% \n","Loss: 0.549 | Acc: 74.502% \n","Loss: 0.549 | Acc: 74.554% \n","Loss: 0.548 | Acc: 74.692% \n","Loss: 0.547 | Acc: 74.870% \n","Loss: 0.547 | Acc: 75.043% \n","Loss: 0.547 | Acc: 75.084% \n","Loss: 0.547 | Acc: 75.042% \n","Loss: 0.546 | Acc: 75.123% \n","Loss: 0.546 | Acc: 75.122% \n","Loss: 0.546 | Acc: 75.120% \n","Loss: 0.546 | Acc: 75.198% \n","Loss: 0.546 | Acc: 75.195% \n","Loss: 0.547 | Acc: 75.077% \n","Loss: 0.546 | Acc: 75.152% \n","Loss: 0.547 | Acc: 75.000% \n","Loss: 0.547 | Acc: 75.037% \n","Loss: 0.548 | Acc: 74.936% \n","Accuracy of the network on the 1164 test images: 75 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 37%|███▋      | 11/30 [1:39:00<2:40:05, 505.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 11\n","Loss: 0.523 | Acc: 81.250% \n","Loss: 0.529 | Acc: 73.438% \n","Loss: 0.541 | Acc: 76.042% \n","Loss: 0.536 | Acc: 77.344% \n","Loss: 0.525 | Acc: 78.125% \n","Loss: 0.526 | Acc: 76.562% \n","Loss: 0.528 | Acc: 75.446% \n","Loss: 0.527 | Acc: 75.391% \n","Loss: 0.526 | Acc: 76.042% \n","Loss: 0.533 | Acc: 75.000% \n","Loss: 0.529 | Acc: 75.568% \n","Loss: 0.529 | Acc: 75.781% \n","Loss: 0.531 | Acc: 75.721% \n","Loss: 0.536 | Acc: 74.777% \n","Loss: 0.533 | Acc: 75.000% \n","Loss: 0.535 | Acc: 75.000% \n","Loss: 0.531 | Acc: 75.735% \n","Loss: 0.533 | Acc: 75.868% \n","Loss: 0.535 | Acc: 75.658% \n","Loss: 0.533 | Acc: 76.094% \n","Loss: 0.538 | Acc: 75.446% \n","Loss: 0.537 | Acc: 75.426% \n","Loss: 0.534 | Acc: 75.951% \n","Loss: 0.534 | Acc: 75.781% \n","Loss: 0.534 | Acc: 75.750% \n","Loss: 0.534 | Acc: 75.841% \n","Loss: 0.536 | Acc: 75.579% \n","Loss: 0.536 | Acc: 75.223% \n","Loss: 0.539 | Acc: 74.892% \n","Loss: 0.536 | Acc: 75.312% \n","Loss: 0.536 | Acc: 75.202% \n","Loss: 0.536 | Acc: 75.000% \n","Loss: 0.535 | Acc: 75.000% \n","Loss: 0.536 | Acc: 75.368% \n","Loss: 0.538 | Acc: 75.089% \n","Loss: 0.540 | Acc: 75.087% \n","Loss: 0.540 | Acc: 75.084% \n","Loss: 0.539 | Acc: 75.247% \n","Loss: 0.540 | Acc: 75.080% \n","Loss: 0.539 | Acc: 75.312% \n","Loss: 0.540 | Acc: 75.076% \n","Loss: 0.539 | Acc: 75.223% \n","Loss: 0.540 | Acc: 74.927% \n","Loss: 0.540 | Acc: 75.071% \n","Loss: 0.539 | Acc: 75.069% \n","Loss: 0.539 | Acc: 75.204% \n","Loss: 0.541 | Acc: 75.133% \n","Loss: 0.539 | Acc: 75.521% \n","Loss: 0.540 | Acc: 75.510% \n","Loss: 0.539 | Acc: 75.688% \n","Loss: 0.538 | Acc: 75.980% \n","Loss: 0.538 | Acc: 76.082% \n","Loss: 0.539 | Acc: 75.767% \n","Loss: 0.541 | Acc: 75.405% \n","Loss: 0.541 | Acc: 75.455% \n","Loss: 0.541 | Acc: 75.558% \n","Loss: 0.541 | Acc: 75.713% \n","Loss: 0.542 | Acc: 75.539% \n","Loss: 0.543 | Acc: 75.371% \n","Loss: 0.542 | Acc: 75.365% \n","Loss: 0.543 | Acc: 75.307% \n","Loss: 0.542 | Acc: 75.454% \n","Loss: 0.542 | Acc: 75.546% \n","Loss: 0.542 | Acc: 75.439% \n","Loss: 0.543 | Acc: 75.288% \n","Loss: 0.544 | Acc: 75.331% \n","Loss: 0.543 | Acc: 75.420% \n","Loss: 0.544 | Acc: 75.184% \n","Loss: 0.543 | Acc: 75.317% \n","Loss: 0.543 | Acc: 75.402% \n","Loss: 0.543 | Acc: 75.396% \n","Loss: 0.542 | Acc: 75.521% \n","Loss: 0.541 | Acc: 75.685% \n","Loss: 0.541 | Acc: 75.802% \n","Loss: 0.541 | Acc: 75.833% \n","Loss: 0.540 | Acc: 75.987% \n","Loss: 0.540 | Acc: 76.015% \n","Loss: 0.542 | Acc: 75.881% \n","Loss: 0.541 | Acc: 75.910% \n","Loss: 0.541 | Acc: 75.898% \n","Loss: 0.541 | Acc: 75.926% \n","Loss: 0.541 | Acc: 75.877% \n","Loss: 0.541 | Acc: 75.828% \n","Loss: 0.541 | Acc: 75.930% \n","Loss: 0.540 | Acc: 75.929% \n","Accuracy of the network on the 1164 test images: 77 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 40%|████      | 12/30 [1:47:06<2:29:53, 499.62s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 12\n","Loss: 0.523 | Acc: 68.750% \n","Loss: 0.529 | Acc: 73.438% \n","Loss: 0.528 | Acc: 75.000% \n","Loss: 0.545 | Acc: 73.438% \n","Loss: 0.542 | Acc: 74.375% \n","Loss: 0.537 | Acc: 76.042% \n","Loss: 0.544 | Acc: 73.661% \n","Loss: 0.538 | Acc: 75.391% \n","Loss: 0.534 | Acc: 76.042% \n","Loss: 0.535 | Acc: 76.250% \n","Loss: 0.529 | Acc: 76.989% \n","Loss: 0.526 | Acc: 77.604% \n","Loss: 0.525 | Acc: 77.885% \n","Loss: 0.529 | Acc: 77.679% \n","Loss: 0.528 | Acc: 77.292% \n","Loss: 0.526 | Acc: 77.734% \n","Loss: 0.528 | Acc: 77.390% \n","Loss: 0.529 | Acc: 77.257% \n","Loss: 0.532 | Acc: 77.138% \n","Loss: 0.531 | Acc: 77.031% \n","Loss: 0.533 | Acc: 76.488% \n","Loss: 0.532 | Acc: 76.562% \n","Loss: 0.530 | Acc: 76.766% \n","Loss: 0.529 | Acc: 77.083% \n","Loss: 0.531 | Acc: 76.750% \n","Loss: 0.530 | Acc: 76.923% \n","Loss: 0.534 | Acc: 76.505% \n","Loss: 0.533 | Acc: 76.562% \n","Loss: 0.532 | Acc: 76.616% \n","Loss: 0.531 | Acc: 76.875% \n","Loss: 0.534 | Acc: 76.613% \n","Loss: 0.531 | Acc: 77.051% \n","Loss: 0.533 | Acc: 77.178% \n","Loss: 0.532 | Acc: 77.482% \n","Loss: 0.531 | Acc: 77.589% \n","Loss: 0.531 | Acc: 77.604% \n","Loss: 0.532 | Acc: 77.618% \n","Loss: 0.534 | Acc: 77.220% \n","Loss: 0.533 | Acc: 77.324% \n","Loss: 0.532 | Acc: 77.266% \n","Loss: 0.530 | Acc: 77.515% \n","Loss: 0.531 | Acc: 77.530% \n","Loss: 0.531 | Acc: 77.471% \n","Loss: 0.531 | Acc: 77.344% \n","Loss: 0.532 | Acc: 77.153% \n","Loss: 0.531 | Acc: 77.174% \n","Loss: 0.530 | Acc: 77.327% \n","Loss: 0.529 | Acc: 77.279% \n","Loss: 0.530 | Acc: 77.296% \n","Loss: 0.529 | Acc: 77.250% \n","Loss: 0.528 | Acc: 77.328% \n","Loss: 0.529 | Acc: 77.404% \n","Loss: 0.529 | Acc: 77.417% \n","Loss: 0.528 | Acc: 77.546% \n","Loss: 0.528 | Acc: 77.500% \n","Loss: 0.529 | Acc: 77.344% \n","Loss: 0.529 | Acc: 77.303% \n","Loss: 0.529 | Acc: 77.155% \n","Loss: 0.530 | Acc: 77.013% \n","Loss: 0.530 | Acc: 76.875% \n","Loss: 0.530 | Acc: 76.998% \n","Loss: 0.530 | Acc: 76.966% \n","Loss: 0.531 | Acc: 76.885% \n","Loss: 0.531 | Acc: 76.953% \n","Loss: 0.532 | Acc: 76.923% \n","Loss: 0.532 | Acc: 76.941% \n","Loss: 0.531 | Acc: 77.099% \n","Loss: 0.531 | Acc: 77.114% \n","Loss: 0.531 | Acc: 77.219% \n","Loss: 0.531 | Acc: 77.188% \n","Loss: 0.531 | Acc: 77.201% \n","Loss: 0.531 | Acc: 77.214% \n","Loss: 0.531 | Acc: 77.354% \n","Loss: 0.531 | Acc: 77.238% \n","Loss: 0.531 | Acc: 77.042% \n","Loss: 0.531 | Acc: 77.097% \n","Loss: 0.532 | Acc: 76.907% \n","Loss: 0.531 | Acc: 76.963% \n","Loss: 0.532 | Acc: 76.820% \n","Loss: 0.533 | Acc: 76.758% \n","Loss: 0.534 | Acc: 76.659% \n","Loss: 0.533 | Acc: 76.677% \n","Loss: 0.534 | Acc: 76.581% \n","Loss: 0.533 | Acc: 76.674% \n","Loss: 0.533 | Acc: 76.592% \n","Accuracy of the network on the 1164 test images: 77 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 43%|████▎     | 13/30 [1:55:16<2:20:45, 496.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 13\n","Loss: 0.431 | Acc: 90.625% \n","Loss: 0.485 | Acc: 82.812% \n","Loss: 0.490 | Acc: 82.292% \n","Loss: 0.512 | Acc: 79.688% \n","Loss: 0.510 | Acc: 80.000% \n","Loss: 0.536 | Acc: 76.562% \n","Loss: 0.531 | Acc: 76.786% \n","Loss: 0.534 | Acc: 76.562% \n","Loss: 0.526 | Acc: 77.431% \n","Loss: 0.524 | Acc: 77.188% \n","Loss: 0.521 | Acc: 77.557% \n","Loss: 0.518 | Acc: 77.865% \n","Loss: 0.516 | Acc: 77.885% \n","Loss: 0.521 | Acc: 77.679% \n","Loss: 0.520 | Acc: 78.125% \n","Loss: 0.515 | Acc: 78.906% \n","Loss: 0.519 | Acc: 78.676% \n","Loss: 0.521 | Acc: 78.646% \n","Loss: 0.518 | Acc: 78.783% \n","Loss: 0.517 | Acc: 79.062% \n","Loss: 0.516 | Acc: 79.167% \n","Loss: 0.515 | Acc: 79.403% \n","Loss: 0.514 | Acc: 79.755% \n","Loss: 0.515 | Acc: 79.427% \n","Loss: 0.517 | Acc: 79.000% \n","Loss: 0.515 | Acc: 79.447% \n","Loss: 0.517 | Acc: 79.282% \n","Loss: 0.516 | Acc: 79.464% \n","Loss: 0.516 | Acc: 79.203% \n","Loss: 0.517 | Acc: 79.062% \n","Loss: 0.520 | Acc: 78.931% \n","Loss: 0.520 | Acc: 79.004% \n","Loss: 0.518 | Acc: 79.167% \n","Loss: 0.520 | Acc: 79.136% \n","Loss: 0.519 | Acc: 79.107% \n","Loss: 0.518 | Acc: 79.340% \n","Loss: 0.517 | Acc: 79.476% \n","Loss: 0.517 | Acc: 79.523% \n","Loss: 0.516 | Acc: 79.567% \n","Loss: 0.516 | Acc: 79.453% \n","Loss: 0.515 | Acc: 79.421% \n","Loss: 0.514 | Acc: 79.688% \n","Loss: 0.513 | Acc: 79.797% \n","Loss: 0.514 | Acc: 79.759% \n","Loss: 0.513 | Acc: 79.722% \n","Loss: 0.512 | Acc: 79.823% \n","Loss: 0.513 | Acc: 79.787% \n","Loss: 0.513 | Acc: 79.688% \n","Loss: 0.514 | Acc: 79.592% \n","Loss: 0.513 | Acc: 79.812% \n","Loss: 0.512 | Acc: 79.841% \n","Loss: 0.513 | Acc: 79.627% \n","Loss: 0.513 | Acc: 79.717% \n","Loss: 0.511 | Acc: 79.861% \n","Loss: 0.512 | Acc: 79.773% \n","Loss: 0.512 | Acc: 79.688% \n","Loss: 0.512 | Acc: 79.770% \n","Loss: 0.512 | Acc: 79.795% \n","Loss: 0.512 | Acc: 79.820% \n","Loss: 0.511 | Acc: 79.844% \n","Loss: 0.511 | Acc: 79.764% \n","Loss: 0.511 | Acc: 79.688% \n","Loss: 0.511 | Acc: 79.613% \n","Loss: 0.512 | Acc: 79.492% \n","Loss: 0.512 | Acc: 79.519% \n","Loss: 0.513 | Acc: 79.356% \n","Loss: 0.513 | Acc: 79.291% \n","Loss: 0.512 | Acc: 79.412% \n","Loss: 0.513 | Acc: 79.393% \n","Loss: 0.512 | Acc: 79.509% \n","Loss: 0.512 | Acc: 79.401% \n","Loss: 0.514 | Acc: 79.297% \n","Loss: 0.514 | Acc: 79.324% \n","Loss: 0.513 | Acc: 79.434% \n","Loss: 0.514 | Acc: 79.292% \n","Loss: 0.515 | Acc: 79.153% \n","Loss: 0.515 | Acc: 79.099% \n","Loss: 0.515 | Acc: 79.006% \n","Loss: 0.516 | Acc: 78.956% \n","Loss: 0.515 | Acc: 79.062% \n","Loss: 0.516 | Acc: 79.090% \n","Loss: 0.515 | Acc: 79.116% \n","Loss: 0.515 | Acc: 79.104% \n","Loss: 0.515 | Acc: 79.204% \n","Loss: 0.515 | Acc: 79.168% \n","Accuracy of the network on the 1164 test images: 79 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 47%|████▋     | 14/30 [2:03:23<2:11:43, 493.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 14\n","Loss: 0.556 | Acc: 71.875% \n","Loss: 0.512 | Acc: 79.688% \n","Loss: 0.504 | Acc: 81.250% \n","Loss: 0.505 | Acc: 80.469% \n","Loss: 0.508 | Acc: 79.375% \n","Loss: 0.521 | Acc: 78.646% \n","Loss: 0.517 | Acc: 78.571% \n","Loss: 0.519 | Acc: 79.297% \n","Loss: 0.518 | Acc: 79.861% \n","Loss: 0.524 | Acc: 79.062% \n","Loss: 0.524 | Acc: 79.261% \n","Loss: 0.523 | Acc: 79.167% \n","Loss: 0.513 | Acc: 80.048% \n","Loss: 0.510 | Acc: 80.134% \n","Loss: 0.510 | Acc: 80.000% \n","Loss: 0.506 | Acc: 80.469% \n","Loss: 0.511 | Acc: 80.147% \n","Loss: 0.511 | Acc: 79.861% \n","Loss: 0.516 | Acc: 79.770% \n","Loss: 0.515 | Acc: 79.844% \n","Loss: 0.513 | Acc: 80.060% \n","Loss: 0.514 | Acc: 79.830% \n","Loss: 0.514 | Acc: 79.755% \n","Loss: 0.510 | Acc: 80.078% \n","Loss: 0.510 | Acc: 80.125% \n","Loss: 0.508 | Acc: 80.288% \n","Loss: 0.507 | Acc: 80.093% \n","Loss: 0.508 | Acc: 80.022% \n","Loss: 0.508 | Acc: 79.957% \n","Loss: 0.508 | Acc: 79.896% \n","Loss: 0.508 | Acc: 80.141% \n","Loss: 0.508 | Acc: 79.883% \n","Loss: 0.506 | Acc: 80.208% \n","Loss: 0.506 | Acc: 79.963% \n","Loss: 0.508 | Acc: 79.821% \n","Loss: 0.509 | Acc: 79.861% \n","Loss: 0.508 | Acc: 80.068% \n","Loss: 0.505 | Acc: 80.428% \n","Loss: 0.504 | Acc: 80.529% \n","Loss: 0.502 | Acc: 80.781% \n","Loss: 0.502 | Acc: 80.640% \n","Loss: 0.501 | Acc: 80.729% \n","Loss: 0.503 | Acc: 80.669% \n","Loss: 0.503 | Acc: 80.682% \n","Loss: 0.504 | Acc: 80.625% \n","Loss: 0.504 | Acc: 80.571% \n","Loss: 0.502 | Acc: 80.785% \n","Loss: 0.503 | Acc: 80.729% \n","Loss: 0.504 | Acc: 80.293% \n","Loss: 0.504 | Acc: 80.188% \n","Loss: 0.505 | Acc: 80.147% \n","Loss: 0.504 | Acc: 80.469% \n","Loss: 0.504 | Acc: 80.366% \n","Loss: 0.504 | Acc: 80.440% \n","Loss: 0.503 | Acc: 80.568% \n","Loss: 0.504 | Acc: 80.469% \n","Loss: 0.504 | Acc: 80.428% \n","Loss: 0.505 | Acc: 80.065% \n","Loss: 0.505 | Acc: 80.032% \n","Loss: 0.504 | Acc: 80.156% \n","Loss: 0.503 | Acc: 80.277% \n","Loss: 0.502 | Acc: 80.242% \n","Loss: 0.503 | Acc: 80.258% \n","Loss: 0.503 | Acc: 80.225% \n","Loss: 0.503 | Acc: 80.096% \n","Loss: 0.503 | Acc: 80.114% \n","Loss: 0.503 | Acc: 80.084% \n","Loss: 0.502 | Acc: 80.239% \n","Loss: 0.501 | Acc: 80.344% \n","Loss: 0.501 | Acc: 80.357% \n","Loss: 0.501 | Acc: 80.414% \n","Loss: 0.501 | Acc: 80.425% \n","Loss: 0.501 | Acc: 80.351% \n","Loss: 0.502 | Acc: 80.321% \n","Loss: 0.501 | Acc: 80.417% \n","Loss: 0.501 | Acc: 80.510% \n","Loss: 0.501 | Acc: 80.438% \n","Loss: 0.501 | Acc: 80.529% \n","Loss: 0.501 | Acc: 80.578% \n","Loss: 0.502 | Acc: 80.352% \n","Loss: 0.502 | Acc: 80.324% \n","Loss: 0.503 | Acc: 80.259% \n","Loss: 0.503 | Acc: 80.309% \n","Loss: 0.503 | Acc: 80.134% \n","Loss: 0.502 | Acc: 80.199% \n","Accuracy of the network on the 1164 test images: 78 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 50%|█████     | 15/30 [2:11:31<2:03:01, 492.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 15\n","Loss: 0.521 | Acc: 78.125% \n","Loss: 0.511 | Acc: 81.250% \n","Loss: 0.483 | Acc: 84.375% \n","Loss: 0.479 | Acc: 85.156% \n","Loss: 0.485 | Acc: 83.125% \n","Loss: 0.480 | Acc: 84.375% \n","Loss: 0.487 | Acc: 82.589% \n","Loss: 0.490 | Acc: 82.422% \n","Loss: 0.499 | Acc: 80.556% \n","Loss: 0.503 | Acc: 80.000% \n","Loss: 0.505 | Acc: 79.830% \n","Loss: 0.505 | Acc: 79.688% \n","Loss: 0.502 | Acc: 80.288% \n","Loss: 0.502 | Acc: 80.357% \n","Loss: 0.504 | Acc: 80.833% \n","Loss: 0.502 | Acc: 80.859% \n","Loss: 0.502 | Acc: 80.882% \n","Loss: 0.499 | Acc: 81.076% \n","Loss: 0.503 | Acc: 81.086% \n","Loss: 0.505 | Acc: 80.625% \n","Loss: 0.503 | Acc: 80.804% \n","Loss: 0.501 | Acc: 81.392% \n","Loss: 0.497 | Acc: 81.658% \n","Loss: 0.497 | Acc: 81.641% \n","Loss: 0.498 | Acc: 81.125% \n","Loss: 0.497 | Acc: 81.250% \n","Loss: 0.497 | Acc: 81.481% \n","Loss: 0.497 | Acc: 81.696% \n","Loss: 0.499 | Acc: 81.358% \n","Loss: 0.501 | Acc: 81.250% \n","Loss: 0.503 | Acc: 80.948% \n","Loss: 0.504 | Acc: 80.566% \n","Loss: 0.503 | Acc: 80.587% \n","Loss: 0.503 | Acc: 80.699% \n","Loss: 0.502 | Acc: 80.893% \n","Loss: 0.500 | Acc: 81.076% \n","Loss: 0.498 | Acc: 81.166% \n","Loss: 0.497 | Acc: 81.332% \n","Loss: 0.498 | Acc: 81.410% \n","Loss: 0.499 | Acc: 81.328% \n","Loss: 0.497 | Acc: 81.631% \n","Loss: 0.498 | Acc: 81.548% \n","Loss: 0.500 | Acc: 81.323% \n","Loss: 0.500 | Acc: 81.392% \n","Loss: 0.502 | Acc: 81.042% \n","Loss: 0.505 | Acc: 80.774% \n","Loss: 0.505 | Acc: 80.718% \n","Loss: 0.507 | Acc: 80.534% \n","Loss: 0.507 | Acc: 80.421% \n","Loss: 0.508 | Acc: 80.312% \n","Loss: 0.508 | Acc: 80.331% \n","Loss: 0.507 | Acc: 80.409% \n","Loss: 0.508 | Acc: 80.130% \n","Loss: 0.507 | Acc: 80.150% \n","Loss: 0.506 | Acc: 80.170% \n","Loss: 0.507 | Acc: 80.134% \n","Loss: 0.507 | Acc: 80.099% \n","Loss: 0.506 | Acc: 80.119% \n","Loss: 0.506 | Acc: 80.138% \n","Loss: 0.506 | Acc: 80.156% \n","Loss: 0.505 | Acc: 80.072% \n","Loss: 0.507 | Acc: 79.788% \n","Loss: 0.507 | Acc: 79.861% \n","Loss: 0.506 | Acc: 80.029% \n","Loss: 0.506 | Acc: 79.952% \n","Loss: 0.506 | Acc: 79.972% \n","Loss: 0.505 | Acc: 80.224% \n","Loss: 0.504 | Acc: 80.331% \n","Loss: 0.503 | Acc: 80.344% \n","Loss: 0.503 | Acc: 80.223% \n","Loss: 0.503 | Acc: 80.062% \n","Loss: 0.504 | Acc: 79.948% \n","Loss: 0.505 | Acc: 79.795% \n","Loss: 0.504 | Acc: 79.856% \n","Loss: 0.503 | Acc: 79.917% \n","Loss: 0.504 | Acc: 79.811% \n","Loss: 0.505 | Acc: 79.870% \n","Loss: 0.504 | Acc: 80.048% \n","Loss: 0.504 | Acc: 80.024% \n","Loss: 0.503 | Acc: 80.039% \n","Loss: 0.503 | Acc: 80.054% \n","Loss: 0.502 | Acc: 80.145% \n","Loss: 0.502 | Acc: 80.233% \n","Loss: 0.502 | Acc: 80.208% \n","Loss: 0.503 | Acc: 80.052% \n","Accuracy of the network on the 1164 test images: 78 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 53%|█████▎    | 16/30 [2:19:39<1:54:33, 490.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 16\n","Loss: 0.463 | Acc: 81.250% \n","Loss: 0.502 | Acc: 73.438% \n","Loss: 0.486 | Acc: 79.167% \n","Loss: 0.481 | Acc: 80.469% \n","Loss: 0.485 | Acc: 80.625% \n","Loss: 0.485 | Acc: 80.729% \n","Loss: 0.495 | Acc: 79.911% \n","Loss: 0.495 | Acc: 80.469% \n","Loss: 0.507 | Acc: 79.167% \n","Loss: 0.504 | Acc: 79.688% \n","Loss: 0.510 | Acc: 78.977% \n","Loss: 0.515 | Acc: 78.385% \n","Loss: 0.512 | Acc: 78.365% \n","Loss: 0.509 | Acc: 78.571% \n","Loss: 0.509 | Acc: 78.750% \n","Loss: 0.510 | Acc: 78.711% \n","Loss: 0.510 | Acc: 78.860% \n","Loss: 0.508 | Acc: 78.993% \n","Loss: 0.506 | Acc: 79.605% \n","Loss: 0.503 | Acc: 79.844% \n","Loss: 0.500 | Acc: 80.357% \n","Loss: 0.497 | Acc: 80.682% \n","Loss: 0.498 | Acc: 80.435% \n","Loss: 0.497 | Acc: 80.469% \n","Loss: 0.497 | Acc: 80.250% \n","Loss: 0.497 | Acc: 80.048% \n","Loss: 0.495 | Acc: 80.440% \n","Loss: 0.500 | Acc: 79.799% \n","Loss: 0.503 | Acc: 79.526% \n","Loss: 0.502 | Acc: 79.896% \n","Loss: 0.500 | Acc: 79.738% \n","Loss: 0.500 | Acc: 79.883% \n","Loss: 0.500 | Acc: 79.735% \n","Loss: 0.499 | Acc: 79.688% \n","Loss: 0.498 | Acc: 79.643% \n","Loss: 0.498 | Acc: 79.688% \n","Loss: 0.495 | Acc: 80.152% \n","Loss: 0.496 | Acc: 80.016% \n","Loss: 0.497 | Acc: 80.048% \n","Loss: 0.496 | Acc: 80.078% \n","Loss: 0.496 | Acc: 80.107% \n","Loss: 0.499 | Acc: 79.911% \n","Loss: 0.500 | Acc: 79.797% \n","Loss: 0.498 | Acc: 79.759% \n","Loss: 0.497 | Acc: 80.000% \n","Loss: 0.496 | Acc: 80.163% \n","Loss: 0.496 | Acc: 80.120% \n","Loss: 0.496 | Acc: 80.208% \n","Loss: 0.494 | Acc: 80.485% \n","Loss: 0.494 | Acc: 80.500% \n","Loss: 0.492 | Acc: 80.699% \n","Loss: 0.493 | Acc: 80.589% \n","Loss: 0.494 | Acc: 80.483% \n","Loss: 0.494 | Acc: 80.440% \n","Loss: 0.493 | Acc: 80.511% \n","Loss: 0.494 | Acc: 80.413% \n","Loss: 0.495 | Acc: 79.989% \n","Loss: 0.497 | Acc: 79.795% \n","Loss: 0.496 | Acc: 79.873% \n","Loss: 0.496 | Acc: 79.896% \n","Loss: 0.497 | Acc: 79.867% \n","Loss: 0.496 | Acc: 79.990% \n","Loss: 0.496 | Acc: 80.109% \n","Loss: 0.496 | Acc: 80.078% \n","Loss: 0.495 | Acc: 80.096% \n","Loss: 0.494 | Acc: 80.256% \n","Loss: 0.496 | Acc: 79.944% \n","Loss: 0.497 | Acc: 79.871% \n","Loss: 0.497 | Acc: 79.710% \n","Loss: 0.498 | Acc: 79.643% \n","Loss: 0.498 | Acc: 79.665% \n","Loss: 0.497 | Acc: 79.731% \n","Loss: 0.497 | Acc: 79.795% \n","Loss: 0.496 | Acc: 79.856% \n","Loss: 0.496 | Acc: 79.792% \n","Loss: 0.496 | Acc: 79.975% \n","Loss: 0.495 | Acc: 80.073% \n","Loss: 0.495 | Acc: 80.048% \n","Loss: 0.495 | Acc: 80.142% \n","Loss: 0.496 | Acc: 80.078% \n","Loss: 0.495 | Acc: 80.208% \n","Loss: 0.495 | Acc: 80.221% \n","Loss: 0.495 | Acc: 80.196% \n","Loss: 0.494 | Acc: 80.208% \n","Loss: 0.494 | Acc: 80.272% \n","Accuracy of the network on the 1164 test images: 81 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 57%|█████▋    | 17/30 [2:27:52<1:46:28, 491.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 17\n","Loss: 0.428 | Acc: 90.625% \n","Loss: 0.467 | Acc: 87.500% \n","Loss: 0.482 | Acc: 83.333% \n","Loss: 0.503 | Acc: 79.688% \n","Loss: 0.497 | Acc: 80.625% \n","Loss: 0.503 | Acc: 79.688% \n","Loss: 0.501 | Acc: 79.911% \n","Loss: 0.502 | Acc: 80.078% \n","Loss: 0.505 | Acc: 80.556% \n","Loss: 0.512 | Acc: 79.375% \n","Loss: 0.510 | Acc: 79.830% \n","Loss: 0.510 | Acc: 79.167% \n","Loss: 0.513 | Acc: 78.365% \n","Loss: 0.512 | Acc: 78.571% \n","Loss: 0.510 | Acc: 78.542% \n","Loss: 0.509 | Acc: 77.930% \n","Loss: 0.506 | Acc: 78.676% \n","Loss: 0.506 | Acc: 78.646% \n","Loss: 0.505 | Acc: 78.618% \n","Loss: 0.505 | Acc: 78.594% \n","Loss: 0.505 | Acc: 78.720% \n","Loss: 0.502 | Acc: 78.835% \n","Loss: 0.504 | Acc: 78.261% \n","Loss: 0.505 | Acc: 77.995% \n","Loss: 0.505 | Acc: 78.000% \n","Loss: 0.506 | Acc: 77.644% \n","Loss: 0.506 | Acc: 77.662% \n","Loss: 0.506 | Acc: 77.902% \n","Loss: 0.509 | Acc: 77.694% \n","Loss: 0.511 | Acc: 77.292% \n","Loss: 0.509 | Acc: 77.520% \n","Loss: 0.508 | Acc: 77.539% \n","Loss: 0.509 | Acc: 77.367% \n","Loss: 0.507 | Acc: 77.482% \n","Loss: 0.508 | Acc: 77.411% \n","Loss: 0.507 | Acc: 77.517% \n","Loss: 0.507 | Acc: 77.703% \n","Loss: 0.506 | Acc: 77.961% \n","Loss: 0.504 | Acc: 78.365% \n","Loss: 0.503 | Acc: 78.516% \n","Loss: 0.503 | Acc: 78.811% \n","Loss: 0.502 | Acc: 78.943% \n","Loss: 0.503 | Acc: 78.852% \n","Loss: 0.503 | Acc: 78.835% \n","Loss: 0.501 | Acc: 79.028% \n","Loss: 0.501 | Acc: 79.076% \n","Loss: 0.500 | Acc: 79.122% \n","Loss: 0.500 | Acc: 79.297% \n","Loss: 0.498 | Acc: 79.592% \n","Loss: 0.498 | Acc: 79.562% \n","Loss: 0.498 | Acc: 79.596% \n","Loss: 0.497 | Acc: 79.808% \n","Loss: 0.496 | Acc: 79.894% \n","Loss: 0.495 | Acc: 79.861% \n","Loss: 0.494 | Acc: 80.170% \n","Loss: 0.494 | Acc: 80.246% \n","Loss: 0.494 | Acc: 80.263% \n","Loss: 0.495 | Acc: 80.011% \n","Loss: 0.494 | Acc: 80.350% \n","Loss: 0.495 | Acc: 80.312% \n","Loss: 0.495 | Acc: 80.430% \n","Loss: 0.495 | Acc: 80.242% \n","Loss: 0.495 | Acc: 80.159% \n","Loss: 0.495 | Acc: 80.225% \n","Loss: 0.496 | Acc: 80.096% \n","Loss: 0.497 | Acc: 80.066% \n","Loss: 0.497 | Acc: 80.084% \n","Loss: 0.496 | Acc: 80.055% \n","Loss: 0.496 | Acc: 80.072% \n","Loss: 0.495 | Acc: 80.223% \n","Loss: 0.495 | Acc: 80.238% \n","Loss: 0.495 | Acc: 80.252% \n","Loss: 0.494 | Acc: 80.265% \n","Loss: 0.494 | Acc: 80.363% \n","Loss: 0.495 | Acc: 80.333% \n","Loss: 0.494 | Acc: 80.345% \n","Loss: 0.494 | Acc: 80.235% \n","Loss: 0.494 | Acc: 80.168% \n","Loss: 0.494 | Acc: 80.182% \n","Loss: 0.494 | Acc: 80.156% \n","Loss: 0.493 | Acc: 80.247% \n","Loss: 0.492 | Acc: 80.412% \n","Loss: 0.492 | Acc: 80.459% \n","Loss: 0.491 | Acc: 80.543% \n","Loss: 0.491 | Acc: 80.567% \n","Accuracy of the network on the 1164 test images: 79 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 60%|██████    | 18/30 [2:36:00<1:38:04, 490.40s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 18\n","Loss: 0.411 | Acc: 90.625% \n","Loss: 0.451 | Acc: 89.062% \n","Loss: 0.480 | Acc: 87.500% \n","Loss: 0.468 | Acc: 86.719% \n","Loss: 0.481 | Acc: 85.625% \n","Loss: 0.480 | Acc: 84.896% \n","Loss: 0.479 | Acc: 85.268% \n","Loss: 0.480 | Acc: 84.766% \n","Loss: 0.482 | Acc: 84.028% \n","Loss: 0.481 | Acc: 84.375% \n","Loss: 0.477 | Acc: 84.375% \n","Loss: 0.476 | Acc: 83.854% \n","Loss: 0.471 | Acc: 84.375% \n","Loss: 0.466 | Acc: 84.375% \n","Loss: 0.466 | Acc: 84.375% \n","Loss: 0.472 | Acc: 83.203% \n","Loss: 0.468 | Acc: 83.456% \n","Loss: 0.471 | Acc: 82.812% \n","Loss: 0.473 | Acc: 82.401% \n","Loss: 0.475 | Acc: 82.500% \n","Loss: 0.472 | Acc: 82.887% \n","Loss: 0.470 | Acc: 82.812% \n","Loss: 0.473 | Acc: 81.658% \n","Loss: 0.475 | Acc: 81.901% \n","Loss: 0.476 | Acc: 82.125% \n","Loss: 0.478 | Acc: 81.611% \n","Loss: 0.482 | Acc: 81.250% \n","Loss: 0.482 | Acc: 81.027% \n","Loss: 0.485 | Acc: 80.711% \n","Loss: 0.483 | Acc: 80.833% \n","Loss: 0.485 | Acc: 80.847% \n","Loss: 0.486 | Acc: 80.859% \n","Loss: 0.487 | Acc: 80.966% \n","Loss: 0.487 | Acc: 80.882% \n","Loss: 0.486 | Acc: 80.893% \n","Loss: 0.486 | Acc: 81.076% \n","Loss: 0.484 | Acc: 81.166% \n","Loss: 0.484 | Acc: 81.250% \n","Loss: 0.484 | Acc: 81.090% \n","Loss: 0.487 | Acc: 80.938% \n","Loss: 0.488 | Acc: 80.869% \n","Loss: 0.486 | Acc: 81.176% \n","Loss: 0.487 | Acc: 81.105% \n","Loss: 0.488 | Acc: 80.966% \n","Loss: 0.487 | Acc: 81.111% \n","Loss: 0.488 | Acc: 81.114% \n","Loss: 0.486 | Acc: 81.250% \n","Loss: 0.486 | Acc: 81.250% \n","Loss: 0.486 | Acc: 81.059% \n","Loss: 0.485 | Acc: 81.188% \n","Loss: 0.486 | Acc: 81.127% \n","Loss: 0.485 | Acc: 81.130% \n","Loss: 0.487 | Acc: 80.955% \n","Loss: 0.486 | Acc: 81.019% \n","Loss: 0.486 | Acc: 81.023% \n","Loss: 0.485 | Acc: 80.971% \n","Loss: 0.487 | Acc: 80.921% \n","Loss: 0.486 | Acc: 80.927% \n","Loss: 0.485 | Acc: 80.985% \n","Loss: 0.485 | Acc: 81.146% \n","Loss: 0.485 | Acc: 80.994% \n","Loss: 0.485 | Acc: 80.998% \n","Loss: 0.486 | Acc: 80.903% \n","Loss: 0.487 | Acc: 80.664% \n","Loss: 0.486 | Acc: 80.673% \n","Loss: 0.485 | Acc: 80.682% \n","Loss: 0.485 | Acc: 80.784% \n","Loss: 0.485 | Acc: 80.653% \n","Loss: 0.485 | Acc: 80.661% \n","Loss: 0.485 | Acc: 80.625% \n","Loss: 0.485 | Acc: 80.766% \n","Loss: 0.486 | Acc: 80.512% \n","Loss: 0.486 | Acc: 80.565% \n","Loss: 0.486 | Acc: 80.532% \n","Loss: 0.486 | Acc: 80.500% \n","Loss: 0.486 | Acc: 80.510% \n","Loss: 0.487 | Acc: 80.479% \n","Loss: 0.486 | Acc: 80.649% \n","Loss: 0.485 | Acc: 80.815% \n","Loss: 0.485 | Acc: 80.664% \n","Loss: 0.485 | Acc: 80.710% \n","Loss: 0.484 | Acc: 80.755% \n","Loss: 0.484 | Acc: 80.836% \n","Loss: 0.484 | Acc: 80.729% \n","Loss: 0.485 | Acc: 80.714% \n","Accuracy of the network on the 1164 test images: 82 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 63%|██████▎   | 19/30 [2:44:08<1:29:45, 489.58s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 19\n","Loss: 0.466 | Acc: 75.000% \n","Loss: 0.484 | Acc: 78.125% \n","Loss: 0.512 | Acc: 75.000% \n","Loss: 0.492 | Acc: 78.125% \n","Loss: 0.478 | Acc: 80.000% \n","Loss: 0.473 | Acc: 80.729% \n","Loss: 0.469 | Acc: 81.250% \n","Loss: 0.469 | Acc: 80.859% \n","Loss: 0.463 | Acc: 82.292% \n","Loss: 0.456 | Acc: 83.750% \n","Loss: 0.456 | Acc: 82.955% \n","Loss: 0.455 | Acc: 83.594% \n","Loss: 0.451 | Acc: 83.654% \n","Loss: 0.458 | Acc: 83.482% \n","Loss: 0.455 | Acc: 84.167% \n","Loss: 0.463 | Acc: 82.812% \n","Loss: 0.464 | Acc: 83.272% \n","Loss: 0.463 | Acc: 83.333% \n","Loss: 0.464 | Acc: 82.895% \n","Loss: 0.462 | Acc: 83.125% \n","Loss: 0.464 | Acc: 82.738% \n","Loss: 0.463 | Acc: 82.670% \n","Loss: 0.467 | Acc: 82.065% \n","Loss: 0.466 | Acc: 82.161% \n","Loss: 0.465 | Acc: 82.375% \n","Loss: 0.465 | Acc: 82.091% \n","Loss: 0.464 | Acc: 81.944% \n","Loss: 0.461 | Acc: 82.366% \n","Loss: 0.461 | Acc: 82.543% \n","Loss: 0.462 | Acc: 82.604% \n","Loss: 0.460 | Acc: 82.964% \n","Loss: 0.462 | Acc: 82.812% \n","Loss: 0.464 | Acc: 82.576% \n","Loss: 0.466 | Acc: 82.537% \n","Loss: 0.465 | Acc: 82.500% \n","Loss: 0.465 | Acc: 82.726% \n","Loss: 0.464 | Acc: 82.855% \n","Loss: 0.465 | Acc: 82.730% \n","Loss: 0.466 | Acc: 82.692% \n","Loss: 0.463 | Acc: 82.969% \n","Loss: 0.466 | Acc: 82.851% \n","Loss: 0.467 | Acc: 82.589% \n","Loss: 0.467 | Acc: 82.558% \n","Loss: 0.466 | Acc: 82.599% \n","Loss: 0.465 | Acc: 82.500% \n","Loss: 0.466 | Acc: 82.541% \n","Loss: 0.467 | Acc: 82.314% \n","Loss: 0.468 | Acc: 82.161% \n","Loss: 0.469 | Acc: 82.079% \n","Loss: 0.469 | Acc: 82.188% \n","Loss: 0.469 | Acc: 82.108% \n","Loss: 0.468 | Acc: 81.971% \n","Loss: 0.467 | Acc: 82.134% \n","Loss: 0.467 | Acc: 82.234% \n","Loss: 0.466 | Acc: 82.330% \n","Loss: 0.466 | Acc: 82.366% \n","Loss: 0.467 | Acc: 82.182% \n","Loss: 0.467 | Acc: 82.328% \n","Loss: 0.467 | Acc: 82.150% \n","Loss: 0.467 | Acc: 82.344% \n","Loss: 0.468 | Acc: 82.172% \n","Loss: 0.469 | Acc: 81.905% \n","Loss: 0.468 | Acc: 81.895% \n","Loss: 0.469 | Acc: 81.836% \n","Loss: 0.468 | Acc: 81.875% \n","Loss: 0.468 | Acc: 81.866% \n","Loss: 0.469 | Acc: 81.856% \n","Loss: 0.469 | Acc: 81.893% \n","Loss: 0.469 | Acc: 81.929% \n","Loss: 0.468 | Acc: 82.054% \n","Loss: 0.469 | Acc: 81.910% \n","Loss: 0.469 | Acc: 81.988% \n","Loss: 0.470 | Acc: 81.978% \n","Loss: 0.469 | Acc: 82.137% \n","Loss: 0.470 | Acc: 81.958% \n","Loss: 0.471 | Acc: 81.785% \n","Loss: 0.471 | Acc: 81.737% \n","Loss: 0.472 | Acc: 81.651% \n","Loss: 0.472 | Acc: 81.685% \n","Loss: 0.471 | Acc: 81.719% \n","Loss: 0.471 | Acc: 81.790% \n","Loss: 0.471 | Acc: 81.822% \n","Loss: 0.471 | Acc: 81.852% \n","Loss: 0.470 | Acc: 81.994% \n","Loss: 0.469 | Acc: 82.039% \n","Accuracy of the network on the 1164 test images: 82 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 67%|██████▋   | 20/30 [2:52:17<1:21:36, 489.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 20\n","Loss: 0.519 | Acc: 78.125% \n","Loss: 0.488 | Acc: 82.812% \n","Loss: 0.503 | Acc: 79.167% \n","Loss: 0.498 | Acc: 79.688% \n","Loss: 0.483 | Acc: 81.875% \n","Loss: 0.478 | Acc: 81.771% \n","Loss: 0.475 | Acc: 81.696% \n","Loss: 0.467 | Acc: 82.422% \n","Loss: 0.468 | Acc: 81.944% \n","Loss: 0.472 | Acc: 80.938% \n","Loss: 0.467 | Acc: 82.386% \n","Loss: 0.466 | Acc: 82.552% \n","Loss: 0.461 | Acc: 82.933% \n","Loss: 0.466 | Acc: 82.812% \n","Loss: 0.467 | Acc: 82.500% \n","Loss: 0.469 | Acc: 82.422% \n","Loss: 0.474 | Acc: 81.434% \n","Loss: 0.477 | Acc: 80.729% \n","Loss: 0.478 | Acc: 80.921% \n","Loss: 0.479 | Acc: 80.312% \n","Loss: 0.480 | Acc: 80.060% \n","Loss: 0.482 | Acc: 79.972% \n","Loss: 0.484 | Acc: 79.755% \n","Loss: 0.485 | Acc: 79.557% \n","Loss: 0.483 | Acc: 79.875% \n","Loss: 0.481 | Acc: 80.048% \n","Loss: 0.482 | Acc: 80.093% \n","Loss: 0.482 | Acc: 80.134% \n","Loss: 0.478 | Acc: 80.496% \n","Loss: 0.476 | Acc: 80.625% \n","Loss: 0.475 | Acc: 80.343% \n","Loss: 0.473 | Acc: 80.762% \n","Loss: 0.472 | Acc: 80.777% \n","Loss: 0.473 | Acc: 80.607% \n","Loss: 0.473 | Acc: 80.446% \n","Loss: 0.474 | Acc: 80.382% \n","Loss: 0.474 | Acc: 80.574% \n","Loss: 0.472 | Acc: 80.839% \n","Loss: 0.472 | Acc: 80.689% \n","Loss: 0.471 | Acc: 80.859% \n","Loss: 0.471 | Acc: 80.716% \n","Loss: 0.471 | Acc: 80.729% \n","Loss: 0.470 | Acc: 80.887% \n","Loss: 0.468 | Acc: 81.037% \n","Loss: 0.467 | Acc: 81.181% \n","Loss: 0.468 | Acc: 81.114% \n","Loss: 0.468 | Acc: 80.984% \n","Loss: 0.468 | Acc: 81.055% \n","Loss: 0.467 | Acc: 81.186% \n","Loss: 0.467 | Acc: 81.188% \n","Loss: 0.468 | Acc: 81.005% \n","Loss: 0.467 | Acc: 81.070% \n","Loss: 0.467 | Acc: 81.132% \n","Loss: 0.467 | Acc: 81.192% \n","Loss: 0.466 | Acc: 81.364% \n","Loss: 0.466 | Acc: 81.306% \n","Loss: 0.465 | Acc: 81.360% \n","Loss: 0.466 | Acc: 81.466% \n","Loss: 0.466 | Acc: 81.515% \n","Loss: 0.466 | Acc: 81.406% \n","Loss: 0.468 | Acc: 81.199% \n","Loss: 0.467 | Acc: 81.200% \n","Loss: 0.467 | Acc: 81.250% \n","Loss: 0.467 | Acc: 81.348% \n","Loss: 0.467 | Acc: 81.298% \n","Loss: 0.467 | Acc: 81.392% \n","Loss: 0.466 | Acc: 81.576% \n","Loss: 0.467 | Acc: 81.480% \n","Loss: 0.467 | Acc: 81.431% \n","Loss: 0.466 | Acc: 81.518% \n","Loss: 0.467 | Acc: 81.470% \n","Loss: 0.467 | Acc: 81.510% \n","Loss: 0.467 | Acc: 81.678% \n","Loss: 0.467 | Acc: 81.757% \n","Loss: 0.467 | Acc: 81.792% \n","Loss: 0.466 | Acc: 81.867% \n","Loss: 0.466 | Acc: 81.899% \n","Loss: 0.466 | Acc: 81.971% \n","Loss: 0.465 | Acc: 82.002% \n","Loss: 0.464 | Acc: 82.070% \n","Loss: 0.465 | Acc: 81.906% \n","Loss: 0.466 | Acc: 81.860% \n","Loss: 0.466 | Acc: 81.890% \n","Loss: 0.466 | Acc: 81.882% \n","Loss: 0.465 | Acc: 81.965% \n","Accuracy of the network on the 1164 test images: 83 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 70%|███████   | 21/30 [3:00:28<1:13:30, 490.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 21\n","Loss: 0.559 | Acc: 65.625% \n","Loss: 0.492 | Acc: 75.000% \n","Loss: 0.473 | Acc: 77.083% \n","Loss: 0.495 | Acc: 73.438% \n","Loss: 0.501 | Acc: 73.750% \n","Loss: 0.489 | Acc: 76.562% \n","Loss: 0.486 | Acc: 77.679% \n","Loss: 0.483 | Acc: 78.125% \n","Loss: 0.486 | Acc: 77.431% \n","Loss: 0.485 | Acc: 78.125% \n","Loss: 0.488 | Acc: 78.693% \n","Loss: 0.486 | Acc: 78.125% \n","Loss: 0.485 | Acc: 78.365% \n","Loss: 0.482 | Acc: 78.795% \n","Loss: 0.483 | Acc: 79.167% \n","Loss: 0.484 | Acc: 79.297% \n","Loss: 0.485 | Acc: 79.044% \n","Loss: 0.486 | Acc: 78.819% \n","Loss: 0.483 | Acc: 79.276% \n","Loss: 0.480 | Acc: 79.688% \n","Loss: 0.483 | Acc: 79.464% \n","Loss: 0.484 | Acc: 79.119% \n","Loss: 0.483 | Acc: 79.348% \n","Loss: 0.484 | Acc: 79.427% \n","Loss: 0.486 | Acc: 79.000% \n","Loss: 0.482 | Acc: 79.327% \n","Loss: 0.482 | Acc: 79.398% \n","Loss: 0.480 | Acc: 79.688% \n","Loss: 0.479 | Acc: 79.849% \n","Loss: 0.480 | Acc: 79.688% \n","Loss: 0.478 | Acc: 79.940% \n","Loss: 0.477 | Acc: 79.980% \n","Loss: 0.477 | Acc: 79.830% \n","Loss: 0.475 | Acc: 80.239% \n","Loss: 0.473 | Acc: 80.625% \n","Loss: 0.474 | Acc: 80.469% \n","Loss: 0.473 | Acc: 80.405% \n","Loss: 0.472 | Acc: 80.510% \n","Loss: 0.473 | Acc: 80.369% \n","Loss: 0.473 | Acc: 80.391% \n","Loss: 0.471 | Acc: 80.564% \n","Loss: 0.470 | Acc: 80.655% \n","Loss: 0.472 | Acc: 80.596% \n","Loss: 0.471 | Acc: 80.682% \n","Loss: 0.471 | Acc: 80.764% \n","Loss: 0.470 | Acc: 80.910% \n","Loss: 0.471 | Acc: 80.785% \n","Loss: 0.470 | Acc: 80.924% \n","Loss: 0.470 | Acc: 80.867% \n","Loss: 0.469 | Acc: 80.938% \n","Loss: 0.467 | Acc: 81.005% \n","Loss: 0.466 | Acc: 81.250% \n","Loss: 0.466 | Acc: 81.191% \n","Loss: 0.467 | Acc: 81.019% \n","Loss: 0.466 | Acc: 81.136% \n","Loss: 0.465 | Acc: 81.306% \n","Loss: 0.466 | Acc: 81.360% \n","Loss: 0.465 | Acc: 81.519% \n","Loss: 0.466 | Acc: 81.462% \n","Loss: 0.466 | Acc: 81.510% \n","Loss: 0.465 | Acc: 81.711% \n","Loss: 0.465 | Acc: 81.653% \n","Loss: 0.466 | Acc: 81.498% \n","Loss: 0.467 | Acc: 81.250% \n","Loss: 0.466 | Acc: 81.346% \n","Loss: 0.466 | Acc: 81.487% \n","Loss: 0.465 | Acc: 81.576% \n","Loss: 0.465 | Acc: 81.572% \n","Loss: 0.465 | Acc: 81.612% \n","Loss: 0.465 | Acc: 81.562% \n","Loss: 0.465 | Acc: 81.690% \n","Loss: 0.465 | Acc: 81.684% \n","Loss: 0.466 | Acc: 81.678% \n","Loss: 0.466 | Acc: 81.630% \n","Loss: 0.466 | Acc: 81.667% \n","Loss: 0.467 | Acc: 81.538% \n","Loss: 0.467 | Acc: 81.615% \n","Loss: 0.467 | Acc: 81.530% \n","Loss: 0.467 | Acc: 81.566% \n","Loss: 0.468 | Acc: 81.445% \n","Loss: 0.467 | Acc: 81.636% \n","Loss: 0.465 | Acc: 81.745% \n","Loss: 0.465 | Acc: 81.777% \n","Loss: 0.464 | Acc: 81.882% \n","Loss: 0.464 | Acc: 81.892% \n","Accuracy of the network on the 1164 test images: 81 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 73%|███████▎  | 22/30 [3:08:40<1:05:23, 490.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 22\n","Loss: 0.453 | Acc: 81.250% \n","Loss: 0.454 | Acc: 84.375% \n","Loss: 0.475 | Acc: 80.208% \n","Loss: 0.466 | Acc: 82.031% \n","Loss: 0.452 | Acc: 83.125% \n","Loss: 0.456 | Acc: 82.292% \n","Loss: 0.450 | Acc: 83.036% \n","Loss: 0.456 | Acc: 82.422% \n","Loss: 0.457 | Acc: 82.986% \n","Loss: 0.463 | Acc: 81.250% \n","Loss: 0.462 | Acc: 81.534% \n","Loss: 0.466 | Acc: 81.510% \n","Loss: 0.464 | Acc: 81.971% \n","Loss: 0.465 | Acc: 82.589% \n","Loss: 0.463 | Acc: 83.125% \n","Loss: 0.461 | Acc: 83.398% \n","Loss: 0.464 | Acc: 83.088% \n","Loss: 0.466 | Acc: 82.812% \n","Loss: 0.465 | Acc: 83.224% \n","Loss: 0.464 | Acc: 83.281% \n","Loss: 0.467 | Acc: 82.589% \n","Loss: 0.466 | Acc: 82.955% \n","Loss: 0.466 | Acc: 83.016% \n","Loss: 0.466 | Acc: 82.943% \n","Loss: 0.463 | Acc: 82.875% \n","Loss: 0.463 | Acc: 83.053% \n","Loss: 0.465 | Acc: 82.639% \n","Loss: 0.466 | Acc: 82.366% \n","Loss: 0.468 | Acc: 82.112% \n","Loss: 0.469 | Acc: 81.667% \n","Loss: 0.468 | Acc: 82.056% \n","Loss: 0.468 | Acc: 82.324% \n","Loss: 0.466 | Acc: 82.481% \n","Loss: 0.466 | Acc: 82.629% \n","Loss: 0.466 | Acc: 82.321% \n","Loss: 0.465 | Acc: 82.465% \n","Loss: 0.466 | Acc: 82.348% \n","Loss: 0.468 | Acc: 82.319% \n","Loss: 0.467 | Acc: 82.532% \n","Loss: 0.466 | Acc: 82.812% \n","Loss: 0.466 | Acc: 82.774% \n","Loss: 0.468 | Acc: 82.589% \n","Loss: 0.467 | Acc: 82.631% \n","Loss: 0.468 | Acc: 82.670% \n","Loss: 0.467 | Acc: 82.639% \n","Loss: 0.468 | Acc: 82.337% \n","Loss: 0.467 | Acc: 82.580% \n","Loss: 0.464 | Acc: 82.747% \n","Loss: 0.465 | Acc: 82.526% \n","Loss: 0.466 | Acc: 82.500% \n","Loss: 0.465 | Acc: 82.475% \n","Loss: 0.465 | Acc: 82.332% \n","Loss: 0.465 | Acc: 82.193% \n","Loss: 0.466 | Acc: 82.176% \n","Loss: 0.465 | Acc: 82.386% \n","Loss: 0.465 | Acc: 82.366% \n","Loss: 0.464 | Acc: 82.511% \n","Loss: 0.464 | Acc: 82.543% \n","Loss: 0.464 | Acc: 82.574% \n","Loss: 0.464 | Acc: 82.552% \n","Loss: 0.463 | Acc: 82.684% \n","Loss: 0.463 | Acc: 82.611% \n","Loss: 0.462 | Acc: 82.738% \n","Loss: 0.463 | Acc: 82.715% \n","Loss: 0.463 | Acc: 82.596% \n","Loss: 0.463 | Acc: 82.576% \n","Loss: 0.463 | Acc: 82.603% \n","Loss: 0.463 | Acc: 82.629% \n","Loss: 0.464 | Acc: 82.563% \n","Loss: 0.465 | Acc: 82.545% \n","Loss: 0.465 | Acc: 82.526% \n","Loss: 0.464 | Acc: 82.726% \n","Loss: 0.463 | Acc: 82.834% \n","Loss: 0.463 | Acc: 82.770% \n","Loss: 0.464 | Acc: 82.708% \n","Loss: 0.463 | Acc: 82.812% \n","Loss: 0.463 | Acc: 82.752% \n","Loss: 0.463 | Acc: 82.692% \n","Loss: 0.465 | Acc: 82.476% \n","Loss: 0.466 | Acc: 82.344% \n","Loss: 0.466 | Acc: 82.215% \n","Loss: 0.465 | Acc: 82.279% \n","Loss: 0.465 | Acc: 82.229% \n","Loss: 0.465 | Acc: 82.292% \n","Loss: 0.464 | Acc: 82.333% \n","Accuracy of the network on the 1164 test images: 83 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 77%|███████▋  | 23/30 [3:16:54<57:20, 491.52s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 23\n","Loss: 0.409 | Acc: 93.750% \n","Loss: 0.426 | Acc: 89.062% \n","Loss: 0.410 | Acc: 87.500% \n","Loss: 0.427 | Acc: 86.719% \n","Loss: 0.425 | Acc: 86.250% \n","Loss: 0.429 | Acc: 84.896% \n","Loss: 0.433 | Acc: 84.375% \n","Loss: 0.442 | Acc: 83.594% \n","Loss: 0.442 | Acc: 83.333% \n","Loss: 0.447 | Acc: 82.500% \n","Loss: 0.449 | Acc: 82.386% \n","Loss: 0.447 | Acc: 82.292% \n","Loss: 0.448 | Acc: 82.452% \n","Loss: 0.451 | Acc: 81.696% \n","Loss: 0.454 | Acc: 81.667% \n","Loss: 0.452 | Acc: 82.227% \n","Loss: 0.450 | Acc: 82.353% \n","Loss: 0.453 | Acc: 82.118% \n","Loss: 0.455 | Acc: 81.743% \n","Loss: 0.454 | Acc: 81.875% \n","Loss: 0.450 | Acc: 82.440% \n","Loss: 0.449 | Acc: 82.386% \n","Loss: 0.448 | Acc: 82.473% \n","Loss: 0.447 | Acc: 82.812% \n","Loss: 0.449 | Acc: 82.500% \n","Loss: 0.450 | Acc: 82.332% \n","Loss: 0.449 | Acc: 82.407% \n","Loss: 0.448 | Acc: 82.589% \n","Loss: 0.450 | Acc: 82.543% \n","Loss: 0.450 | Acc: 82.604% \n","Loss: 0.450 | Acc: 82.661% \n","Loss: 0.452 | Acc: 82.324% \n","Loss: 0.452 | Acc: 82.292% \n","Loss: 0.452 | Acc: 82.261% \n","Loss: 0.451 | Acc: 82.411% \n","Loss: 0.452 | Acc: 82.465% \n","Loss: 0.452 | Acc: 82.264% \n","Loss: 0.452 | Acc: 82.484% \n","Loss: 0.453 | Acc: 82.452% \n","Loss: 0.454 | Acc: 82.266% \n","Loss: 0.454 | Acc: 82.317% \n","Loss: 0.454 | Acc: 82.366% \n","Loss: 0.453 | Acc: 82.413% \n","Loss: 0.456 | Acc: 82.102% \n","Loss: 0.455 | Acc: 82.361% \n","Loss: 0.456 | Acc: 82.133% \n","Loss: 0.457 | Acc: 82.247% \n","Loss: 0.457 | Acc: 82.227% \n","Loss: 0.457 | Acc: 82.143% \n","Loss: 0.456 | Acc: 82.250% \n","Loss: 0.457 | Acc: 82.047% \n","Loss: 0.458 | Acc: 82.091% \n","Loss: 0.457 | Acc: 82.252% \n","Loss: 0.456 | Acc: 82.292% \n","Loss: 0.455 | Acc: 82.500% \n","Loss: 0.455 | Acc: 82.533% \n","Loss: 0.455 | Acc: 82.566% \n","Loss: 0.456 | Acc: 82.543% \n","Loss: 0.455 | Acc: 82.733% \n","Loss: 0.456 | Acc: 82.656% \n","Loss: 0.456 | Acc: 82.531% \n","Loss: 0.456 | Acc: 82.460% \n","Loss: 0.455 | Acc: 82.589% \n","Loss: 0.455 | Acc: 82.617% \n","Loss: 0.455 | Acc: 82.596% \n","Loss: 0.455 | Acc: 82.576% \n","Loss: 0.454 | Acc: 82.649% \n","Loss: 0.455 | Acc: 82.537% \n","Loss: 0.456 | Acc: 82.428% \n","Loss: 0.456 | Acc: 82.188% \n","Loss: 0.455 | Acc: 82.350% \n","Loss: 0.456 | Acc: 82.378% \n","Loss: 0.456 | Acc: 82.277% \n","Loss: 0.456 | Acc: 82.348% \n","Loss: 0.456 | Acc: 82.375% \n","Loss: 0.457 | Acc: 82.319% \n","Loss: 0.457 | Acc: 82.346% \n","Loss: 0.456 | Acc: 82.572% \n","Loss: 0.456 | Acc: 82.555% \n","Loss: 0.456 | Acc: 82.617% \n","Loss: 0.456 | Acc: 82.639% \n","Loss: 0.455 | Acc: 82.660% \n","Loss: 0.455 | Acc: 82.605% \n","Loss: 0.456 | Acc: 82.515% \n","Loss: 0.455 | Acc: 82.517% \n","Accuracy of the network on the 1164 test images: 82 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 80%|████████  | 24/30 [3:25:04<49:06, 491.11s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 24\n","Loss: 0.438 | Acc: 81.250% \n","Loss: 0.443 | Acc: 85.938% \n","Loss: 0.447 | Acc: 87.500% \n","Loss: 0.460 | Acc: 85.156% \n","Loss: 0.449 | Acc: 85.000% \n","Loss: 0.450 | Acc: 85.417% \n","Loss: 0.447 | Acc: 85.714% \n","Loss: 0.447 | Acc: 85.156% \n","Loss: 0.448 | Acc: 84.375% \n","Loss: 0.453 | Acc: 83.438% \n","Loss: 0.455 | Acc: 83.239% \n","Loss: 0.465 | Acc: 82.292% \n","Loss: 0.468 | Acc: 82.452% \n","Loss: 0.471 | Acc: 82.143% \n","Loss: 0.471 | Acc: 82.083% \n","Loss: 0.469 | Acc: 82.227% \n","Loss: 0.468 | Acc: 82.169% \n","Loss: 0.469 | Acc: 81.771% \n","Loss: 0.475 | Acc: 81.414% \n","Loss: 0.471 | Acc: 81.719% \n","Loss: 0.468 | Acc: 81.696% \n","Loss: 0.467 | Acc: 81.534% \n","Loss: 0.467 | Acc: 81.386% \n","Loss: 0.463 | Acc: 82.031% \n","Loss: 0.462 | Acc: 82.000% \n","Loss: 0.458 | Acc: 82.332% \n","Loss: 0.460 | Acc: 82.292% \n","Loss: 0.459 | Acc: 82.254% \n","Loss: 0.458 | Acc: 82.651% \n","Loss: 0.456 | Acc: 82.917% \n","Loss: 0.456 | Acc: 82.863% \n","Loss: 0.454 | Acc: 82.715% \n","Loss: 0.457 | Acc: 82.292% \n","Loss: 0.455 | Acc: 82.445% \n","Loss: 0.456 | Acc: 82.321% \n","Loss: 0.456 | Acc: 82.205% \n","Loss: 0.455 | Acc: 82.348% \n","Loss: 0.455 | Acc: 82.237% \n","Loss: 0.456 | Acc: 82.212% \n","Loss: 0.456 | Acc: 82.031% \n","Loss: 0.458 | Acc: 81.936% \n","Loss: 0.459 | Acc: 81.994% \n","Loss: 0.457 | Acc: 82.195% \n","Loss: 0.456 | Acc: 82.386% \n","Loss: 0.456 | Acc: 82.292% \n","Loss: 0.457 | Acc: 82.065% \n","Loss: 0.457 | Acc: 82.048% \n","Loss: 0.457 | Acc: 82.227% \n","Loss: 0.457 | Acc: 82.270% \n","Loss: 0.457 | Acc: 82.188% \n","Loss: 0.457 | Acc: 82.169% \n","Loss: 0.457 | Acc: 82.091% \n","Loss: 0.457 | Acc: 81.958% \n","Loss: 0.457 | Acc: 81.887% \n","Loss: 0.457 | Acc: 81.761% \n","Loss: 0.456 | Acc: 81.864% \n","Loss: 0.456 | Acc: 82.072% \n","Loss: 0.457 | Acc: 81.897% \n","Loss: 0.457 | Acc: 81.833% \n","Loss: 0.458 | Acc: 81.771% \n","Loss: 0.458 | Acc: 81.762% \n","Loss: 0.457 | Acc: 81.956% \n","Loss: 0.457 | Acc: 81.895% \n","Loss: 0.457 | Acc: 81.934% \n","Loss: 0.458 | Acc: 81.923% \n","Loss: 0.458 | Acc: 81.913% \n","Loss: 0.458 | Acc: 81.996% \n","Loss: 0.457 | Acc: 82.031% \n","Loss: 0.458 | Acc: 81.884% \n","Loss: 0.458 | Acc: 81.875% \n","Loss: 0.458 | Acc: 81.998% \n","Loss: 0.458 | Acc: 81.988% \n","Loss: 0.457 | Acc: 82.021% \n","Loss: 0.457 | Acc: 82.010% \n","Loss: 0.457 | Acc: 82.125% \n","Loss: 0.456 | Acc: 82.155% \n","Loss: 0.455 | Acc: 82.265% \n","Loss: 0.455 | Acc: 82.252% \n","Loss: 0.454 | Acc: 82.318% \n","Loss: 0.454 | Acc: 82.422% \n","Loss: 0.454 | Acc: 82.407% \n","Loss: 0.454 | Acc: 82.317% \n","Loss: 0.454 | Acc: 82.304% \n","Loss: 0.454 | Acc: 82.440% \n","Loss: 0.453 | Acc: 82.444% \n","Accuracy of the network on the 1164 test images: 82 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 83%|████████▎ | 25/30 [3:33:13<40:51, 490.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 25\n","Loss: 0.503 | Acc: 78.125% \n","Loss: 0.478 | Acc: 82.812% \n","Loss: 0.480 | Acc: 79.167% \n","Loss: 0.483 | Acc: 78.125% \n","Loss: 0.468 | Acc: 80.000% \n","Loss: 0.461 | Acc: 80.208% \n","Loss: 0.462 | Acc: 80.804% \n","Loss: 0.470 | Acc: 79.688% \n","Loss: 0.470 | Acc: 79.861% \n","Loss: 0.464 | Acc: 81.250% \n","Loss: 0.460 | Acc: 81.250% \n","Loss: 0.464 | Acc: 80.208% \n","Loss: 0.466 | Acc: 80.048% \n","Loss: 0.471 | Acc: 79.911% \n","Loss: 0.465 | Acc: 80.417% \n","Loss: 0.461 | Acc: 80.664% \n","Loss: 0.459 | Acc: 80.882% \n","Loss: 0.457 | Acc: 80.903% \n","Loss: 0.456 | Acc: 80.921% \n","Loss: 0.456 | Acc: 80.781% \n","Loss: 0.456 | Acc: 80.506% \n","Loss: 0.457 | Acc: 80.824% \n","Loss: 0.457 | Acc: 80.978% \n","Loss: 0.455 | Acc: 81.380% \n","Loss: 0.457 | Acc: 81.250% \n","Loss: 0.458 | Acc: 81.010% \n","Loss: 0.460 | Acc: 81.019% \n","Loss: 0.459 | Acc: 81.027% \n","Loss: 0.458 | Acc: 81.034% \n","Loss: 0.457 | Acc: 81.354% \n","Loss: 0.454 | Acc: 81.653% \n","Loss: 0.453 | Acc: 81.738% \n","Loss: 0.452 | Acc: 82.008% \n","Loss: 0.454 | Acc: 81.985% \n","Loss: 0.456 | Acc: 81.875% \n","Loss: 0.454 | Acc: 82.205% \n","Loss: 0.455 | Acc: 82.095% \n","Loss: 0.455 | Acc: 82.072% \n","Loss: 0.454 | Acc: 82.212% \n","Loss: 0.456 | Acc: 82.031% \n","Loss: 0.453 | Acc: 82.241% \n","Loss: 0.455 | Acc: 82.068% \n","Loss: 0.455 | Acc: 82.267% \n","Loss: 0.453 | Acc: 82.457% \n","Loss: 0.452 | Acc: 82.639% \n","Loss: 0.451 | Acc: 82.745% \n","Loss: 0.452 | Acc: 82.513% \n","Loss: 0.451 | Acc: 82.747% \n","Loss: 0.451 | Acc: 82.653% \n","Loss: 0.452 | Acc: 82.625% \n","Loss: 0.451 | Acc: 82.843% \n","Loss: 0.449 | Acc: 82.993% \n","Loss: 0.449 | Acc: 83.078% \n","Loss: 0.450 | Acc: 83.044% \n","Loss: 0.451 | Acc: 82.841% \n","Loss: 0.451 | Acc: 82.701% \n","Loss: 0.450 | Acc: 82.785% \n","Loss: 0.450 | Acc: 82.759% \n","Loss: 0.450 | Acc: 82.733% \n","Loss: 0.450 | Acc: 82.708% \n","Loss: 0.450 | Acc: 82.684% \n","Loss: 0.450 | Acc: 82.712% \n","Loss: 0.450 | Acc: 82.788% \n","Loss: 0.450 | Acc: 82.568% \n","Loss: 0.451 | Acc: 82.452% \n","Loss: 0.450 | Acc: 82.576% \n","Loss: 0.450 | Acc: 82.696% \n","Loss: 0.449 | Acc: 82.812% \n","Loss: 0.448 | Acc: 82.971% \n","Loss: 0.448 | Acc: 83.036% \n","Loss: 0.447 | Acc: 83.055% \n","Loss: 0.446 | Acc: 83.160% \n","Loss: 0.446 | Acc: 83.219% \n","Loss: 0.446 | Acc: 83.319% \n","Loss: 0.446 | Acc: 83.250% \n","Loss: 0.446 | Acc: 83.183% \n","Loss: 0.446 | Acc: 83.157% \n","Loss: 0.447 | Acc: 83.173% \n","Loss: 0.447 | Acc: 83.070% \n","Loss: 0.446 | Acc: 83.086% \n","Loss: 0.448 | Acc: 82.909% \n","Loss: 0.448 | Acc: 82.889% \n","Loss: 0.447 | Acc: 82.944% \n","Loss: 0.448 | Acc: 82.812% \n","Loss: 0.447 | Acc: 82.922% \n","Accuracy of the network on the 1164 test images: 81 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 87%|████████▋ | 26/30 [3:41:21<32:38, 489.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 26\n","Loss: 0.441 | Acc: 81.250% \n","Loss: 0.409 | Acc: 84.375% \n","Loss: 0.448 | Acc: 81.250% \n","Loss: 0.442 | Acc: 82.812% \n","Loss: 0.433 | Acc: 84.375% \n","Loss: 0.451 | Acc: 83.333% \n","Loss: 0.451 | Acc: 83.482% \n","Loss: 0.454 | Acc: 83.203% \n","Loss: 0.451 | Acc: 84.028% \n","Loss: 0.443 | Acc: 85.312% \n","Loss: 0.450 | Acc: 84.659% \n","Loss: 0.452 | Acc: 84.375% \n","Loss: 0.453 | Acc: 84.615% \n","Loss: 0.457 | Acc: 84.598% \n","Loss: 0.454 | Acc: 84.792% \n","Loss: 0.453 | Acc: 84.961% \n","Loss: 0.457 | Acc: 84.559% \n","Loss: 0.458 | Acc: 84.375% \n","Loss: 0.452 | Acc: 84.704% \n","Loss: 0.448 | Acc: 85.000% \n","Loss: 0.446 | Acc: 85.417% \n","Loss: 0.446 | Acc: 85.227% \n","Loss: 0.448 | Acc: 84.918% \n","Loss: 0.449 | Acc: 84.896% \n","Loss: 0.450 | Acc: 84.875% \n","Loss: 0.453 | Acc: 84.495% \n","Loss: 0.452 | Acc: 84.375% \n","Loss: 0.455 | Acc: 83.929% \n","Loss: 0.453 | Acc: 83.944% \n","Loss: 0.453 | Acc: 83.854% \n","Loss: 0.455 | Acc: 83.770% \n","Loss: 0.456 | Acc: 83.594% \n","Loss: 0.454 | Acc: 83.712% \n","Loss: 0.453 | Acc: 83.915% \n","Loss: 0.454 | Acc: 83.482% \n","Loss: 0.454 | Acc: 83.507% \n","Loss: 0.454 | Acc: 83.277% \n","Loss: 0.453 | Acc: 83.306% \n","Loss: 0.453 | Acc: 83.253% \n","Loss: 0.451 | Acc: 83.516% \n","Loss: 0.451 | Acc: 83.384% \n","Loss: 0.454 | Acc: 82.812% \n","Loss: 0.453 | Acc: 82.922% \n","Loss: 0.455 | Acc: 82.670% \n","Loss: 0.454 | Acc: 82.500% \n","Loss: 0.455 | Acc: 82.133% \n","Loss: 0.454 | Acc: 82.247% \n","Loss: 0.454 | Acc: 82.161% \n","Loss: 0.452 | Acc: 82.398% \n","Loss: 0.452 | Acc: 82.500% \n","Loss: 0.451 | Acc: 82.721% \n","Loss: 0.451 | Acc: 82.692% \n","Loss: 0.451 | Acc: 82.842% \n","Loss: 0.453 | Acc: 82.523% \n","Loss: 0.452 | Acc: 82.727% \n","Loss: 0.453 | Acc: 82.645% \n","Loss: 0.452 | Acc: 82.785% \n","Loss: 0.451 | Acc: 82.812% \n","Loss: 0.451 | Acc: 82.892% \n","Loss: 0.452 | Acc: 82.865% \n","Loss: 0.450 | Acc: 82.992% \n","Loss: 0.450 | Acc: 83.065% \n","Loss: 0.451 | Acc: 82.837% \n","Loss: 0.451 | Acc: 82.910% \n","Loss: 0.450 | Acc: 82.885% \n","Loss: 0.450 | Acc: 82.955% \n","Loss: 0.449 | Acc: 83.069% \n","Loss: 0.449 | Acc: 83.088% \n","Loss: 0.448 | Acc: 83.152% \n","Loss: 0.447 | Acc: 83.304% \n","Loss: 0.447 | Acc: 83.407% \n","Loss: 0.446 | Acc: 83.464% \n","Loss: 0.445 | Acc: 83.604% \n","Loss: 0.445 | Acc: 83.615% \n","Loss: 0.444 | Acc: 83.625% \n","Loss: 0.444 | Acc: 83.635% \n","Loss: 0.444 | Acc: 83.644% \n","Loss: 0.444 | Acc: 83.694% \n","Loss: 0.444 | Acc: 83.663% \n","Loss: 0.444 | Acc: 83.633% \n","Loss: 0.444 | Acc: 83.603% \n","Loss: 0.444 | Acc: 83.575% \n","Loss: 0.444 | Acc: 83.584% \n","Loss: 0.444 | Acc: 83.668% \n","Loss: 0.445 | Acc: 83.548% \n","Accuracy of the network on the 1164 test images: 83 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 90%|█████████ | 27/30 [3:49:28<24:26, 488.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 27\n","Loss: 0.467 | Acc: 78.125% \n","Loss: 0.483 | Acc: 73.438% \n","Loss: 0.505 | Acc: 73.958% \n","Loss: 0.494 | Acc: 77.344% \n","Loss: 0.469 | Acc: 80.625% \n","Loss: 0.460 | Acc: 82.812% \n","Loss: 0.467 | Acc: 81.696% \n","Loss: 0.467 | Acc: 80.859% \n","Loss: 0.463 | Acc: 81.597% \n","Loss: 0.461 | Acc: 81.875% \n","Loss: 0.464 | Acc: 81.534% \n","Loss: 0.456 | Acc: 82.552% \n","Loss: 0.453 | Acc: 83.173% \n","Loss: 0.455 | Acc: 83.036% \n","Loss: 0.455 | Acc: 83.333% \n","Loss: 0.453 | Acc: 83.594% \n","Loss: 0.452 | Acc: 83.456% \n","Loss: 0.452 | Acc: 83.507% \n","Loss: 0.451 | Acc: 83.553% \n","Loss: 0.446 | Acc: 83.906% \n","Loss: 0.442 | Acc: 84.226% \n","Loss: 0.441 | Acc: 84.233% \n","Loss: 0.440 | Acc: 84.375% \n","Loss: 0.440 | Acc: 84.505% \n","Loss: 0.439 | Acc: 84.500% \n","Loss: 0.437 | Acc: 84.856% \n","Loss: 0.435 | Acc: 85.301% \n","Loss: 0.435 | Acc: 85.268% \n","Loss: 0.438 | Acc: 85.237% \n","Loss: 0.436 | Acc: 85.521% \n","Loss: 0.433 | Acc: 85.786% \n","Loss: 0.431 | Acc: 85.938% \n","Loss: 0.431 | Acc: 85.701% \n","Loss: 0.430 | Acc: 85.754% \n","Loss: 0.431 | Acc: 85.536% \n","Loss: 0.429 | Acc: 85.764% \n","Loss: 0.431 | Acc: 85.642% \n","Loss: 0.432 | Acc: 85.444% \n","Loss: 0.432 | Acc: 85.256% \n","Loss: 0.432 | Acc: 85.234% \n","Loss: 0.431 | Acc: 85.290% \n","Loss: 0.434 | Acc: 84.821% \n","Loss: 0.434 | Acc: 84.956% \n","Loss: 0.434 | Acc: 84.872% \n","Loss: 0.433 | Acc: 84.931% \n","Loss: 0.435 | Acc: 84.715% \n","Loss: 0.435 | Acc: 84.707% \n","Loss: 0.435 | Acc: 84.505% \n","Loss: 0.435 | Acc: 84.566% \n","Loss: 0.433 | Acc: 84.812% \n","Loss: 0.434 | Acc: 84.804% \n","Loss: 0.434 | Acc: 84.736% \n","Loss: 0.433 | Acc: 84.788% \n","Loss: 0.432 | Acc: 84.664% \n","Loss: 0.433 | Acc: 84.602% \n","Loss: 0.432 | Acc: 84.487% \n","Loss: 0.432 | Acc: 84.594% \n","Loss: 0.432 | Acc: 84.698% \n","Loss: 0.431 | Acc: 84.746% \n","Loss: 0.431 | Acc: 84.792% \n","Loss: 0.433 | Acc: 84.631% \n","Loss: 0.432 | Acc: 84.627% \n","Loss: 0.432 | Acc: 84.772% \n","Loss: 0.433 | Acc: 84.717% \n","Loss: 0.434 | Acc: 84.567% \n","Loss: 0.433 | Acc: 84.612% \n","Loss: 0.433 | Acc: 84.655% \n","Loss: 0.433 | Acc: 84.697% \n","Loss: 0.433 | Acc: 84.692% \n","Loss: 0.433 | Acc: 84.732% \n","Loss: 0.432 | Acc: 84.771% \n","Loss: 0.432 | Acc: 84.766% \n","Loss: 0.431 | Acc: 84.932% \n","Loss: 0.430 | Acc: 85.008% \n","Loss: 0.431 | Acc: 85.000% \n","Loss: 0.431 | Acc: 84.992% \n","Loss: 0.432 | Acc: 84.943% \n","Loss: 0.433 | Acc: 84.856% \n","Loss: 0.433 | Acc: 84.929% \n","Loss: 0.434 | Acc: 84.805% \n","Loss: 0.434 | Acc: 84.838% \n","Loss: 0.434 | Acc: 84.870% \n","Loss: 0.434 | Acc: 84.940% \n","Loss: 0.434 | Acc: 84.821% \n","Loss: 0.434 | Acc: 84.836% \n","Accuracy of the network on the 1164 test images: 83 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 93%|█████████▎| 28/30 [3:57:33<16:15, 487.72s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 28\n","Loss: 0.378 | Acc: 87.500% \n","Loss: 0.391 | Acc: 84.375% \n","Loss: 0.409 | Acc: 84.375% \n","Loss: 0.423 | Acc: 82.812% \n","Loss: 0.426 | Acc: 83.750% \n","Loss: 0.431 | Acc: 83.333% \n","Loss: 0.430 | Acc: 83.929% \n","Loss: 0.428 | Acc: 83.594% \n","Loss: 0.428 | Acc: 84.375% \n","Loss: 0.429 | Acc: 85.000% \n","Loss: 0.432 | Acc: 84.659% \n","Loss: 0.429 | Acc: 85.156% \n","Loss: 0.429 | Acc: 84.615% \n","Loss: 0.436 | Acc: 83.482% \n","Loss: 0.434 | Acc: 83.750% \n","Loss: 0.438 | Acc: 83.008% \n","Loss: 0.439 | Acc: 83.088% \n","Loss: 0.442 | Acc: 83.160% \n","Loss: 0.440 | Acc: 83.224% \n","Loss: 0.438 | Acc: 83.750% \n","Loss: 0.439 | Acc: 83.929% \n","Loss: 0.436 | Acc: 83.949% \n","Loss: 0.437 | Acc: 83.560% \n","Loss: 0.438 | Acc: 83.333% \n","Loss: 0.438 | Acc: 83.375% \n","Loss: 0.438 | Acc: 83.173% \n","Loss: 0.437 | Acc: 82.986% \n","Loss: 0.440 | Acc: 82.812% \n","Loss: 0.442 | Acc: 82.435% \n","Loss: 0.443 | Acc: 82.188% \n","Loss: 0.443 | Acc: 82.157% \n","Loss: 0.443 | Acc: 82.129% \n","Loss: 0.442 | Acc: 82.481% \n","Loss: 0.442 | Acc: 82.629% \n","Loss: 0.442 | Acc: 82.500% \n","Loss: 0.444 | Acc: 82.378% \n","Loss: 0.442 | Acc: 82.517% \n","Loss: 0.443 | Acc: 82.566% \n","Loss: 0.443 | Acc: 82.532% \n","Loss: 0.443 | Acc: 82.656% \n","Loss: 0.443 | Acc: 82.470% \n","Loss: 0.443 | Acc: 82.366% \n","Loss: 0.443 | Acc: 82.413% \n","Loss: 0.443 | Acc: 82.386% \n","Loss: 0.444 | Acc: 82.292% \n","Loss: 0.444 | Acc: 82.473% \n","Loss: 0.444 | Acc: 82.314% \n","Loss: 0.444 | Acc: 82.161% \n","Loss: 0.444 | Acc: 82.143% \n","Loss: 0.443 | Acc: 82.188% \n","Loss: 0.442 | Acc: 82.292% \n","Loss: 0.441 | Acc: 82.512% \n","Loss: 0.441 | Acc: 82.488% \n","Loss: 0.440 | Acc: 82.639% \n","Loss: 0.438 | Acc: 82.841% \n","Loss: 0.439 | Acc: 82.812% \n","Loss: 0.439 | Acc: 82.730% \n","Loss: 0.439 | Acc: 82.705% \n","Loss: 0.437 | Acc: 82.733% \n","Loss: 0.438 | Acc: 82.760% \n","Loss: 0.437 | Acc: 82.889% \n","Loss: 0.437 | Acc: 82.812% \n","Loss: 0.438 | Acc: 82.887% \n","Loss: 0.438 | Acc: 82.910% \n","Loss: 0.438 | Acc: 82.933% \n","Loss: 0.437 | Acc: 83.049% \n","Loss: 0.438 | Acc: 83.069% \n","Loss: 0.439 | Acc: 82.812% \n","Loss: 0.439 | Acc: 82.835% \n","Loss: 0.438 | Acc: 82.946% \n","Loss: 0.439 | Acc: 82.790% \n","Loss: 0.439 | Acc: 82.769% \n","Loss: 0.438 | Acc: 82.791% \n","Loss: 0.439 | Acc: 82.897% \n","Loss: 0.440 | Acc: 82.792% \n","Loss: 0.439 | Acc: 82.936% \n","Loss: 0.440 | Acc: 82.914% \n","Loss: 0.440 | Acc: 82.973% \n","Loss: 0.440 | Acc: 82.991% \n","Loss: 0.439 | Acc: 83.125% \n","Loss: 0.440 | Acc: 83.063% \n","Loss: 0.440 | Acc: 83.041% \n","Loss: 0.440 | Acc: 83.133% \n","Loss: 0.440 | Acc: 83.073% \n","Loss: 0.440 | Acc: 82.996% \n","Accuracy of the network on the 1164 test images: 84 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n"," 97%|█████████▋| 29/30 [4:05:38<08:06, 486.92s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch: 29\n","Loss: 0.392 | Acc: 81.250% \n","Loss: 0.427 | Acc: 85.938% \n","Loss: 0.418 | Acc: 85.417% \n","Loss: 0.419 | Acc: 83.594% \n","Loss: 0.454 | Acc: 81.875% \n","Loss: 0.440 | Acc: 82.292% \n","Loss: 0.429 | Acc: 83.929% \n","Loss: 0.432 | Acc: 83.594% \n","Loss: 0.430 | Acc: 84.375% \n","Loss: 0.427 | Acc: 84.688% \n","Loss: 0.424 | Acc: 85.227% \n","Loss: 0.429 | Acc: 84.115% \n","Loss: 0.429 | Acc: 84.615% \n","Loss: 0.431 | Acc: 83.705% \n","Loss: 0.429 | Acc: 83.958% \n","Loss: 0.424 | Acc: 84.180% \n","Loss: 0.424 | Acc: 84.007% \n","Loss: 0.428 | Acc: 83.854% \n","Loss: 0.432 | Acc: 83.717% \n","Loss: 0.428 | Acc: 84.375% \n","Loss: 0.427 | Acc: 84.375% \n","Loss: 0.428 | Acc: 84.233% \n","Loss: 0.428 | Acc: 84.375% \n","Loss: 0.427 | Acc: 84.505% \n","Loss: 0.428 | Acc: 84.500% \n","Loss: 0.424 | Acc: 84.976% \n","Loss: 0.425 | Acc: 84.954% \n","Loss: 0.423 | Acc: 85.491% \n","Loss: 0.422 | Acc: 85.560% \n","Loss: 0.422 | Acc: 85.312% \n","Loss: 0.420 | Acc: 85.484% \n","Loss: 0.421 | Acc: 85.156% \n","Loss: 0.420 | Acc: 85.417% \n","Loss: 0.422 | Acc: 85.294% \n","Loss: 0.421 | Acc: 85.357% \n","Loss: 0.423 | Acc: 85.243% \n","Loss: 0.422 | Acc: 85.389% \n","Loss: 0.422 | Acc: 85.444% \n","Loss: 0.424 | Acc: 85.417% \n","Loss: 0.424 | Acc: 85.234% \n","Loss: 0.423 | Acc: 85.290% \n","Loss: 0.423 | Acc: 85.268% \n","Loss: 0.423 | Acc: 85.247% \n","Loss: 0.423 | Acc: 85.298% \n","Loss: 0.423 | Acc: 85.208% \n","Loss: 0.422 | Acc: 85.122% \n","Loss: 0.422 | Acc: 85.173% \n","Loss: 0.423 | Acc: 84.896% \n","Loss: 0.422 | Acc: 85.013% \n","Loss: 0.421 | Acc: 85.125% \n","Loss: 0.420 | Acc: 85.233% \n","Loss: 0.420 | Acc: 85.337% \n","Loss: 0.421 | Acc: 85.142% \n","Loss: 0.422 | Acc: 84.954% \n","Loss: 0.423 | Acc: 84.773% \n","Loss: 0.425 | Acc: 84.598% \n","Loss: 0.428 | Acc: 84.211% \n","Loss: 0.428 | Acc: 84.052% \n","Loss: 0.427 | Acc: 84.269% \n","Loss: 0.427 | Acc: 84.271% \n","Loss: 0.427 | Acc: 84.273% \n","Loss: 0.426 | Acc: 84.425% \n","Loss: 0.427 | Acc: 84.375% \n","Loss: 0.427 | Acc: 84.326% \n","Loss: 0.428 | Acc: 84.231% \n","Loss: 0.429 | Acc: 84.091% \n","Loss: 0.429 | Acc: 84.142% \n","Loss: 0.429 | Acc: 84.145% \n","Loss: 0.428 | Acc: 84.284% \n","Loss: 0.428 | Acc: 84.375% \n","Loss: 0.428 | Acc: 84.155% \n","Loss: 0.427 | Acc: 84.288% \n","Loss: 0.427 | Acc: 84.289% \n","Loss: 0.428 | Acc: 84.164% \n","Loss: 0.428 | Acc: 84.167% \n","Loss: 0.428 | Acc: 84.087% \n","Loss: 0.428 | Acc: 84.091% \n","Loss: 0.429 | Acc: 84.014% \n","Loss: 0.429 | Acc: 84.059% \n","Loss: 0.428 | Acc: 84.258% \n","Loss: 0.427 | Acc: 84.336% \n","Loss: 0.427 | Acc: 84.223% \n","Loss: 0.428 | Acc: 84.224% \n","Loss: 0.428 | Acc: 84.189% \n","Loss: 0.428 | Acc: 84.247% \n","Accuracy of the network on the 1164 test images: 86 %\n","saved model ./outputs\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n","100%|██████████| 30/30 [4:13:44<00:00, 507.48s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"GR8gh-968nYB"},"source":[""],"execution_count":null,"outputs":[]}]}