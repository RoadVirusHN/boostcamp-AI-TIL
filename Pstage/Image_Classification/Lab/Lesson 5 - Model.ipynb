{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "found-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-morocco",
   "metadata": {},
   "source": [
    "## Lesson 5 - Model\n",
    " - 이번 실습 자료에서는 강의시간에 다루었던 파이토치 모델을 정의하는 방법에 대해 실습하겠습니다.\n",
    " - 파이토치 모델은 기본적으로 `nn.Module` 클래스를 상속하여 사용합니다.\n",
    "     - [공식문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)에 따르면 `nn.Module` 은 다음과 같은 기능을 합니다\n",
    "     ```\n",
    "     Base class for all neural network modules.\n",
    "     Your models should also subclass this class.\n",
    "     Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "impaired-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "practical-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-administration",
   "metadata": {},
   "source": [
    "### 모델 디버깅\n",
    " - 파이토치 모델들은 다음과 같읕 방법들을 통해 파라미터를 눈으로 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "compressed-march",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight         - size: torch.Size([3, 1, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[-0.3075, -0.1211,  0.2804],\n",
      "          [ 0.0672,  0.1570,  0.1869],\n",
      "          [ 0.3014, -0.3113,  0.2751]]],\n",
      "\n",
      "\n",
      "        [[[-0.0343,  0.1386,  0.2855],\n",
      "          [-0.1404, -0.2366,  0.2645],\n",
      "          [ 0.0988, -0.3201,  0.1348]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0911, -0.2588, -0.2141],\n",
      "          [ 0.1516,  0.2519, -0.1032],\n",
      "          [-0.0731, -0.2122, -0.0473]]]], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "conv1.bias           - size: torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([-0.0271, -0.1532,  0.1104], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "bn1.weight           - size: torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "bn1.bias             - size: torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "conv2.weight         - size: torch.Size([5, 3, 3, 3])\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0192,  0.0430,  0.1560],\n",
      "          [ 0.1750, -0.1111,  0.0899],\n",
      "          [-0.0983, -0.1136,  0.0906]],\n",
      "\n",
      "         [[-0.1076,  0.0908,  0.0726],\n",
      "          [-0.0873, -0.0415,  0.0167],\n",
      "          [-0.1352, -0.1738,  0.0183]],\n",
      "\n",
      "         [[-0.1782,  0.1501,  0.0157],\n",
      "          [-0.0850,  0.1876,  0.1851],\n",
      "          [-0.0584,  0.1445,  0.0163]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1683,  0.1716,  0.0950],\n",
      "          [ 0.1135,  0.0995,  0.0496],\n",
      "          [ 0.0874, -0.0718,  0.1001]],\n",
      "\n",
      "         [[-0.0570,  0.1192,  0.0754],\n",
      "          [ 0.1335,  0.1670, -0.0067],\n",
      "          [ 0.0036,  0.0019, -0.0030]],\n",
      "\n",
      "         [[-0.0198,  0.0023,  0.1026],\n",
      "          [ 0.0156,  0.0831, -0.1372],\n",
      "          [ 0.0823, -0.0319, -0.0447]]],\n",
      "\n",
      "\n",
      "        [[[-0.0440,  0.1876, -0.0753],\n",
      "          [ 0.0419,  0.1471, -0.0799],\n",
      "          [-0.1395,  0.1831,  0.1885]],\n",
      "\n",
      "         [[ 0.0658, -0.1506, -0.0126],\n",
      "          [ 0.1484,  0.1667, -0.1363],\n",
      "          [-0.0593, -0.0022,  0.0088]],\n",
      "\n",
      "         [[-0.0998, -0.1478, -0.0097],\n",
      "          [-0.1836, -0.0277,  0.0828],\n",
      "          [ 0.0172, -0.1696,  0.0569]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0229, -0.1462,  0.0144],\n",
      "          [ 0.0106, -0.0873, -0.1388],\n",
      "          [-0.1228,  0.1097, -0.1374]],\n",
      "\n",
      "         [[ 0.0472,  0.1697, -0.1618],\n",
      "          [-0.1204, -0.0874,  0.0043],\n",
      "          [-0.0697, -0.1920,  0.0995]],\n",
      "\n",
      "         [[ 0.0430, -0.0846,  0.1823],\n",
      "          [ 0.0452, -0.1833,  0.1077],\n",
      "          [-0.0880, -0.1865, -0.1718]]],\n",
      "\n",
      "\n",
      "        [[[-0.1191,  0.0999, -0.0251],\n",
      "          [-0.0673, -0.1422,  0.1671],\n",
      "          [ 0.0421,  0.0045,  0.0173]],\n",
      "\n",
      "         [[-0.1536,  0.0295,  0.0453],\n",
      "          [-0.0847, -0.1281, -0.0127],\n",
      "          [-0.0030,  0.1844,  0.1446]],\n",
      "\n",
      "         [[ 0.1007,  0.1908,  0.0434],\n",
      "          [-0.0968,  0.0997,  0.1359],\n",
      "          [-0.0105, -0.0676,  0.1189]]]], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. using named_parameters()\n",
    "for param, weight in model.named_parameters():\n",
    "    print(f\"{param:20} - size: {weight.size()}\")\n",
    "    print(weight)\n",
    "    print(\"-\" * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "precious-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.3075, -0.1211,  0.2804],\n",
      "          [ 0.0672,  0.1570,  0.1869],\n",
      "          [ 0.3014, -0.3113,  0.2751]]],\n",
      "\n",
      "\n",
      "        [[[-0.0343,  0.1386,  0.2855],\n",
      "          [-0.1404, -0.2366,  0.2645],\n",
      "          [ 0.0988, -0.3201,  0.1348]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0911, -0.2588, -0.2141],\n",
      "          [ 0.1516,  0.2519, -0.1032],\n",
      "          [-0.0731, -0.2122, -0.0473]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0271, -0.1532,  0.1104], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 2. directly access with member variable\n",
    "print(model.conv1.weight)\n",
    "print(model.conv1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-brighton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "native-forwarding",
   "metadata": {},
   "source": [
    "### 학습된 모델 저장하기\n",
    " - `torch.save(model.state_dict(), save_path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaning-sodium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saving success at ./runs/best.pth\n",
      "Saved models : ['best.pth']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_folder = \"./runs/\"\n",
    "save_path = os.path.join(save_folder, \"best.pth\")   # ./runs/best.pth\n",
    "os.makedirs(save_folder, exist_ok=True)  \n",
    "\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saving success at {save_path}\")\n",
    "print(f\"Saved models : {os.listdir(save_folder)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-dublin",
   "metadata": {},
   "source": [
    "### 저장된 모델 불러오기\n",
    " - model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "positive-concrete",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading success from ./runs/best.pth\n"
     ]
    }
   ],
   "source": [
    "new_model = Model()\n",
    "new_model.load_state_dict(torch.load(save_path))\n",
    "print(f\"Model loading success from {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-banana",
   "metadata": {},
   "source": [
    "#### 저장된 모델이 잘 불러와졌는지 확인해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "active-battlefield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter conv1.weight    from trained model and loaded model is equal? -> True\n",
      "parameter conv1.bias      from trained model and loaded model is equal? -> True\n",
      "parameter bn1.weight      from trained model and loaded model is equal? -> True\n",
      "parameter bn1.bias        from trained model and loaded model is equal? -> True\n",
      "parameter conv2.weight    from trained model and loaded model is equal? -> True\n"
     ]
    }
   ],
   "source": [
    "for (name, trained_weight), (_, saved_weight) in zip(model.named_parameters(), new_model.named_parameters()):\n",
    "    is_equal = torch.equal(trained_weight, saved_weight)\n",
    "    print(f\"parameter {name:15} from trained model and loaded model is equal? -> {is_equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-twenty",
   "metadata": {},
   "source": [
    "#### state_dict() 이 무엇인가요?\n",
    " - 모델의 저장과 로딩에 `state_dict()` 을 사용하는데, 기능이 무엇인가요?\n",
    " - 기본적으로 위에서 살펴본 `.named_parameters()` 와 매우 유사합니다\n",
    " - model parameter 를 Key 로 가지고, model weights 를 Value 로 가지는 파이썬 딕셔너리일 뿐입니다. \n",
    "   (정확한 Type 은 파이썬 내장 라이브러리 collections.OrderDict 입니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adverse-butler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight         - size: torch.Size([3, 1, 3, 3])\n",
      "tensor([[[[-0.3075, -0.1211,  0.2804],\n",
      "          [ 0.0672,  0.1570,  0.1869],\n",
      "          [ 0.3014, -0.3113,  0.2751]]],\n",
      "\n",
      "\n",
      "        [[[-0.0343,  0.1386,  0.2855],\n",
      "          [-0.1404, -0.2366,  0.2645],\n",
      "          [ 0.0988, -0.3201,  0.1348]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0911, -0.2588, -0.2141],\n",
      "          [ 0.1516,  0.2519, -0.1032],\n",
      "          [-0.0731, -0.2122, -0.0473]]]])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "conv1.bias           - size: torch.Size([3])\n",
      "tensor([-0.0271, -0.1532,  0.1104])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.weight           - size: torch.Size([3])\n",
      "tensor([1., 1., 1.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.bias             - size: torch.Size([3])\n",
      "tensor([0., 0., 0.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.running_mean     - size: torch.Size([3])\n",
      "tensor([0., 0., 0.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.running_var      - size: torch.Size([3])\n",
      "tensor([1., 1., 1.])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "bn1.num_batches_tracked - size: torch.Size([])\n",
      "tensor(0)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "conv2.weight         - size: torch.Size([5, 3, 3, 3])\n",
      "tensor([[[[-0.0192,  0.0430,  0.1560],\n",
      "          [ 0.1750, -0.1111,  0.0899],\n",
      "          [-0.0983, -0.1136,  0.0906]],\n",
      "\n",
      "         [[-0.1076,  0.0908,  0.0726],\n",
      "          [-0.0873, -0.0415,  0.0167],\n",
      "          [-0.1352, -0.1738,  0.0183]],\n",
      "\n",
      "         [[-0.1782,  0.1501,  0.0157],\n",
      "          [-0.0850,  0.1876,  0.1851],\n",
      "          [-0.0584,  0.1445,  0.0163]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1683,  0.1716,  0.0950],\n",
      "          [ 0.1135,  0.0995,  0.0496],\n",
      "          [ 0.0874, -0.0718,  0.1001]],\n",
      "\n",
      "         [[-0.0570,  0.1192,  0.0754],\n",
      "          [ 0.1335,  0.1670, -0.0067],\n",
      "          [ 0.0036,  0.0019, -0.0030]],\n",
      "\n",
      "         [[-0.0198,  0.0023,  0.1026],\n",
      "          [ 0.0156,  0.0831, -0.1372],\n",
      "          [ 0.0823, -0.0319, -0.0447]]],\n",
      "\n",
      "\n",
      "        [[[-0.0440,  0.1876, -0.0753],\n",
      "          [ 0.0419,  0.1471, -0.0799],\n",
      "          [-0.1395,  0.1831,  0.1885]],\n",
      "\n",
      "         [[ 0.0658, -0.1506, -0.0126],\n",
      "          [ 0.1484,  0.1667, -0.1363],\n",
      "          [-0.0593, -0.0022,  0.0088]],\n",
      "\n",
      "         [[-0.0998, -0.1478, -0.0097],\n",
      "          [-0.1836, -0.0277,  0.0828],\n",
      "          [ 0.0172, -0.1696,  0.0569]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0229, -0.1462,  0.0144],\n",
      "          [ 0.0106, -0.0873, -0.1388],\n",
      "          [-0.1228,  0.1097, -0.1374]],\n",
      "\n",
      "         [[ 0.0472,  0.1697, -0.1618],\n",
      "          [-0.1204, -0.0874,  0.0043],\n",
      "          [-0.0697, -0.1920,  0.0995]],\n",
      "\n",
      "         [[ 0.0430, -0.0846,  0.1823],\n",
      "          [ 0.0452, -0.1833,  0.1077],\n",
      "          [-0.0880, -0.1865, -0.1718]]],\n",
      "\n",
      "\n",
      "        [[[-0.1191,  0.0999, -0.0251],\n",
      "          [-0.0673, -0.1422,  0.1671],\n",
      "          [ 0.0421,  0.0045,  0.0173]],\n",
      "\n",
      "         [[-0.1536,  0.0295,  0.0453],\n",
      "          [-0.0847, -0.1281, -0.0127],\n",
      "          [-0.0030,  0.1844,  0.1446]],\n",
      "\n",
      "         [[ 0.1007,  0.1908,  0.0434],\n",
      "          [-0.0968,  0.0997,  0.1359],\n",
      "          [-0.0105, -0.0676,  0.1189]]]])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for param, weight in model.state_dict().items():\n",
    "    print(f\"{param:20} - size: {weight.size()}\")\n",
    "    print(weight)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moderate-romantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.state_dict() type is : <class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "print(f\"model.state_dict() type is : {type(model.state_dict())}\")\n",
    "type(model.state_dict()) == OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-consent",
   "metadata": {},
   "source": [
    "#### `named_parameters()` 을 안쓰고 `state_dict()` 을 사용하는 이유가 무언인가요? (둘이 뭐가 다른가요)\n",
    " - `named_parameters()` : returns only parameters\n",
    " - `state_dict()`: returns both parameters and buffers (e.g. BN runnin_mean, running_var)\n",
    " \n",
    " [Reference](https://stackoverflow.com/a/54747245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "liable-glenn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1.weight', 'conv1.bias', 'bn1.weight', 'bn1.bias', 'conv2.weight']\n",
      "\n",
      "['conv1.weight',\n",
      " 'conv1.bias',\n",
      " 'bn1.weight',\n",
      " 'bn1.bias',\n",
      " 'bn1.running_mean',\n",
      " 'bn1.running_var',\n",
      " 'bn1.num_batches_tracked',\n",
      " 'conv2.weight']\n"
     ]
    }
   ],
   "source": [
    "pprint([name for (name, param) in model.named_parameters()])  # named_parameters() : returns only parameters\n",
    "print()\n",
    "pprint(list(model.state_dict().keys()))  # state_dict(): retuns both parameters and buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-wilson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "announced-settle",
   "metadata": {},
   "source": [
    "### CPU vs GPU\n",
    " - DL 모델은 다양한 프로세서(CPU, GPU, TPU) 를 사용하여 학습을 할 수 있습니다.\n",
    " - 따라서, 특정 프로세서에서 학습을 진행하고 싶은 경우 명시적으로 지정해주어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-clinic",
   "metadata": {},
   "source": [
    "#### cpu()\n",
    "Moves all model parameters and buffers to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "future-prophet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "for weight in model.parameters():\n",
    "    print(f\"model device: {weight.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-wildlife",
   "metadata": {},
   "source": [
    "#### cuda()\n",
    "Moves all model parameters and buffers to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dynamic-treatment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "for weight in model.parameters():\n",
    "    print(f\"model device: {weight.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-seafood",
   "metadata": {},
   "source": [
    "#### to()\n",
    "Moves and/or casts the parameters and buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "first-afternoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set model device to cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "model device: cpu\n",
      "\n",
      "Set model device to cuda\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "model device: cuda:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device_options = ['cpu', 'cuda']\n",
    "for device_option in device_options:\n",
    "    device = torch.device(device_option)\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Set model device to {device_option}\")\n",
    "    for weight in model.parameters():\n",
    "        print(f\"model device: {weight.device}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-strand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "authorized-satisfaction",
   "metadata": {},
   "source": [
    "### forward\n",
    " - nn.Module 을 상속한 객체를 직접 호출할 때 수행하는 연산을 정의합니다.\n",
    " - `model(input)` 을 통해 모델의 예측값을 계산할 수 있습니다.\n",
    " - Defines the computation performed at every call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "occasional-elizabeth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output: torch.Size([1, 5, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2637, 0.0000, 0.0480, 0.0925, 0.0913, 0.1502, 0.5207, 0.0000],\n",
       "          [0.4928, 0.0000, 0.2912, 0.3257, 0.0000, 0.2229, 0.3872, 0.0000],\n",
       "          [0.5168, 0.0583, 0.0000, 0.4722, 0.0000, 0.5150, 0.4291, 0.0000],\n",
       "          [0.5005, 0.0000, 0.0122, 0.2355, 0.1027, 0.0208, 0.0000, 0.3233],\n",
       "          [0.4526, 0.4850, 0.0000, 0.0000, 0.0000, 0.0000, 0.7164, 0.2416],\n",
       "          [0.7197, 0.4460, 0.3492, 0.0000, 0.0000, 0.5701, 0.6689, 0.0000],\n",
       "          [0.0457, 0.0000, 0.3621, 0.0000, 0.0000, 0.2412, 0.8430, 0.8527],\n",
       "          [0.0000, 0.0000, 0.8859, 0.2283, 0.0000, 0.0281, 0.8295, 0.3793]],\n",
       "\n",
       "         [[0.5273, 0.7732, 0.7192, 0.6034, 0.2004, 0.2880, 0.3503, 0.3437],\n",
       "          [0.4659, 0.7963, 0.6939, 0.7552, 0.1433, 0.1198, 0.5264, 0.7004],\n",
       "          [0.5375, 0.3246, 0.4418, 0.4889, 0.5910, 0.6967, 0.3209, 0.6398],\n",
       "          [0.1876, 0.3465, 0.1148, 0.6569, 0.8055, 0.2917, 0.2914, 0.8800],\n",
       "          [0.3019, 0.4247, 1.0522, 0.3664, 0.3435, 0.5519, 1.0127, 0.5991],\n",
       "          [0.1429, 0.4592, 0.7553, 0.2548, 1.0697, 0.7961, 0.6253, 1.0349],\n",
       "          [0.4979, 0.0740, 0.4090, 1.5217, 1.2441, 0.1257, 0.4393, 0.4245],\n",
       "          [0.2336, 0.1966, 0.7940, 1.3113, 0.9604, 0.5891, 0.0721, 0.6549]],\n",
       "\n",
       "         [[0.1793, 0.3879, 0.3664, 0.0871, 0.0000, 0.0000, 0.0000, 0.4282],\n",
       "          [0.0000, 0.4291, 0.0000, 0.8412, 0.5586, 0.0000, 0.0000, 0.5605],\n",
       "          [0.0000, 0.0000, 0.0481, 0.0000, 0.6379, 0.1348, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.3061, 0.0000, 0.2169, 0.4531, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.4403, 0.5964, 0.0000, 0.0000, 0.5081, 0.6322],\n",
       "          [0.0000, 0.0000, 0.0000, 0.7748, 0.0000, 0.6885, 0.0000, 0.4130],\n",
       "          [0.0000, 0.0000, 0.0000, 1.1728, 0.6202, 0.0507, 0.0701, 0.0000],\n",
       "          [0.2171, 0.0000, 0.0000, 0.3220, 0.6453, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.1319, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0924, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.1660, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.1552, 0.2057, 0.4117, 0.7309, 0.5180, 0.0565, 0.3622, 0.1134],\n",
       "          [0.1429, 0.0000, 0.0000, 0.2185, 0.7093, 0.5231, 0.3456, 0.0000],\n",
       "          [0.3704, 0.2073, 0.1811, 0.2577, 0.0000, 0.4173, 0.6463, 0.4422],\n",
       "          [0.4408, 0.3946, 0.4738, 0.2613, 0.0954, 0.1242, 0.7737, 0.0000],\n",
       "          [0.1316, 0.1031, 0.0000, 0.3437, 0.5363, 0.0000, 0.1677, 0.6872],\n",
       "          [0.3295, 0.2124, 0.6342, 0.0000, 0.0000, 0.1681, 0.0338, 0.0000],\n",
       "          [0.6041, 0.6750, 0.6895, 0.0000, 0.0000, 0.3596, 0.3729, 0.4284],\n",
       "          [0.1188, 0.3089, 0.4313, 0.5930, 0.0000, 0.0686, 0.8062, 0.6278]]]],\n",
       "       device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 1, 12, 12).to(device)\n",
    "model.to(device)\n",
    "output = model(dummy_input)\n",
    "print(f\"model output: {output.size()}\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-southeast",
   "metadata": {},
   "source": [
    "#### Cautions\n",
    " - 모델과 인풋의 device 는 반드시 같아야 합니다.\n",
    " - 그렇지 않으면 (Runtime) Error 가 발생합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "informative-russia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output: torch.Size([1, 5, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "cpu_device = torch.device('cpu')\n",
    "gpu_device = torch.device('cuda')\n",
    "\n",
    "# device is same\n",
    "dummy_input = dummy_input.to(gpu_device)\n",
    "model.to(gpu_device)\n",
    "output = model(dummy_input)  # Fine \n",
    "print(f\"model output: {output.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "looking-document",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-52d7e79efd68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# device is different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# raise Error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"model output: {output.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5f680354f5ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "dummy_input = dummy_input.to(cpu_device)\n",
    "model.to(gpu_device)\n",
    "\n",
    "# device is different\n",
    "# RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\n",
    "output = model(dummy_input)  # raise Error\n",
    "print(f\"model output: {output.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-density",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "collective-highland",
   "metadata": {},
   "source": [
    "### requires_grad()\n",
    " - autograd 가 해당 모델의 연산을 기록할지를 결정합니다\n",
    " - false 일 시, 수행하는 연산을 기록하지 않고 따라서 역전파가 되지 않아 학습에서 제외됩니다.\n",
    " - Change if autograd should record operations on parameters in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad = False\n",
    "model.requires_grad_(requires_grad=False)\n",
    "for param, weight in model.named_parameters():\n",
    "    print(f\"param {param:15} required gradient? -> {weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires_grad = True\n",
    "model.requires_grad_(requires_grad=True)\n",
    "for param, weight in model.named_parameters():\n",
    "    print(f\"param {param:15} required gradient? -> {weight.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "entire-commercial",
   "metadata": {},
   "source": [
    "### train(), eval()\n",
    " - 모델을 training(evaluation) 모드로 전환합니다.\n",
    " - training 과 evaluation 이 다르게 작용하는 모듈들(Dropout, BatchNorm) 에 영향을 줍니다.\n",
    " - 학습 단계에서는 training 모드로, 인퍼런스 단계에서는 eval 모드로 전환해주어야 합니다.\n",
    " - [아래](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py#L111-L118)는 BatchNorm2d 의 파이토치 구현입니다. `self.training=True` 일 경우에만, `running_mean`, `running_var` 을 tracking 합니다.\n",
    " \n",
    "```\n",
    "if self.training and self.track_running_stats:\n",
    "    # TODO: if statement only here to tell the jit to skip emitting this when it is None\n",
    "    if self.num_batches_tracked is not None:\n",
    "        self.num_batches_tracked = self.num_batches_tracked + 1\n",
    "        if self.momentum is None:  # use cumulative moving average\n",
    "            exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
    "        else:  # use exponential moving average\n",
    "            exponential_average_factor = self.momentum\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lucky-bracelet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.bn1.training: True\n"
     ]
    }
   ],
   "source": [
    "model.train()  # set model to train mode\n",
    "print(f\"model.bn1.training: {model.bn1.training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "realistic-demonstration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.bn1.training: False\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # set model to eval mode\n",
    "print(f\"model.bn1.training: {model.bn1.training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-musical",
   "metadata": {},
   "source": [
    "### 파이토치 공식 문서에서 nn.Module 에 관한 더 많은 정보를 얻을 수 있습니다.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "\n",
    "궁금증이 생기면 공식 문서를 참고하는걸 강력 추천합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
