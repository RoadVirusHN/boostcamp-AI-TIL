{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"torch.autograd.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNAc5i9Uvm1VEdx2btsl7L6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"YoRpR1fcm0V-","executionInfo":{"status":"ok","timestamp":1615391513463,"user_tz":-540,"elapsed":1021,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["import torch\r\n","from torch import nn \r\n","from torch.nn import functional as F"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zInhnEaanEbV","executionInfo":{"status":"ok","timestamp":1615387834746,"user_tz":-540,"elapsed":868,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"1c246506-d741-49de-9449-b1a760036df7"},"source":["x = torch.randn(2, requires_grad=True) # requires_grad=True를 해야 gradient를 저장하여 사용 가능\r\n","y = x*3\r\n","gradients = torch.tensor([100, 0.1], dtype=torch.float)\r\n","y.backward(gradients)\r\n","print(x.grad)# (dy/dx)*gradient 출력\r\n","# 굳이 주어진 gradients값이 곱해 출력하는 이유는, gradient가 또다른 gradient일 때, chain rule을 만족시키기 위해\r\n","# 필요없다면 1을 넣어주면 된다."],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensor([300.0000,   0.3000])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"-mrMUz86pJf_","executionInfo":{"status":"error","timestamp":1615388282066,"user_tz":-540,"elapsed":866,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"63950391-9dd5-4e67-a29e-6d9c3acfdd0d"},"source":["# requires_grad=False의 예제\r\n","x = torch.randn(2, requires_grad=False) # requires_grad=True를 해야 gradient를 저장하여 사용 가능\r\n","y = x*3\r\n","gradients = torch.tensor([100, 0.1], dtype=torch.float)\r\n","y.backward(gradients)\r\n","print(x.grad)"],"execution_count":5,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-2c97ebdb1611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"nONB_YNzq2e3","executionInfo":{"status":"error","timestamp":1615388722033,"user_tz":-540,"elapsed":844,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"d038395f-a43c-444c-d461-56371f897bc4"},"source":["x = torch.randn(2, requires_grad=True)\r\n","y = x*3\r\n","gradients = torch.tensor([100, 0.1], dtype=torch.float)\r\n","y.backward(gradients, retain_graph= False) # False 일 경우 ErrorThrow\r\n","print(x.grad) \r\n","y.backward(gradients) # 2번 미분해야하는 경우 retain_graph=True, 번 호출 시, 미분 값이 더해져 중첩됨 \r\n","print(x.grad)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["tensor([300.0000,   0.3000])\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-e9a840e81a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# False 일 경우 ErrorThrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2번 미분해야하는 경우 retain_graph=True, 번 호출 시, 미분 값이 더해져 중첩됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WeBHutYapVbW","executionInfo":{"status":"ok","timestamp":1615388690205,"user_tz":-540,"elapsed":842,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"567721f0-c5a1-46bf-862c-1b6141cbf001"},"source":["x = torch.randn(2, requires_grad=True)\r\n","y = x*3\r\n","gradients = torch.tensor([100, 0.1], dtype=torch.float)\r\n","y.backward(gradients, retain_graph= True) # retain_graph=True가 아니면, backward하고 난 뒤, 최적화를 위해 graph를 지워버림(종말단의 gradient만 필요하므로)\r\n","print(x.grad) \r\n","y.backward(gradients) # 2번 미분해야하는 경우 retain_graph=True,여러번 호출 시, 미분 값이 더해져 중첩됨 \r\n","print(x.grad)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tensor([300.0000,   0.3000])\n","tensor([600.0000,   0.6000])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FqqU8PJrDY-","executionInfo":{"status":"ok","timestamp":1615388957807,"user_tz":-540,"elapsed":1167,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"8cf73d8a-1b35-417d-f63b-40b9627c2f57"},"source":["x = torch.randn(2, requires_grad=True)\r\n","y = x * 3\r\n","z = x / 2\r\n","w = x + y \r\n","# 각기 다른 computation graph 가짐\r\n","print(w,y,z,sep='\\n') \r\n","print(w.grad_fn,y.grad_fn,z.grad_fn,sep='\\n') \r\n","# grad_fn attr을 통하여 호출할 backpropagation class를 알 수 있다"],"execution_count":15,"outputs":[{"output_type":"stream","text":["tensor([-0.4437, -1.3125], grad_fn=<AddBackward0>)\n","tensor([-0.3328, -0.9844], grad_fn=<MulBackward0>)\n","tensor([-0.0555, -0.1641], grad_fn=<DivBackward0>)\n","<AddBackward0 object at 0x7f1db82fbc50>\n","<MulBackward0 object at 0x7f1dbd2ee150>\n","<DivBackward0 object at 0x7f1db8bb4390>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hj0RQXOQtjKf","executionInfo":{"status":"ok","timestamp":1615391373801,"user_tz":-540,"elapsed":853,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["# CAM을 이용한 CNN visualization 등에서 중간층 Layer의 Gradient값이 필요할 때가 있다.\r\n","# hook을 사용하여 중간의 gradient 값을 저장해놓을 수 있다.\r\n","# hooking : 양 객체 사이의 통신내용(메세지, 값, 이벤트, gradient 등)을 가져오는 것\r\n","\r\n","# register_hook(hook) : registers a backward hook\r\n","# register_forward_hook(hook) : registers a forward hook\r\n","class SimpleNet(nn.Module):# hook을 사용해보기위한 평범한 NN\r\n","  def __init__(self):\r\n","    super(SimpleNet, self).__init__()\r\n","    self.conv1 = nn.Conv2d(1, 10, 5)\r\n","    self.pool1 = nn.MaxPool2d(2, 2)\r\n","    self.conv2 = nn.Conv2d(10, 20, 5)\r\n","    self.pool2 = nn.MaxPool2d(2,2)\r\n","    self.fc = nn.Linear(320, 50)\r\n","    self.out = nn.Linear(50, 10)\r\n","\r\n","  def forward(self, input):\r\n","    x = self.pool1(F.relu(self.conv1(input)))\r\n","    x = self.pool2(F.relu(self.conv2(x)))\r\n","    x = x.view(x.size(0),-1)\r\n","    x = F.relu(self.fc(x))\r\n","    x = F.relu(self.out(x))\r\n","    return x"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"cWlQRtL70sPY","executionInfo":{"status":"ok","timestamp":1615391375518,"user_tz":-540,"elapsed":824,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}}},"source":["# hook을 사용하기 위해...\r\n","# 1. Hook의 signature 정의\r\n","# hook이 실행됬을 때 해야할 활동들을 정의\r\n","# prototype에 맞게 해줘야함, self, input, output이 argument\r\n","def hook_func(self, input, output):\r\n","  # type(self) should be tensor\r\n","  print('Inside ' + self.__class__.__name__ + ' forward')\r\n","  print('')\r\n","  print('input: ', type(input))\r\n","  print('input[0]: ', type(input[0]))\r\n","  print('output: ', type(output))\r\n","  print('')\r\n"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKWfcTMox-8_","executionInfo":{"status":"ok","timestamp":1615393378653,"user_tz":-540,"elapsed":1135,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"311a9f7b-9127-4f8a-abfc-160145dd35d7"},"source":["net = SimpleNet()\r\n","h = net.conv1.register_forward_hook(hook_func) # 1번째 layer에 forward 시 hook 실행 등록\r\n","print(h)\r\n","print(net.conv2.register_forward_hook(hook_func)) # 2번째 layer에forward 시 hook 실행 등록"],"execution_count":61,"outputs":[{"output_type":"stream","text":["<torch.utils.hooks.RemovableHandle object at 0x7f1db8206810>\n","<torch.utils.hooks.RemovableHandle object at 0x7f1db806f890>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HDMHJOXb1OWB","executionInfo":{"status":"ok","timestamp":1615393378654,"user_tz":-540,"elapsed":802,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"edf09867-282c-44ea-ee57-e6f37581d82f"},"source":["input = torch.randn(1, 1, 28, 28)\r\n","out = net(input) # forward 시 hook function 실행되어 print 됨"],"execution_count":62,"outputs":[{"output_type":"stream","text":["Inside Conv2d forward\n","\n","input:  <class 'tuple'>\n","input[0]:  <class 'torch.Tensor'>\n","output:  <class 'torch.Tensor'>\n","\n","Inside Conv2d forward\n","\n","input:  <class 'tuple'>\n","input[0]:  <class 'torch.Tensor'>\n","output:  <class 'torch.Tensor'>\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8P4Yc9b8j05","executionInfo":{"status":"ok","timestamp":1615393379828,"user_tz":-540,"elapsed":836,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"9e61fa7a-49b7-4d08-ae0d-ea07403d4217"},"source":["h.remove() # register 지우기, 1번째 layer의 register return(=handler)을 받아 remove() 함수로 지움\r\n","out = net(input) # 2번째 layer hook만 print됨"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Inside Conv2d forward\n","\n","input:  <class 'tuple'>\n","input[0]:  <class 'torch.Tensor'>\n","output:  <class 'torch.Tensor'>\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DuPXzcvD2wKA","executionInfo":{"status":"ok","timestamp":1615392527357,"user_tz":-540,"elapsed":815,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"fe2ca6a1-3c2a-4632-9bab-cae3360652ea"},"source":["# register_forward_hook(hook_pre) # layer의 forwar_pass 이전에 호줄되는 함수\r\n","def hook_pre(self, input, output):\r\n","  print('Inside ' + self.__class__.__name__ + ' forward')\r\n","  print('')\r\n","  print('input: ', type(input))\r\n","  print('input[0]: ', type(input[0]))\r\n","\r\n","net = SimpleNet()\r\n","net.conv1.register_forward_hook(hook_pre) # layer의 forwar_pass 이전에 호줄되는 함수\r\n","\r\n","input = torch.randn(1, 1, 28, 28)\r\n","out = net(input)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Inside Conv2d forward\n","\n","input:  <class 'tuple'>\n","input[0]:  <class 'torch.Tensor'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qz8bHl6E5ocw","executionInfo":{"status":"ok","timestamp":1615393211806,"user_tz":-540,"elapsed":882,"user":{"displayName":"윤준석","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjCmI1sT1yHDMSiz3v1SLvL13Y6JDuE9ORflX7O=s64","userId":"08731269143009806715"}},"outputId":"4505ad90-94c5-4d32-cacf-9e805f893040"},"source":["# register_backward_hook(hook_grad) # layer의 backward_pass 이후에 호줄되는 함수\r\n","# register_full_backward_hook(hook_grad)# forward시 autograd nodes가 많이 포함될 경우 full 버전을 써야함 \r\n","\r\n","def hook_grad(self, grad_input, grad_output): \r\n","  # grad_input, grad_output을 내부 로직에서 바꾸면 안된다\r\n","  # 굳이 바꾸고 싶으면 return new_grad_input, new_grad_output\r\n","  # 새로운 input, output을 정해주고 return 해줘야함\r\n","  print('Inside ' + self.__class__.__name__ + ' backward')\r\n","  print('Inside classs:' + self.__class__.__name__)\r\n","\r\n","  print('grad_input: ', type(grad_input))\r\n","  print('grad_input[0]: ', type(grad_input[0]))\r\n","  print('grad_output: ', type(grad_output))\r\n","  print('grad_output[0]: ', type(grad_output[0]))\r\n","\r\n","net = SimpleNet()\r\n","net.conv1.register_backward_hook(hook_grad) # backward_pass 이전에 호줄되는 함수\r\n","# net.conv1.register_full_backward_hook(hook_grad) # forward시 autograd nodes가 많이 포함될 경우 full 버전을 써야함 \r\n","\r\n","input = torch.randn(1, 1, 28, 28)\r\n","out = net(input)\r\n","\r\n","target = torch.tensor([3], dtype=torch.long)\r\n","loss_fn = nn.CrossEntropyLoss()\r\n","err = loss_fn(out, target)\r\n","err.backward()"],"execution_count":58,"outputs":[{"output_type":"stream","text":["Inside Conv2d backward\n","Inside classs:Conv2d\n","grad_input:  <class 'tuple'>\n","grad_input[0]:  <class 'NoneType'>\n","grad_output:  <class 'tuple'>\n","grad_output[0]:  <class 'torch.Tensor'>\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"QF8s-3IP9CnZ"},"source":["save_feat = [] # featuremap을 저장할 전역 변수 실행\r\n","def hook_feat(module, input, output): # hook function 정의\r\n","  save_feat.append(output)\r\n","  return output\r\n","\r\n","# model layer를 돌면서 원하는 layer에 hook 등록\r\n","for name, module in model.get_model_shortcuts(): # model layer들을 가져오는 함수\r\n","  if(name == 'target_layer_name'):\r\n","    module.register_forward_hook(hook_feat)\r\n","\r\n","\r\n","img = img.unsqueeze(0)\r\n","s = model(img)[0]\r\n","print(save_feat)"],"execution_count":null,"outputs":[]}]}